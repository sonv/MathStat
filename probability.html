<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Probability | MATH 310: Mathematical Statistics (brief notes)</title>
  <meta name="description" content="Chapter 1 Probability | MATH 310: Mathematical Statistics (brief notes)" />
  <meta name="generator" content="bookdown 0.37 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Probability | MATH 310: Mathematical Statistics (brief notes)" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Probability | MATH 310: Mathematical Statistics (brief notes)" />
  
  
  

<meta name="author" content="Truong-Son Van" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="part-1-background.html"/>
<link rel="next" href="part-2-inference.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a>Mathematical Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Disclaimer</a></li>
<li class="chapter" data-level="" data-path="syllabus.html"><a href="syllabus.html"><i class="fa fa-check"></i>Syllabus</a>
<ul>
<li class="chapter" data-level="" data-path="syllabus.html"><a href="syllabus.html#key-information"><i class="fa fa-check"></i>Key information</a></li>
<li class="chapter" data-level="" data-path="syllabus.html"><a href="syllabus.html#textbooks-and-references"><i class="fa fa-check"></i>Textbooks and references</a>
<ul>
<li class="chapter" data-level="" data-path="syllabus.html"><a href="syllabus.html#course-description"><i class="fa fa-check"></i>Course description</a></li>
<li class="chapter" data-level="" data-path="syllabus.html"><a href="syllabus.html#learning-objectives"><i class="fa fa-check"></i>Learning objectives</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="syllabus.html"><a href="syllabus.html#assessment"><i class="fa fa-check"></i>Assessment</a></li>
<li class="chapter" data-level="" data-path="syllabus.html"><a href="syllabus.html#core-content"><i class="fa fa-check"></i>Core content</a></li>
<li class="chapter" data-level="" data-path="syllabus.html"><a href="syllabus.html#project-description"><i class="fa fa-check"></i>Project description</a>
<ul>
<li class="chapter" data-level="" data-path="syllabus.html"><a href="syllabus.html#project-guidelines"><i class="fa fa-check"></i>Project Guidelines</a></li>
<li class="chapter" data-level="" data-path="syllabus.html"><a href="syllabus.html#project-timeline"><i class="fa fa-check"></i>Project Timeline</a></li>
<li class="chapter" data-level="" data-path="syllabus.html"><a href="syllabus.html#possible-topics"><i class="fa fa-check"></i>Possible topics</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="syllabus.html"><a href="syllabus.html#late-assignments"><i class="fa fa-check"></i>Late assignments</a></li>
<li class="chapter" data-level="" data-path="syllabus.html"><a href="syllabus.html#time-expectations"><i class="fa fa-check"></i>Time expectations</a>
<ul>
<li class="chapter" data-level="" data-path="syllabus.html"><a href="syllabus.html#collaboration-plagiarism"><i class="fa fa-check"></i>Collaboration &amp; Plagiarism</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="syllabus.html"><a href="syllabus.html#learning-support"><i class="fa fa-check"></i>Learning Support</a></li>
<li class="chapter" data-level="" data-path="syllabus.html"><a href="syllabus.html#wellbeing"><i class="fa fa-check"></i>Wellbeing</a></li>
<li class="chapter" data-level="" data-path="syllabus.html"><a href="syllabus.html#tentative-course-schedule"><i class="fa fa-check"></i>Tentative Course Schedule</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-1-background.html"><a href="part-1-background.html"><i class="fa fa-check"></i>PART 1: Background</a></li>
<li class="chapter" data-level="1" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>1</b> Probability</a>
<ul>
<li class="chapter" data-level="1.1" data-path="probability.html"><a href="probability.html#review"><i class="fa fa-check"></i><b>1.1</b> Review</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="probability.html"><a href="probability.html#probability-space"><i class="fa fa-check"></i><b>1.1.1</b> Probability Space</a></li>
<li class="chapter" data-level="1.1.2" data-path="probability.html"><a href="probability.html#random-variables"><i class="fa fa-check"></i><b>1.1.2</b> Random Variables</a></li>
<li class="chapter" data-level="1.1.3" data-path="probability.html"><a href="probability.html#joint-distribution-of-rvs"><i class="fa fa-check"></i><b>1.1.3</b> Joint distribution of RVs</a></li>
<li class="chapter" data-level="1.1.4" data-path="probability.html"><a href="probability.html#some-important-random-variables"><i class="fa fa-check"></i><b>1.1.4</b> Some important random variables</a></li>
<li class="chapter" data-level="1.1.5" data-path="probability.html"><a href="probability.html#independent-random-variables"><i class="fa fa-check"></i><b>1.1.5</b> Independent random variables</a></li>
<li class="chapter" data-level="1.1.6" data-path="probability.html"><a href="probability.html#transformations-of-rvs"><i class="fa fa-check"></i><b>1.1.6</b> Transformations of RVs</a></li>
<li class="chapter" data-level="1.1.7" data-path="probability.html"><a href="probability.html#expectation"><i class="fa fa-check"></i><b>1.1.7</b> Expectation</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="probability.html"><a href="probability.html#moment-generating-and-characteristic-functions"><i class="fa fa-check"></i><b>1.2</b> Moment Generating and Characteristic Functions</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="probability.html"><a href="probability.html#moment-generating-functions"><i class="fa fa-check"></i><b>1.2.1</b> Moment Generating Functions</a></li>
<li class="chapter" data-level="1.2.2" data-path="probability.html"><a href="probability.html#characteristic-functions"><i class="fa fa-check"></i><b>1.2.2</b> Characteristic Functions</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="probability.html"><a href="probability.html#inequalities"><i class="fa fa-check"></i><b>1.3</b> Inequalities</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="probability.html"><a href="probability.html#typical-tail-bound-inequalities"><i class="fa fa-check"></i><b>1.3.1</b> Typical tail bound inequalities</a></li>
<li class="chapter" data-level="1.3.2" data-path="probability.html"><a href="probability.html#exponential-concentration-inequalities"><i class="fa fa-check"></i><b>1.3.2</b> Exponential concentration inequalities</a></li>
<li class="chapter" data-level="1.3.3" data-path="probability.html"><a href="probability.html#inequalities-for-expectations"><i class="fa fa-check"></i><b>1.3.3</b> Inequalities for expectations</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="probability.html"><a href="probability.html#law-of-large-numbers"><i class="fa fa-check"></i><b>1.4</b> Law of Large Numbers</a></li>
<li class="chapter" data-level="1.5" data-path="probability.html"><a href="probability.html#central-limit-theorem"><i class="fa fa-check"></i><b>1.5</b> Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-2-inference.html"><a href="part-2-inference.html"><i class="fa fa-check"></i>PART 2: Inference</a></li>
<li class="chapter" data-level="2" data-path="sampling-estimating-cdf-and-statistical-functionals.html"><a href="sampling-estimating-cdf-and-statistical-functionals.html"><i class="fa fa-check"></i><b>2</b> Sampling, Estimating CDF and Statistical Functionals</a>
<ul>
<li class="chapter" data-level="2.1" data-path="sampling-estimating-cdf-and-statistical-functionals.html"><a href="sampling-estimating-cdf-and-statistical-functionals.html#empirical-distribution"><i class="fa fa-check"></i><b>2.1</b> Empirical Distribution</a></li>
<li class="chapter" data-level="2.2" data-path="sampling-estimating-cdf-and-statistical-functionals.html"><a href="sampling-estimating-cdf-and-statistical-functionals.html#statistical-functionals"><i class="fa fa-check"></i><b>2.2</b> Statistical Functionals</a></li>
<li class="chapter" data-level="2.3" data-path="sampling-estimating-cdf-and-statistical-functionals.html"><a href="sampling-estimating-cdf-and-statistical-functionals.html#bootstrap"><i class="fa fa-check"></i><b>2.3</b> Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="parametric-inference-parameter-estimation.html"><a href="parametric-inference-parameter-estimation.html"><i class="fa fa-check"></i><b>3</b> Parametric Inference (Parameter Estimation)</a>
<ul>
<li class="chapter" data-level="3.1" data-path="parametric-inference-parameter-estimation.html"><a href="parametric-inference-parameter-estimation.html#method-of-moments"><i class="fa fa-check"></i><b>3.1</b> Method of Moments</a></li>
<li class="chapter" data-level="3.2" data-path="parametric-inference-parameter-estimation.html"><a href="parametric-inference-parameter-estimation.html#method-of-maximum-likelihood"><i class="fa fa-check"></i><b>3.2</b> Method of Maximum Likelihood</a></li>
<li class="chapter" data-level="3.3" data-path="parametric-inference-parameter-estimation.html"><a href="parametric-inference-parameter-estimation.html#bayesian-approach"><i class="fa fa-check"></i><b>3.3</b> Bayesian Approach</a></li>
<li class="chapter" data-level="3.4" data-path="parametric-inference-parameter-estimation.html"><a href="parametric-inference-parameter-estimation.html#expectation-maximization-algorithm"><i class="fa fa-check"></i><b>3.4</b> Expectation-Maximization Algorithm</a></li>
<li class="chapter" data-level="3.5" data-path="parametric-inference-parameter-estimation.html"><a href="parametric-inference-parameter-estimation.html#unbiased-estimators"><i class="fa fa-check"></i><b>3.5</b> Unbiased Estimators</a></li>
<li class="chapter" data-level="3.6" data-path="parametric-inference-parameter-estimation.html"><a href="parametric-inference-parameter-estimation.html#efficiency-cramer-rao-inequality"><i class="fa fa-check"></i><b>3.6</b> Efficiency: Cramer-Rao Inequality</a></li>
<li class="chapter" data-level="3.7" data-path="parametric-inference-parameter-estimation.html"><a href="parametric-inference-parameter-estimation.html#sufficiency-and-unbiasedness-rao-blackwell-theorem"><i class="fa fa-check"></i><b>3.7</b> Sufficiency and Unbiasedness: Rao-Blackwell Theorem</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>4</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="4.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#neyman-pearson-lemma"><i class="fa fa-check"></i><b>4.1</b> Neyman-Pearson Lemma</a></li>
<li class="chapter" data-level="4.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#wald-test"><i class="fa fa-check"></i><b>4.2</b> Wald Test</a></li>
<li class="chapter" data-level="4.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#likelihood-ratio-test"><i class="fa fa-check"></i><b>4.3</b> Likelihood Ratio Test</a></li>
<li class="chapter" data-level="4.4" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#comparing-samples"><i class="fa fa-check"></i><b>4.4</b> Comparing samples</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-3-models.html"><a href="part-3-models.html"><i class="fa fa-check"></i>PART 3: Models</a></li>
<li class="chapter" data-level="5" data-path="linear-least-squares.html"><a href="linear-least-squares.html"><i class="fa fa-check"></i><b>5</b> Linear Least Squares</a>
<ul>
<li class="chapter" data-level="5.1" data-path="linear-least-squares.html"><a href="linear-least-squares.html#simple-linear-regression"><i class="fa fa-check"></i><b>5.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="5.2" data-path="linear-least-squares.html"><a href="linear-least-squares.html#matrix-approach"><i class="fa fa-check"></i><b>5.2</b> Matrix Approach</a></li>
<li class="chapter" data-level="5.3" data-path="linear-least-squares.html"><a href="linear-least-squares.html#statistical-properties"><i class="fa fa-check"></i><b>5.3</b> Statistical Properties</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MATH 310: Mathematical Statistics (brief notes)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="probability" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">Chapter 1</span> Probability<a href="probability.html#probability" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<blockquote>
<p>“If we have an atom that is in an excited state and so is going to emit a photon, we cannot say when it will emit the photon. It has a certain amplitude to emit the photon at any time, and we can predict only a probability for emission; we cannot predict the future exactly.”</p>
<footer>
— Richard Feynman
</footer>
</blockquote>
<div id="review" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> Review<a href="probability.html#review" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="probability-space" class="section level3 hasAnchor" number="1.1.1">
<h3><span class="header-section-number">1.1.1</span> Probability Space<a href="probability.html#probability-space" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="definition">
<p><span id="def:unlabeled-div-1" class="definition"><strong>Definition 1.1  (Sigma-algebra) </strong></span>Let <span class="math inline">\(\Omega\)</span> be a set.
A set <span class="math inline">\(\Sigma \subseteq \mathcal{P}(\Omega)\)</span> of subsets of <span class="math inline">\(\Omega\)</span> is called a <span class="math inline">\(\sigma\)</span>-algebra
of <span class="math inline">\(\Omega\)</span> if</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\Omega\in \Sigma\)</span></p></li>
<li><p><span class="math inline">\(F \in \Sigma \implies F^C \in \Sigma\)</span></p></li>
<li><p>If <span class="math inline">\(F_n \in \Sigma\)</span> for all <span class="math inline">\(n\in \mathbb{N}\)</span>, then
<span class="math display">\[ \bigcup_n F_n \in \Sigma .\]</span></p></li>
</ol>
</div>
<p>It is extremely convenient to deal with things called open sets.
The definition of those are a bit out of the scope of this class.
However,
in the case of the real line <span class="math inline">\(\mathbb{R}\)</span>, open sets are defined to be made of by finite intersections and arbitrary unions of open intervals <span class="math inline">\((a,b)\)</span>.
For example, <span class="math inline">\((0,1)\cup (2,3)\)</span> is an open set.</p>
<p>Interestingly, <span class="math inline">\(\mathbb{R}\)</span> and <span class="math inline">\(\emptyset\)</span> are called clopen sets (here’s a funny YouTube video about clopen sets: <a href="https://www.youtube.com/watch?v=SyD4p8_y8Kw" class="uri">https://www.youtube.com/watch?v=SyD4p8_y8Kw</a>)</p>
<p>A Borel <span class="math inline">\(\sigma\)</span>-algebra is the smallest <span class="math inline">\(\sigma\)</span>-algebra that contains all the open sets.
We denote the Borel <span class="math inline">\(\sigma\)</span>-algebra of a set <span class="math inline">\(\Omega\)</span> to be <span class="math inline">\(\mathcal{B}(\Omega)\)</span>.
This is a rather abstract definition. There is no clear way to construct a sigma algebra from a collection of sets. However, the construction is not important as the reassurance that this object does exist to give us nice domains to work with when we define a probability measure (see below definition).</p>
<div class="exercise">
<p><span id="exr:unlabeled-div-2" class="exercise"><strong>Exercise 1.1  </strong></span>(<em>Challenging– not required but good for the brain</em>)
It turns out that if <span class="math inline">\(\Omega\)</span> is a discrete set, it is typical to have the set of open sets contain every set of singletons, i.e.,
the set <span class="math inline">\(\{ a \}\)</span> is open for every <span class="math inline">\(a\in \Omega\)</span>.
Take this as an assumption, show that
for any discrete set <span class="math inline">\(\Omega\)</span>, <span class="math inline">\(\mathcal{B}(\Omega) = \mathcal{P}(\Omega)\)</span>.</p>
</div>
<p>What open sets really are is not important for now. The important thing is that for <span class="math inline">\(\mathbb{R}^n\)</span>
open sets are made of open intervals/ open boxes.
Your typical intuitions still work.</p>
<p>Philosophically, the <span class="math inline">\(\sigma\)</span>-algebra represents the details of information we could have access to.
There are certain events that are building blocks of knowledge and that we don’t have
access to finer details.</p>
<p>Think about the <span class="math inline">\(\sigma\)</span>-algebra as a consistent model of what can be known (observed). For example, you can never know
what’s going on in the houses on the street unless you have been to them.
But somehow, together, you are still able to piece all the information you have about the houses
to make sense of the world. This is related to the problem of information. How much information is enough to be useful in certain situation?!</p>
<p>To have a consistent system is not the same as to know everything. The system
you see/invent can never be exhaustively true, but you can still say something about
the reality if you can have a system that is consistent with what you observe. This
is why we do sampling!!</p>
<p>When you have a consistent model, you now want to encode the model in such a way
that it helps you with describing/predict the reality you see.
A way to do that with no full knowledge of anything is to assign the certain number
to measure the chance for something to happen at a given time.
This encoding needs to happen on the model you constructed. This leads to the following definition of
probability space.</p>
<div class="definition">
<p><span id="def:unlabeled-div-3" class="definition"><strong>Definition 1.2  (Probability Space) </strong></span>A <em>Probability Space</em> is a triple <span class="math inline">\((\Omega, \mathcal{F}, \mathbb{P})\)</span>, where
<span class="math inline">\(\Omega\)</span> is a set called <em>sample space</em>, <span class="math inline">\(\mathcal{F}\)</span> is a <span class="math inline">\(\sigma\)</span>-algebra on <span class="math inline">\(\Omega\)</span>,
<span class="math inline">\(\mathbb{P}: \mathcal{F}\to [0,1]\)</span>, called a <em>Probability Measure</em>, is a function that satisfies the following:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\mathbb{P}(\Omega) =1\)</span>,</p></li>
<li><p>If <span class="math inline">\(F\)</span> is a disjoint union of <span class="math inline">\(\left\{ F_n \right\}_{n=1}^\infty\)</span>, then
<span class="math display">\[ \mathbb{P}(F) = \sum_{n=1}^\infty \mathbb{P}(F_n) . \]</span></p></li>
</ol>
<p>Each element <span class="math inline">\(\omega \in \Omega\)</span> is called an <em>outcome</em> and each subset <span class="math inline">\(A \in \mathcal{F}\)</span>
is called an <em>event</em>.</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-4" class="definition"><strong>Definition 1.3  (Independent Events) </strong></span>Let <span class="math inline">\(A, B \in \mathcal{F}\)</span> be events. We say that <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent
if
<span class="math display">\[ \mathbb{P}(A \cap B) = \mathbb{P}(A) \mathbb{P}(B)  \,. \]</span></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-5" class="definition"><strong>Definition 1.4  (Conditional Probability) </strong></span>Let <span class="math inline">\(A, B \in \mathcal{F}\)</span> be events such that <span class="math inline">\(\mathbb{P}(B) &gt;0\)</span>. Then, the conditional probability of
<span class="math inline">\(A\)</span> given <span class="math inline">\(B\)</span> is
<span class="math display">\[ \mathbb{P}(A \vert B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)} \,. \]</span></p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-6" class="theorem"><strong>Theorem 1.1  (Bayes's Theorem) </strong></span>Let <span class="math inline">\(A, B \in \mathcal{F}\)</span> be events such that <span class="math inline">\(\mathbb{P}(A)&gt;0\)</span> and <span class="math inline">\(\mathbb{P}(B) &gt;0\)</span>.
Then,
<span class="math display">\[\mathbb{P}(A | B) = \frac{\mathbb{P}(B | A) \mathbb{P}(A)}{\mathbb{P}(B)} \,.\]</span></p>
</div>
<p>In modern statistics, there are names for the above terms:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\mathbb{P}(A | B)\)</span> is called <em>Posterior Probability</em>,</p></li>
<li><p><span class="math inline">\(\mathbb{P}(B | A)\)</span> is called <em>Likelihood</em>,</p></li>
<li><p><span class="math inline">\(\mathbb{P}(A)\)</span> is called <em>Prior Probability</em>,</p></li>
<li><p><span class="math inline">\(\mathbb{P}(B)\)</span> is called <em>Evidence</em>.</p></li>
</ol>
<p>The theorem is often expressed in words as:</p>
<p><span class="math display">\[ \text{Posterior Probability} = \frac{\text{Likelihood} \times \text{Prior Probability}}{\text{Evidence}} \]</span></p>
<p>It is a good idea to ponder why those mathematical terms have those names.</p>
</div>
<div id="random-variables" class="section level3 hasAnchor" number="1.1.2">
<h3><span class="header-section-number">1.1.2</span> Random Variables<a href="probability.html#random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The notion of probability alone isn’t sufficient for us to describe ideas about the world.
We need to have a notion of objects that associated with probabilities.
This brings about the idea of <em>random variable</em>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-7" class="definition"><strong>Definition 1.5  (Random Variable) </strong></span>Let <span class="math inline">\((\Omega, \mathcal{B}(\Omega), \mathbb{P})\)</span> be a probability space and <span class="math inline">\((S, \mathcal{B}(S))\)</span> a <span class="math inline">\(\sigma\)</span>-algebra.
A random variable is a (Borel measurable) function from <span class="math inline">\(\Omega \to S\)</span>.</p>
<ul>
<li><span class="math inline">\(S\)</span> is called the <em>state space</em> of <span class="math inline">\(X\)</span>.</li>
</ul>
</div>
<p>In this course, we will restrict our attentions to two types of random variables: discrete and continuous.</p>
<div class="definition">
<p><span id="def:unlabeled-div-8" class="definition"><strong>Definition 1.6  (Discrete RV) </strong></span><span class="math inline">\(X: \Omega \to S\)</span> is called
a discrete RV if <span class="math inline">\(S\)</span> is a countable set.</p>
<ul>
<li>A <em>probability function</em> or <em>probability mass function</em> for <span class="math inline">\(X\)</span> is a function
<span class="math inline">\(f_X: S \to [0,1]\)</span> defined by
<span class="math display">\[ f_X(x) = \mathbb{P}(X = x) \,. \]</span></li>
</ul>
</div>
<p>In contrast to the simplicity of discrete RV. Continuous RVs are a little bit messier to describe.
This is because of the lack of background in measure theory so we can talk about this concept in
a more precise way.</p>
<div class="definition">
<p><span id="def:unlabeled-div-9" class="definition"><strong>Definition 1.7  (Continuous RV) </strong></span>A <em>continuous random variable</em> is a measurable function <span class="math inline">\(X:\Omega \to S\)</span> is continuous
if it satisfies the following conditions:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(S = \mathbb{R}^n\)</span> for some <span class="math inline">\(n\in \mathbb{N}\)</span>.</p></li>
<li><p>There exists an (integrable) function <span class="math inline">\(f_X\)</span> such that <span class="math inline">\(f_X(x) \geq 0\)</span>
for all <span class="math inline">\(x, \int_{\mathbb{R}^n} f_X(x) d x=1\)</span> and for every open cube <span class="math inline">\(C \subseteq \mathbb{R}^n\)</span>,
<span class="math display">\[
\mathbb{P}(X\in C)=\int_C f_X(x) dV .
\]</span></p></li>
</ol>
<p>The function <span class="math inline">\(f_X\)</span> is called the <em>probability density function (PDF)</em>.</p>
</div>
<p>If two RVs <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> share the same probability function, we say that they
have the same distribution and denote them by
<span class="math display">\[ X \stackrel{d}{=}Y.\]</span></p>
<p>In this case we also say that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <strong>equal in distribution</strong>.</p>
<div class="remark">
<p><span id="unlabeled-div-10" class="remark"><em>Remark</em>. </span>To make the presentation more compact and clean,
notationally, we will write
<span class="math display">\[ \int f(x)  dx \]</span>
to mean both integral (for continuous RV) and summation (for discrete RV).</p>
</div>
<p>There are more general concepts of continuous RV where we don’t need to require
<span class="math inline">\(S\)</span> to be a Euclidean space as in the above definition.
However, such concepts require the readers to be familiar with advanced subjects
like Topology and Measure Theory. It is particularly important to know these two
subjects
in order to thoroughly understand Stochastic Processes.</p>
<div class="exercise">
<p><span id="exr:unlabeled-div-11" class="exercise"><strong>Exercise 1.2  </strong></span>Create a random variable that represents the results of <span class="math inline">\(n\)</span> coin flips.</p>
</div>
<p>For real-valued RV <span class="math inline">\(X:\Omega \to \mathbb{R}\)</span> we have the concept of cumulative distribution function.</p>
<div class="definition">
<p><span id="def:unlabeled-div-12" class="definition"><strong>Definition 1.8  (Cumulative Distribution Function) </strong></span>Given a RV <span class="math inline">\(X:\Omega \to \mathbb{R}\)</span>.
The <em>cumulative distribution function of <span class="math inline">\(X\)</span></em> or CDF, is
a function <span class="math inline">\(F_X : \mathbb{R}\to [0,1]\)</span>
defined by
<span class="math display">\[ F_X (x) = \mathbb{P}(X \leq x) \,. \]</span></p>
</div>
<p><strong>Notationally, we use the notation <span class="math inline">\(X\sim F\)</span> to mean
RV <span class="math inline">\(X\)</span> with distribution <span class="math inline">\(F\)</span>.</strong></p>
<div class="exercise">
<p><span id="exr:unlabeled-div-13" class="exercise"><strong>Exercise 1.3  </strong></span>Given a real-valued continuous RV <span class="math inline">\(X: \Omega \to \mathbb{R}\)</span>, prove that
if <span class="math inline">\(f_X\)</span> is continuous then
<span class="math display">\[
F_X(x)=\int_{-\infty}^x f_X(t) d t
\]</span>
is differentiable for every <span class="math inline">\(x\)</span>
and <span class="math inline">\(f_X(x)=F_X^{\prime}(x)\)</span>.</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-14" class="exercise"><strong>Exercise 1.4  </strong></span>Let <span class="math inline">\(X\)</span> be an RV with CDF <span class="math inline">\(F\)</span> and <span class="math inline">\(Y\)</span> with CDF <span class="math inline">\(G\)</span>.
Suppose <span class="math inline">\(F(x) = G(x)\)</span> for all <span class="math inline">\(x\)</span>. Show that for every set <span class="math inline">\(A\)</span> that
is a countable union of open intervals,
<span class="math display">\[ \mathbb{P}(X \in A) = \mathbb{P}(Y \in A) \,.\]</span></p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-15" class="exercise"><strong>Exercise 1.5  </strong></span>Let <span class="math inline">\(X:\Omega \to \mathbb{R}\)</span> be an RV and <span class="math inline">\(F_X\)</span> be its CDF.
Prove the following:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(F\)</span> is non-decreasing: if <span class="math inline">\(x_1 \leq x_2\)</span>, then <span class="math inline">\(F(x_1) \leq F(x_2)\)</span>.</p></li>
<li><p><span class="math inline">\(F\)</span> is normalized:
<span class="math display">\[ \lim_{x\to -\infty} F(x) = 0 \,,\]</span>
and
<span class="math display">\[ \lim_{x\to \infty} F(x) = 1 \,.\]</span></p></li>
<li><p><span class="math inline">\(F\)</span> is right-continuous:
<span class="math display">\[ F(x) = F(x+) = \lim_{y \searrow x} F(y) \,.\]</span></p></li>
</ol>
</div>
</div>
<div id="joint-distribution-of-rvs" class="section level3 hasAnchor" number="1.1.3">
<h3><span class="header-section-number">1.1.3</span> Joint distribution of RVs<a href="probability.html#joint-distribution-of-rvs" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let <span class="math inline">\(X:\Omega \to S\)</span> and <span class="math inline">\(Y: \Omega \to S\)</span> be RVs.
We denote
<span class="math display">\[ \mathbb{P}(X \in A; Y \in B) = \mathbb{P}(\{X\in A\} \cap \{Y \in B \}) \,. \]</span></p>
<p>For discrete RVs, the joint probability function of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> has the following meaning
<span class="math display">\[f_{XY}(x,y) = \mathbb{P}(X = x; Y = y)\]</span></p>
<p>For continuous RVs, the situations are more complicated as we can’t make sense of <span class="math inline">\(\mathbb{P}(X = x; Y = y)\)</span> (this is always 0 in most situation and in some other situation, one can’t even talk about it– this is a topic of more advanced course in measure theory).
However, we can have
<span class="math display">\[\mathbb{P}(X \in A; Y \in B) = \int_{X \in A} \int_{Y \in B} f_{XY} (x,y) \, dx dy \,.\]</span></p>
<p>Another way to look at the above is the following.
We can even consider
<span class="math inline">\(X: \Omega \to S^n\)</span>,
where <span class="math inline">\(n\geq 2\)</span>.
Instead of thinking about this as one RV, we can think about this
as a vector of RVs:
<span class="math display">\[ X = \begin{pmatrix} X_1 \\ \vdots \\ X_n \end{pmatrix},\]</span>
where <span class="math inline">\(X_1, \dots X_n: \Omega \to S\)</span> are RVs.
Because of this, we have can write the density function as
<span class="math display">\[ f_X(x) = f_{X_1 X_2 \dots X_n}(x)\]</span></p>
<p>Some people call <span class="math inline">\(X\)</span> a random vector.</p>
<p><span class="math inline">\(f_{X_1\dots X_n}\)</span> is called the joint probability distribution.</p>
<div class="exercise">
<p><span id="exr:unlabeled-div-16" class="exercise"><strong>Exercise 1.6  </strong></span>True or false:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(f_{XY}(x,y) = f_{X}(x) + f_{Y}(y)\)</span></p></li>
<li><p><span class="math inline">\(f_{XY}(x,y) = f_{X}(x) f_{Y}(y)\)</span></p></li>
</ol>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-17" class="definition"><strong>Definition 1.9  (Marginal density) </strong></span>The marginal density of <span class="math inline">\(X_i\)</span> is <F12>
<span class="math display">\[f_{X_i}(x_i) = \int f_{X_1\dots X_n}(x_1, \dots, x_n) \, dx_1\dots dx_{i-1} dx_{i+1} \dots dx_n\]</span>
(integrate coordinate except the <span class="math inline">\(i\)</span>-th coordinate.</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-18" class="exercise"><strong>Exercise 1.7  </strong></span>Can you construct <span class="math inline">\(f_{XY}\)</span> if you know <span class="math inline">\(f_X\)</span> and <span class="math inline">\(f_Y\)</span>?</p>
</div>
</div>
<div id="some-important-random-variables" class="section level3 hasAnchor" number="1.1.4">
<h3><span class="header-section-number">1.1.4</span> Some important random variables<a href="probability.html#some-important-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ol style="list-style-type: decimal">
<li><p>Point mass distribution (Dirac delta):
Given a discrete probability <span class="math inline">\(X: \Omega \to S\)</span>. <span class="math inline">\(X\)</span> has a point mass distribution at <span class="math inline">\(a \in S\)</span>
if
<span class="math display">\[ \mathbb{P}( X = a) = 1.\]</span>
We call <span class="math inline">\(X\)</span> a point mass RV and write <span class="math inline">\(X \sim \delta_a\)</span>.</p>
<p><em>Question.</em> Suppose <span class="math inline">\(S = \mathbb{N}\)</span>. Write down <span class="math inline">\(F_X\)</span> for the point mass RV <span class="math inline">\(X\)</span>.</p></li>
<li><p>Discrete uniform distribution:
<span class="math inline">\(f_X(k) = \frac 1 n\,,\)</span>
<span class="math inline">\(k \in \{1,\dots, n\}\)</span>.</p></li>
<li><p>Bernoulli distribution:
let <span class="math inline">\(X:\Omega \to \{0,1\}\)</span> be RV that represents a binary coin flip.
Suppose <span class="math inline">\(\mathbb{P}(X = 1) = p\)</span> for some <span class="math inline">\(p \in [0,1]\)</span>.
Then <span class="math inline">\(X\)</span> has a Bernoulli distribution, written as <span class="math inline">\(X \sim \text{Bernoulli}(p)\)</span>.
The probability function is
<span class="math display">\[f_X(x) = p^x (1-p)^{1-x}.\]</span>
We write <span class="math inline">\(X \sim \text{Bernoulli}(p)\)</span>.</p></li>
<li><p>Binomial distribution:
let <span class="math inline">\(X:\Omega \to \mathbb{N}\)</span> be the RV that represents the number of heads out of <span class="math inline">\(n\)</span> independent coin flips.
Then
<span class="math display">\[ f_X = \begin{cases}
{n \choose x} p^x (1-p)^{n-x}\,, x\in \{0,1,\dots, n\}\\
     0 \,, \text{otherwise}
\end{cases}\]</span>
We write <span class="math inline">\(X \sim \text{Binomial}(n,p)\)</span>.</p></li>
<li><p>Poisson distribution: <span class="math inline">\(X \sim \text{Poisson}(\lambda)\)</span>.
<span class="math display">\[f_X (k) = e^{-\lambda} \frac{\lambda^k}{k!}\,, k = 0, 1, 2 \dots\]</span>
<span class="math inline">\(X\)</span> is a RV that describe a given number of events occurring in a
fixed interval of time or space if these events occur with a known constant mean rate and
independently of the time since the last event</p></li>
<li><p>Gaussian: <span class="math inline">\(X \sim N(\sigma,\mu)\)</span>.
<span class="math display">\[f_X(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-(x-\mu)^2/(2\sigma^2)}.\]</span></p></li>
</ol>
<div class="exercise">
<p><span id="exr:unlabeled-div-19" class="exercise"><strong>Exercise 1.8  </strong></span>Let <span class="math inline">\(X_{n,p} \sim \text{Binomial}(n,p)\)</span>. Suppose that as <span class="math inline">\(n\to \infty\)</span>, <span class="math inline">\(p \to 0\)</span> in such a way
that <span class="math inline">\(np = \lambda\)</span> always.
Let <span class="math inline">\(x\in \mathbb{N}\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p>For <span class="math inline">\(n\)</span> very very large, what is the behaviour of<br />
<span class="math display">\[ \frac{n!}{(n-x)!} \,.\]</span>
(You should just get some power of <span class="math inline">\(n\)</span>)</p></li>
<li><p>Show that
<span class="math display">\[\lim_{n\to \infty} f_{X_{n,p}}(x) = \frac{\lambda^x e^{-\lambda}}{x!}.\]</span></p></li>
<li><p>Interpret this result.</p></li>
</ol>
</div>
</div>
<div id="independent-random-variables" class="section level3 hasAnchor" number="1.1.5">
<h3><span class="header-section-number">1.1.5</span> Independent random variables<a href="probability.html#independent-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="definition">
<p><span id="def:unlabeled-div-20" class="definition"><strong>Definition 1.10  </strong></span>Let <span class="math inline">\(X:\Omega \to S\)</span> and <span class="math inline">\(Y: \Omega \to S\)</span> be RVs.
We say that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent if,
for every <span class="math inline">\(A, B \in \mathcal{B}(S)\)</span>, we have
<span class="math display">\[ \mathbb{P}( X \in A; Y \in B) = \mathbb{P}(X \in A) \mathbb{P}(Y \in B) \,, \]</span>
and write <span class="math inline">\(X \perp Y\)</span>.</p>
</div>
<p>So, if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent,
<span class="math display">\[f_{XY}(x,y) = f_X(x) f_Y(y).\]</span></p>
<div class="definition">
<p><span id="def:unlabeled-div-21" class="definition"><strong>Definition 1.11  </strong></span>Let <span class="math inline">\(X:\Omega \to S\)</span> and <span class="math inline">\(Y: \Omega \to S\)</span> be RVs.
Suppose that <span class="math inline">\(f_Y(y) &gt;0\)</span>.
The conditional probability mass function of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y\)</span> is
<span class="math display">\[ f_{X|Y} (x|y) = \frac{f_{XY}(x,y)}{f_Y(y)} \,.\]</span></p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-22" class="exercise"><strong>Exercise 1.9  </strong></span>Let
<span class="math display">\[ f(x,y) = \begin{cases} x+y \,, 0\leq x \leq 1, 0 \leq y \leq 1\\
            0\,, \text{otherwise} \end{cases}.\]</span>
What is <span class="math inline">\(\mathbb{P}( X &lt; 1/4 \vert Y = 1/3)\)</span>.</p>
</div>
<p>Note that the above exercise is a little bit weird and counter-intuitive.
While
<span class="math inline">\(\mathbb{P}( X &lt; 1/4 , Y = 1/3) = 0\)</span> (why?),
<span class="math inline">\(\mathbb{P}( X &lt; 1/4 | Y = 1/3) \not= 0\)</span></p>
<p>A very important RV is the <em>multivariate Normal RV</em>, which obeys the
following density function</p>
<p><span class="math display">\[f(x; \mu, \Sigma) = \frac{1}{(2\pi)^{n/2} | \Sigma |^{1/2}} \exp\big( -\frac{1}{2} (x-mu)^T \Sigma^{-1} (x-\mu)  \big).\]</span></p>
</div>
<div id="transformations-of-rvs" class="section level3 hasAnchor" number="1.1.6">
<h3><span class="header-section-number">1.1.6</span> Transformations of RVs<a href="probability.html#transformations-of-rvs" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Sometimes, we don’t work with RV directly but certain characteristics of RVs.
Those characteristics are represented by certain transformation.
If the functions are nice enough, we can actually have a recipe to generate
the probability density function.</p>
<p>Suppose <span class="math inline">\(g: S^n \to \mathbb{R}\)</span> and <span class="math inline">\(Z = g(X_1, \dots, X_n)\)</span>.
Let <span class="math inline">\(A_z = \{ (x_1,\dots, x_n): g(x_1,\dots, x_n) \leq z \}\)</span>.
Then
<span class="math display">\[F_Z(z) = \mathbb{P}(Z \leq z) = \mathbb{P}(g(X_1, \dots, X_n) \leq z) = \int_{A_z} f_{X_1\dots X_n}(x_1,\dots,x_n) \, dx_1\dots dx_n, \]</span>
and
<span class="math display">\[f_Z(z) = F_Z&#39;(z).\]</span></p>
<div class="exercise">
<p><span id="exr:unlabeled-div-23" class="exercise"><strong>Exercise 1.10  </strong></span></p>
<ol style="list-style-type: decimal">
<li><p>Let <span class="math inline">\(X, Y \sim \text{Uniform}(0,1)\)</span> be independent RVs, i.e.,
<span class="math display">\[f_{X}(x) = f_{Y}(y) = 1\,.\]</span>
What is the density function for the RV <span class="math inline">\(Z = X + Y\)</span>?</p></li>
<li><p>Same question but <span class="math inline">\(X, Y \sim N(0,1)\)</span>.</p></li>
</ol>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-24" class="definition"><strong>Definition 1.12  </strong></span>RVs that are independent and share the same distribution are called
<em>independent and identically distributed</em> RVs.</p>
<p>We often shorthand this by IID RVs.</p>
</div>
</div>
<div id="expectation" class="section level3 hasAnchor" number="1.1.7">
<h3><span class="header-section-number">1.1.7</span> Expectation<a href="probability.html#expectation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="definition">
<p><span id="def:unlabeled-div-25" class="definition"><strong>Definition 1.13  </strong></span>Let <span class="math inline">\(X\)</span> be a RV.</p>
<ol style="list-style-type: decimal">
<li><p>The <em>expected value</em>, or <em>expectation</em>, or <em>mean</em>, or <em>first moment</em> of <span class="math inline">\(X\)</span> is defined to be
<span class="math display">\[ \mathbb{E}X = \int x f(x)  dx. \]</span></p></li>
<li><p>The <em>variance</em> of <span class="math inline">\(X\)</span> is defined to be
<span class="math display">\[\mathbb{E}\left( X - \mathbb{E}X \right)^2\]</span></p></li>
</ol>
<p>We often denote <span class="math inline">\(\mu_X\)</span> to be the expectation of <span class="math inline">\(X\)</span>, <span class="math inline">\(\sigma_X^2\)</span> (<span class="math inline">\(\mathrm{Var}(X), \mathbb{V}(X)\)</span>) to be the variance of <span class="math inline">\(X\)</span>.</p>
<p>The square root of the variance, <span class="math inline">\(\sigma\)</span>, is called the <em>standard deviation</em>.</p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-26" class="theorem"><strong>Theorem 1.2  </strong></span>Let <span class="math inline">\(X:\Omega \to S\)</span> be a RV, <span class="math inline">\(r: S \to S\)</span> be a function and <span class="math inline">\(Y = r(X)\)</span>.
Then <span class="math display">\[ \mathbb{E}Y = \mathbb{E}(r(X)) = \int r(x) f(x)  dx \]</span></p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-27" class="exercise"><strong>Exercise 1.11  </strong></span></p>
<ol style="list-style-type: decimal">
<li>Let <span class="math inline">\(X \sim \text{Uniform}(0,1)\)</span>. Compute <span class="math inline">\(\mathbb{E}Y\)</span>, where
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(Y = e^X\)</span>.</li>
<li><span class="math inline">\(Y = \max(X, 1-X)\)</span></li>
</ol></li>
<li>Let <span class="math inline">\(X,Y\)</span> be RVs that have jointly uniform distribution on the unit square.
Compute <span class="math inline">\(\mathbb{E}(X^2 + Y^2)\)</span>.</li>
</ol>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-28" class="definition"><strong>Definition 1.14  </strong></span>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be RVs. The <em>covariance</em> between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is defined by
<span class="math display">\[ \mathrm{Cov}(X,Y) = \mathbb{E}\left( (X- \mu_X)(Y - \mu_Y)   \right) .\]</span></p>
<p>The <em>correlation</em> of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is defined to be
<span class="math display">\[ \rho_{X,Y} = \frac{\mathrm{Cov}(X,Y)}{\sigma_X \sigma_Y} \,.\]</span></p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-29" class="theorem"><strong>Theorem 1.3  </strong></span></p>
<ol style="list-style-type: decimal">
<li><p>Let <span class="math inline">\(X_i\)</span>, <span class="math inline">\(i=1, \dots, n\)</span> be RVs and <span class="math inline">\(a_i\)</span>’s be constants.
Then
<span class="math display">\[ \mathbb{E}\left(\sum_i a_i X_i \right) = \sum_i a_i \mathbb{E}X_i. \]</span></p></li>
<li><p><span class="math display">\[ \mathbb{V}X_i = \mathbb{E}(X_i^2) - \mu_{X_i}^2 \]</span></p></li>
<li><p><span class="math display">\[ \mathbb{V}\left( \sum_i a_i X_i  \right) = \sum_i a_i^2 \mathbb{V}(X_i) \]</span></p></li>
<li><p><span class="math display">\[ \mathbb{V}\left( \sum_i a_i X_i  \right) = \sum_i a_i^2 \mathbb{V}(X_i) + 2\sum_{i&lt;j} a_i a_j \mathrm{Cov}(X_i, X_j) \,. \]</span></p></li>
<li><p>Suppose further that <span class="math inline">\(X_i\)</span>’s are independent, then
<span class="math display">\[ \mathbb{E}\left( \prod_{i=1}^n X_i  \right) = \prod_{i=1}^n \mathbb{E}X_i\]</span>
and
<span class="math display">\[ \mathbb{V}\left( \sum_i a_i X_i  \right) = \sum_i a_i^2 \mathbb{V}(X_i) \,. \]</span></p></li>
</ol>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-30" class="definition"><strong>Definition 1.15  (Conditional Expectation) </strong></span>Let <span class="math inline">\(X,Y:\Omega \to S\)</span>, where <span class="math inline">\(S\)</span> is either <span class="math inline">\(\mathbb{N}\)</span> or <span class="math inline">\(\mathbb{R}\)</span>.
The conditional expectation of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y\)</span> is a RV <span class="math inline">\(\mathbb{E}[X | Y] : \Omega \to \mathbb{R}\)</span>
that satisfies the following
<span class="math display">\[ \mathbb{E}[X | Y](y) := \mathbb{E}[X | Y = y] = \int x f_{X|Y}(x|y) \, dx . \]</span>
If <span class="math inline">\(r:S^2 \to S\)</span> is a function, then
<span class="math display">\[ \mathbb{E}[r(X,Y) | Y = y] = \int r(x,y) f_{X|Y}(x|y) \, dx .\]</span></p>
</div>
<p>One can generalize this definition to higher dimension via the coordinate-wise conditional
expectation. We will omit this definition in order to keep the presentation simple.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-31" class="theorem"><strong>Theorem 1.4  </strong></span>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be Rvs. We have that
<span class="math display">\[ \mathbb{E}[ \mathbb{E}[X | Y]] = \mathbb{E}X. \]</span></p>
</div>
</div>
</div>
<div id="moment-generating-and-characteristic-functions" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> Moment Generating and Characteristic Functions<a href="probability.html#moment-generating-and-characteristic-functions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="definition">
<p><span id="def:unlabeled-div-32" class="definition"><strong>Definition 1.16  </strong></span>Let <span class="math inline">\(X\)</span> be a RV.
1. The <em>moment generating function</em> MGF, or <em>Laplace transform</em>, of <span class="math inline">\(X\)</span> is <span class="math inline">\(\varphi: \mathbb{R}\to \mathbb{R}\)</span> defined by
<span class="math display">\[ \varphi_X (t) = \mathbb{E}\left( e^{t X}  \right),  \]</span>
where <span class="math inline">\(t\)</span> varies over the real numbers.</p>
<ol start="2" style="list-style-type: decimal">
<li>The <em>characteristic function</em>, or <em>Fourier transform</em> of <span class="math inline">\(X\)</span> is <span class="math inline">\(\varphi: \mathbb{R}\to \mathbb{C}\)</span> defined by
<span class="math display">\[\phi_X(\theta) = \mathbb{E}e^{i\theta X} .\]</span></li>
</ol>
</div>
<div class="lemma">
<p><span id="lem:unlabeled-div-33" class="lemma"><strong>Lemma 1.1  </strong></span></p>
<ol style="list-style-type: decimal">
<li><p>Let <span class="math inline">\(X\)</span> be a RV and <span class="math inline">\(Y = aX + b\)</span>, then
<span class="math display">\[ \varphi_Y(t) = e^{bt} \varphi_{X}(at)\]</span></p></li>
<li><p><span class="math display">\[ \varphi_X^{(k)}(0) = \mathbb{E}(X^k) \]</span></p></li>
<li><p>Let <span class="math inline">\(X_i\)</span>, <span class="math inline">\(i= 1, \dots, n\)</span> be independent RVs and <span class="math inline">\(Y = \sum_i X_i\)</span>.
Then
<span class="math display">\[ \varphi_Y (t) = \prod \varphi_{X_i}(t). \]</span></p></li>
<li><p><span class="math display">\[| \phi (\theta) | \leq 1\]</span></p></li>
<li><p>Denote <span class="math inline">\(\overline{z}\)</span> to be the complex conjugate of <span class="math inline">\(z\)</span> in the complex plane.
<span class="math display">\[ \phi_{-X} (\theta) = \overline{\phi_X (\theta)} \]</span></p></li>
<li><p><span class="math display">\[\phi_Y (\theta) = e^{i b \theta} \phi(a\theta) \]</span></p></li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-34" class="exercise"><strong>Exercise 1.12  </strong></span>Prove the above lemma.</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-35" class="exercise"><strong>Exercise 1.13  </strong></span>Let <span class="math inline">\(X \sim \exp(1)\)</span>, i.e,
<span class="math display">\[ f_X(x) = \begin{cases} e^{-x} \,, x \geq 0 \\ 0 \,, x &lt; 0 \,. \end{cases}\]</span>
Compute <span class="math inline">\(\varphi_X\)</span>.</p>
</div>
<p>Recall that two RVs <span class="math inline">\(X \stackrel{d}{=}Y\)</span> means that <span class="math inline">\(F_X (x) = F_Y(x)\)</span>.
Two common ways to characterize the equality in distribution are
to use the generating functions and the characteristic functions.</p>
<p>These ideas are not orginally from probability but from engineering/mechanics, where Laplace and Fourier transforms are understood very well
since the 18th century.</p>
<div class="exercise">
<p><span id="exr:moments" class="exercise"><strong>Exercise 1.14  </strong></span>In general, differentiation is not commutative with integration, that is
<span class="math display">\[ \frac{d}{dt} \int \not= \int \frac{d}{dt}. \]</span>
However, assuming that this is true for certain moment generating functions <span class="math inline">\(\varphi_X\)</span>.
Show that
<span class="math display">\[ \varphi_X^{(n)}(0) = \mathbb{E}( X^n),\]</span>
where <span class="math inline">\(f^{(n)}\)</span> denotes the <span class="math inline">\(n\)</span>-th derivative of <span class="math inline">\(f\)</span>.
<span class="math inline">\(\mathbb{E}(X^n)\)</span> is called the <span class="math inline">\(n\)</span>-th moment of <span class="math inline">\(X\)</span> and it tells you the tail behavior of <span class="math inline">\(f_X\)</span>.</p>
</div>
<div id="moment-generating-functions" class="section level3 hasAnchor" number="1.2.1">
<h3><span class="header-section-number">1.2.1</span> Moment Generating Functions<a href="probability.html#moment-generating-functions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="theorem">
<p><span id="thm:moment-generating" class="theorem"><strong>Theorem 1.5  </strong></span>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be RVs. If <span class="math inline">\(\varphi_X(t) = \varphi_Y(t)\)</span> for all <span class="math inline">\(t\)</span> in an interval around 0, then
<span class="math display">\[ X  \stackrel{d}{=}Y \,.\]</span></p>
</div>
<p>The full proof of this is beyond this class (and could be a great topic for a project).
However, we will prove this for finte RVs.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-36" class="proposition"><strong>Proposition 1.1  (Finite RV case) </strong></span>Let <span class="math inline">\(X,Y: \Omega \to \{1,2, \dots, N\}\)</span> be RVs. If <span class="math inline">\(\varphi_X(t) = \varphi_Y(t)\)</span> in an interval around in an interval
<span class="math inline">\((-\epsilon , \epsilon)\)</span>, then
<span class="math display">\[ X  \stackrel{d}{=}Y \,.\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-37" class="proof"><em>Proof</em>. </span>We have that
<span class="math display">\[ \varphi_X (t) = \mathbb{E}( e^{tX}) = \sum_{i= 1}^N e^{it}\mathbb{P}( X = i) \]</span>
and
<span class="math display">\[ \varphi_Y (t) = \mathbb{E}( e^{tX}) = \sum_{i= 1}^N e^{it}\mathbb{P}( Y = i). \]</span>
Therefore,
<span class="math display">\[ 0 = \varphi_X(t) - \varphi_Y(t) = \sum_{i=1}^N (e^t)^i \left( \mathbb{P}(X = i) - \mathbb{P}(Y = i)  \right) \]</span>
for every <span class="math inline">\(t \in (-\epsilon , \epsilon)\)</span>.
Therefore, as the above is a polynomial,
<span class="math display">\[ \mathbb{P}( X = i) = \mathbb{P}(Y = i) \]</span>
where <span class="math inline">\(i = 1, \dots, N\)</span>.</p>
</div>
<p>Note that if the above summation is infinite, then we cannot conclude that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>
has the same distribution as easily as we just did. More work has to be done to show
this.</p>
<p>A note of caution: the assumption that <span class="math inline">\(\varphi_X = \varphi_Y\)</span> in an interval around <span class="math inline">\(0\)</span>
is crucial in general.</p>
<p>An interesting observation arises: for analytic functions we have the Taylor series
<span class="math display">\[ f(x) = \sum_{i=0}^\infty \frac{f^{(n)}(0)}{n!} x^n. \]</span>
Exercise <a href="probability.html#exr:moments">1.14</a> tells you that the <span class="math inline">\(n\)</span>-th derivative at <span class="math inline">\(0\)</span>
of a moment generating function would be the <span class="math inline">\(n\)</span>-th moment of the RV.</p>
<p>Question: Is knowing the moments of <span class="math inline">\(X\)</span> enough to determine its probability distribution?</p>
<p>The answer is NO. One can take a look at the discussion about this problem here:
<a href="https://mathoverflow.net/questions/3525/when-are-probability-distributions-completely-determined-by-their-moments" class="uri">https://mathoverflow.net/questions/3525/when-are-probability-distributions-completely-determined-by-their-moments</a>.</p>
<p>However, things are nice for finite RVs.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-38" class="proposition"><strong>Proposition 1.2  </strong></span>Let <span class="math inline">\(X,Y: \Omega \to \{1,2, \dots, N\}\)</span> be RVs. Suppose that
<span class="math display">\[ \mathbb{E}(X^n) = \mathbb{E}(Y^n) &lt; \infty\]</span>
for every <span class="math inline">\(n\in \mathbb{N}\)</span>. Then
<span class="math display">\[ X \stackrel{d}{=}Y .\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-39" class="proof"><em>Proof</em>. </span>Consider
<span class="math display">\[ \varphi_X(t) = \mathbb{E}(e^{Xt}) = \sum_{i = 1}^N e^{it} \mathbb{P}( X = i). \]</span>
This is a finite sums of analytic functions and is, therefore, analytic.
Thus, <span class="math inline">\(\varphi_X\)</span> can be expanded into Taylor series, i.e.,
<span class="math display">\[ \varphi_X(t)  = \sum_{n=0}^\infty \frac{\varphi_X^{(n)}(0)}{n!} t^n
= \sum_{n=0}^\infty \frac{\mathbb{E}(X^n)}{n!} t^n.\]</span>
This means that the moments of <span class="math inline">\(X\)</span> determines its moment generating function
(which may not be true in general).</p>
<p>A similar argument can be made for <span class="math inline">\(\varphi_Y\)</span> and as the coefficents of the
Taylor series are the same (being the moments of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>), we conclude
that
<span class="math display">\[\varphi_X = \varphi_Y.\]</span>
Therefore,
by Theorem <a href="probability.html#thm:moment-generating">1.5</a>,
<span class="math display">\[ X \stackrel{d}{=}Y, \]</span>
as desired.</p>
</div>
</div>
<div id="characteristic-functions" class="section level3 hasAnchor" number="1.2.2">
<h3><span class="header-section-number">1.2.2</span> Characteristic Functions<a href="probability.html#characteristic-functions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Similar idea with the moment generating functions, but
characteristic functions are easier to work with and we don’t have to
work with special case of finite RVs.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-40" class="theorem"><strong>Theorem 1.6  </strong></span>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be RVs. If <span class="math inline">\(\phi_X(t) = \phi_Y(t)\)</span> for all <span class="math inline">\(t\)</span> in an interval around 0, then
<span class="math display">\[ X  \stackrel{d}{=}Y \,.\]</span></p>
</div>
<p>In order to prove this theorem, we need the following important result, called
inversion formula of the characteristic functions.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-41" class="theorem"><strong>Theorem 1.7  (Inversion Formula) </strong></span>Let <span class="math inline">\(X:\Omega \to S\)</span> be a RV (either continuous or discrete)
and <span class="math inline">\(\phi_X\)</span> be its characteristic function.
Then
<span class="math display">\[  \lim_{T \to \infty} \frac{1}{2\pi}\int_{-T}^T \frac{e^{-i\theta a}
- e^{-i\theta b}}{i\theta} \phi_X(\theta) \, d\theta
= \mathbb{P}( a &lt; X &lt; b) + \frac{1}{2} \left( \mathbb{P}(X = a) + \mathbb{P}(X = b)  \right). \]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-42" class="proof"><em>Proof</em>. </span>We have
<span class="math display">\[\begin{aligned}
\frac{1}{2\pi} \int_{-T}^T \frac{ e^{ - i \theta a} - e^{- i \theta b}}{i\theta} \phi_X(\theta) \, d\theta
&amp; = \frac{1}{2\pi} \int_{-T}^T \frac{ e^{ - i \theta a} - e^{- i \theta b}}{i\theta} \int_{\mathbb{R}} e^{i\theta x} f_X(x) \, d\theta dx \\
&amp; = \int_{\mathbb{R}}\frac{1}{\pi} \int_{-T}^T \frac{ e^{ i \theta (x - a)} - e^{i \theta (x- b)}}{ 2 i\theta}  f_X(x) \, d\theta dx \\
\end{aligned}\]</span></p>
<p>Note that since <span class="math inline">\(\cos(t)/t\)</span> is odd and <span class="math inline">\(\sin(t)/t\)</span> is even, and that <span class="math inline">\(e^{i\theta} = \cos(\theta) + i \sin(\theta)\)</span>,
we have
<span class="math display">\[ \frac{1}{2}\int_{-T}^T \frac{e^{i\theta c}}{i \theta} = \int_0^T \frac{\sin(\theta c)}{\theta} \, d\theta. \]</span></p>
<p>Therefore,
<span class="math display">\[
\begin{aligned}
\frac{1}{2\pi} \int_{-T}^T \frac{ e^{ - i \theta a} - e^{- i \theta b}}{i\theta} \phi_X(\theta) \, d\theta
&amp;= \frac{1}{\pi} \int_{\mathbb{R}} \int_0^T \left(  \frac{\sin((x - a)\theta)}{\theta} -  \frac{\sin((x - b)\theta)}{\theta} \right) f_X(x) \, d\theta dx
\end{aligned}
\]</span>
Taking the limit <span class="math inline">\(T\to\infty\)</span> and using the fact that
<span class="math display">\[
\lim_{T \to \infty}\int_0^T \frac{\sin ((x-a)\theta)}{\theta} d\theta =
\begin{cases}
    \frac{-\pi}{2} \,, &amp; x &lt; a \,,\\
    \frac{\pi}{2} \,, &amp; x &gt; a \,, \\
    0 \,, &amp; x = a \,.
\end{cases}\]</span>
Therefore,
<span class="math display">\[
\begin{aligned}
\lim_{T\to \infty}\frac{1}{2\pi}
\int_{\mathbb{R}}\int_{-T}^T \frac{ e^{ - i \theta a} - e^{- i \theta b}}{i\theta} \phi_X(\theta) \, d\theta dx
&amp;=
\left(\int_{(a,\infty)} f_X(x) \, dx  - \int_{ (-\infty, a]} f_X(x) \, dx \right)\\
&amp; \quad -
\left(\int_{(b,\infty)} f_X(x) \, dx  - \int_{ (-\infty, b]} f_X(x) \, dx \right) \\
&amp;= (\mathbb{P}(X&gt;a) - \mathbb{P}(X\leq a)) - (\mathbb{P}(X&gt;b) - \mathbb{P}(X\leq b)) \\
&amp;= \mathbb{P}( a &lt; X &lt; b) + \frac{1}{2} \left( \mathbb{P}(X = a) + \mathbb{P}(X = b)  \right),
\end{aligned}
\]</span>
as desired.</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-43" class="exercise"><strong>Exercise 1.15  </strong></span>Verify that
<span class="math display">\[\lim_{T \to \infty} \int_0^\infty \frac{\sin(x)}{x} \, dx = \frac{\pi}{2}.\]</span>
If you can’t, watch this:
<a href="https://www.youtube.com/watch?v=Bq5TB6cZNng" class="uri">https://www.youtube.com/watch?v=Bq5TB6cZNng</a>.</p>
<p>Another way is to use contour integral from complex analysis.</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-44" class="exercise"><strong>Exercise 1.16  </strong></span>Let <span class="math inline">\(X_1, \dots, X_n \sim \mathrm{Uniform}(0,1)\)</span> be independent and <span class="math inline">\(Y_n = \max\{ X_1, \dots, X_n \}\)</span>.
Find <span class="math inline">\(\mathbb{E}(Y_n)\)</span>.</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-45" class="exercise"><strong>Exercise 1.17  </strong></span>Let <span class="math inline">\(X:\Omega \to (0,\infty)\)</span> be continuous positive RV. Suppose <span class="math inline">\(\mathbb{E}(X)\)</span> exist.
Show
<span class="math inline">\(\mathbb{E}(X) = \int_0^\infty \mathbb{P}(X &gt; x) \, dx\)</span>.
(Hint: Fubini. This is called the layer cake theorem).</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-46" class="exercise"><strong>Exercise 1.18  </strong></span>The exponential distribution with parameter <span class="math inline">\(\lambda\)</span> (denoted by <span class="math inline">\(\exp(\lambda)\)</span>)
is used to model waiting time (see <a href="https://en.wikipedia.org/wiki/Exponential_distribution" class="uri">https://en.wikipedia.org/wiki/Exponential_distribution</a>).
The probability density function of the exponential distribution is given by
<span class="math display">\[f(x) = \begin{cases} \lambda e^{-\lambda x} &amp; x\geq 0 \\ 0 &amp; x&lt; 0 \end{cases}.\]</span></p>
<ol style="list-style-type: decimal">
<li><p>Find the moment-generating function of <span class="math inline">\(X \sim \exp(\lambda)\)</span>.</p></li>
<li><p>Use moment-generating function to show that if <span class="math inline">\(X\)</span> is exponential distributed,
then so is <span class="math inline">\(cX\)</span>.</p></li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-47" class="exercise"><strong>Exercise 1.19  </strong></span>Let <span class="math inline">\(X \sim N(\mu_1, \sigma_1)\)</span> and <span class="math inline">\(Y \sim (\mu_2, \sigma_2)\)</span> be independent.
Use the moment generating function to show that <span class="math inline">\(Z = c_1 X + c_2 Y\)</span> is again a normal distribution.
What are <span class="math inline">\(\mathbb{E}(Z)\)</span> and <span class="math inline">\(\mathbb{V}(Z)\)</span>?</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-48" class="exercise"><strong>Exercise 1.20  </strong></span>Find the moment-generating function of a Bernoulli RV, and use it to find
the mean, variance, and third moment.</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-49" class="exercise"><strong>Exercise 1.21  </strong></span>Let <span class="math inline">\(X: \Omega \to S\)</span> be a RV and <span class="math inline">\(S = \mathbb{N}\)</span>.
The <em>probability generating function</em> of <span class="math inline">\(X\)</span> is defined to be
<span class="math display">\[ G(s) = \sum_{k=1}^\infty s^k \mathbb{P}(X = k). \]</span></p>
<ol style="list-style-type: lower-alpha">
<li><p>Show that
<span class="math display">\[ \mathbb{P}( X = k) = \frac{1}{k!} \frac{d^k}{ds^k} G(s) \vert_{s=0} \]</span></p></li>
<li><p>Show that
<span class="math display">\[ \frac{dG}{ds} \vert_{s=1} = \mathbb{E}(X) \]</span>
and
<span class="math display">\[ \frac{d^2G}{ds^2} \vert_{s=1} = \mathbb{E}[X(X-1)]. \]</span></p></li>
<li><p>Express the probability-generating function in terms of moment-generating function.</p></li>
<li><p>Find the probability-generating function of the Poisson distribution.</p></li>
</ol>
</div>
</div>
</div>
<div id="inequalities" class="section level2 hasAnchor" number="1.3">
<h2><span class="header-section-number">1.3</span> Inequalities<a href="probability.html#inequalities" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="typical-tail-bound-inequalities" class="section level3 hasAnchor" number="1.3.1">
<h3><span class="header-section-number">1.3.1</span> Typical tail bound inequalities<a href="probability.html#typical-tail-bound-inequalities" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="theorem">
<p><span id="thm:unlabeled-div-50" class="theorem"><strong>Theorem 1.8  (Markov's Inequality) </strong></span>Let <span class="math inline">\(X\)</span> be a non-negative RV and <span class="math inline">\(\mathbb{E}(X)\)</span> exists.
Then, for each <span class="math inline">\(k &gt;0\)</span>,
<span class="math display">\[ \mathbb{P}( X &gt; k) \leq \frac{\mathbb{E}(X)}{k}. \]</span></p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-51" class="theorem"><strong>Theorem 1.9  (Chebyshev's Inequality) </strong></span>Let <span class="math inline">\(X\)</span> be a RV such with expected value <span class="math inline">\(\mu\)</span> and standard variation <span class="math inline">\(\sigma\)</span>.
Then for each <span class="math inline">\(k &gt;0\)</span>,
<span class="math display">\[ \mathbb{P}(| X - \mu | &gt; k \sigma ) \leq \frac{1}{k^2}. \]</span></p>
</div>
</div>
<div id="exponential-concentration-inequalities" class="section level3 hasAnchor" number="1.3.2">
<h3><span class="header-section-number">1.3.2</span> Exponential concentration inequalities<a href="probability.html#exponential-concentration-inequalities" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="theorem">
<p><span id="thm:unlabeled-div-52" class="theorem"><strong>Theorem 1.10  (Mill's inequality) </strong></span>Let <span class="math inline">\(Z \sim N(0,1)\)</span>. Then,
for each <span class="math inline">\(t &gt;0\)</span>,
<span class="math display">\[ \mathbb{P}(|Z| &gt; t) \leq \sqrt{\frac{2}{\pi}} \frac{e^{-t^2/2}}{t}. \]</span></p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-53" class="theorem"><strong>Theorem 1.11  (Hoeffding's inequality) </strong></span>Let <span class="math inline">\(X_1, \dots, X_n\)</span> be independent RVs such that
<span class="math inline">\(\mathbb{E}( X_i ) = 0\)</span>, <span class="math inline">\(a_i \leq Y_i \leq b_i\)</span>.
For each <span class="math inline">\(\epsilon &gt;0\)</span> and <span class="math inline">\(t&gt;0\)</span>, we have
<span class="math display">\[ \mathbb{P}\left(  \sum_{i=1}^n X_i \geq \epsilon \right)
\leq e^{-t\epsilon} \prod_{i=1}^n e^{t^2(b_i - a_i)^2/8}. \]</span></p>
</div>
</div>
<div id="inequalities-for-expectations" class="section level3 hasAnchor" number="1.3.3">
<h3><span class="header-section-number">1.3.3</span> Inequalities for expectations<a href="probability.html#inequalities-for-expectations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="theorem">
<p><span id="thm:unlabeled-div-54" class="theorem"><strong>Theorem 1.12  (Cauchy-Schwartz inequality) </strong></span>Let <span class="math inline">\(X, Y\)</span> be RVs with finite variances. Then,
<span class="math display">\[\mathbb{E}( |XY| ) \leq \sqrt{ \mathbb{E}(X^2) \mathbb{E}(Y^2) }.\]</span></p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-55" class="theorem"><strong>Theorem 1.13  (Jensen's inequality) </strong></span>Suppose <span class="math inline">\(g:\mathbb{R}\to \mathbb{R}\)</span> is a convex function.
Then
<span class="math display">\[ \mathbb{E}g(X) \geq g(\mathbb{E}X). \]</span></p>
</div>
</div>
</div>
<div id="law-of-large-numbers" class="section level2 hasAnchor" number="1.4">
<h2><span class="header-section-number">1.4</span> Law of Large Numbers<a href="probability.html#law-of-large-numbers" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="theorem">
<p><span id="thm:unlabeled-div-56" class="theorem"><strong>Theorem 1.14  </strong></span>Let <span class="math inline">\(X_i\)</span>, <span class="math inline">\(i\in \mathbb{N}\)</span> be independent RVs such that <span class="math inline">\(\mathbb{E}(X_i) = \mu\)</span>
and <span class="math inline">\(\mathbb{V}(X_i) = \sigma^2\)</span>.
Then, for each <span class="math inline">\(\epsilon &gt; 0\)</span>,
<span class="math display">\[ \lim_{n\to \infty} \mathbb{P}\left( \left| \frac{1}{n} \sum_{i=1}^n X_i - \mu  \right| &gt; \epsilon   \right) = 0 \,.\]</span></p>
</div>
<p>The above kind of convergence is sometimes called <em>convergence in probability</em>.
There are other modes of convergence such as convergence almost surely and
uniform convergence.</p>
</div>
<div id="central-limit-theorem" class="section level2 hasAnchor" number="1.5">
<h2><span class="header-section-number">1.5</span> Central Limit Theorem<a href="probability.html#central-limit-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="definition">
<p><span id="def:unlabeled-div-57" class="definition"><strong>Definition 1.17  (Convergence in distribution) </strong></span>Let <span class="math inline">\(\{X_i\}_{i \in \mathbb{N}}\)</span> be a sequence of RVs with CDF <span class="math inline">\(F_i\)</span>.
Let <span class="math inline">\(X\)</span> be a RV with CDF <span class="math inline">\(F\)</span>.
We say that <span class="math inline">\(X_n\)</span> convergence to <span class="math inline">\(X\)</span> in distribution if
<span class="math display">\[ \lim_{n\to \infty} F_n (x) = F(x) \]</span> at every point at which <span class="math inline">\(F\)</span> is continuous.</p>
</div>
<div class="theorem">
<p><span id="thm:cont" class="theorem"><strong>Theorem 1.15  (Continuity theorems) </strong></span>Let <span class="math inline">\(X_i\)</span>, <span class="math inline">\(i \in \mathbb{N}\)</span> RVs with CDF <span class="math inline">\(F_i\)</span> and <span class="math inline">\(X\)</span> a RV with CDF <span class="math inline">\(\bar F\)</span>.
Suppose that either</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\varphi_{X_n}(s)\)</span> converges to <span class="math inline">\(\varphi_X(s)\)</span> for all <span class="math inline">\(s\)</span> in some open interval around <span class="math inline">\(0\)</span>.</p></li>
<li><p><span class="math inline">\(\lim_{n\to\infty} \phi_{X_n}(s) = \phi_X(s)\)</span> for every <span class="math inline">\(s \in \mathbb{R}\)</span>.</p>
<p>Then <span class="math inline">\(X_n \to X\)</span> in distribution.</p></li>
</ol>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-58" class="theorem"><strong>Theorem 1.16  (Central limit theorem) </strong></span>Let <span class="math inline">\(\{ X_i \}_{i\in \mathbb{N}}\)</span> be a sequence of IID RVs with mean <span class="math inline">\(0\)</span> and
variance <span class="math inline">\(\sigma^2 &lt; \infty\)</span>.
Define
<span class="math display">\[ Z_n = \frac{\sum_{i=1}^n X_i}{\sigma \sqrt{n}}. \]</span>
Then <span class="math inline">\(Z_n\)</span> converges to <span class="math inline">\(Z \sim N(0,1)\)</span> in distribution.</p>
</div>
<p>There are a few ways to go about proving this theorem.
Two most common ways employ the MGF and the characteristic function.
Both methods rely on the one crucial idea of using the Taylor expansion,
which we will see shortly.
We present the proof using MGF (adopted from Rice’s book) and leave it to the reader the proof using characteristic function.</p>
<div class="proof">
<p><span id="unlabeled-div-59" class="proof"><em>Proof</em>. </span>For each <span class="math inline">\(n \in \mathbb{N}\)</span>, we have that
<span class="math display">\[ \varphi_{Z_n} (t) = \left( \varphi_{X_1} \left( \frac{t}{\sigma \sqrt{n}} \right) \right)^n. \]</span></p>
<p>Observe first that
<span class="math inline">\(\varphi_{X_1}&#39;(0) = \mathbb{E}X_1 = 0\)</span> and <span class="math inline">\(\varphi_{X_1}&#39;&#39;(0) = \mathbb{E}X_1^2 = \sigma^2\)</span>.
So, performing Taylor expansion for <span class="math inline">\(\varphi_{X_1}\)</span>, we get
<span class="math display">\[
\begin{aligned}
\varphi_{X_1} \left( \frac{t}{\sigma \sqrt{n}} \right)  
        &amp;=  1 + \varphi_{X_1}&#39;(0) \left( \frac{t}{\sigma \sqrt{n}} \right)
           + \frac{1}{2} \varphi_{X_1}&#39;&#39;(0) \left( \frac{t}{\sigma \sqrt{n}} \right)^2 + \epsilon_n  \\
        &amp; = 1 + \frac{1}{2} \sigma^2 \left( \frac{t}{\sigma \sqrt{n}} \right)^2 + \epsilon_n \\
        &amp; = 1 + \frac{1}{2} \left( \frac{t^2}{n} \right) + \epsilon_n \,.
\end{aligned}\]</span>
where <span class="math inline">\(\epsilon_n / (t^2 / (n\sigma^2)) \to 0\)</span> as <span class="math inline">\(n \to \infty\)</span>.
It can then be shown that
<span class="math display">\[ \lim_{n\to \infty} \varphi_{Z_n}(t) = \lim_{n\to \infty} \left( 1 + \frac{1}{2} \left( \frac{t^2}{n} \right) + \epsilon_n  \right) = e^{t^2/2} = \varphi_Z .\]</span>
Combine this with Theorem <a href="probability.html#thm:cont">1.15</a>, we arrive at our result.</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-60" class="exercise"><strong>Exercise 1.22  </strong></span>Prove the central limit theorem using the characteristic function.</p>
</div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="part-1-background.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="part-2-inference.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["MathStat.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
