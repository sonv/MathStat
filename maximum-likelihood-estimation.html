<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3.4 Maximum Likelihood Estimation | MATH 310: Mathematical Statistics (brief notes)</title>
  <meta name="description" content="3.4 Maximum Likelihood Estimation | MATH 310: Mathematical Statistics (brief notes)" />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="3.4 Maximum Likelihood Estimation | MATH 310: Mathematical Statistics (brief notes)" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3.4 Maximum Likelihood Estimation | MATH 310: Mathematical Statistics (brief notes)" />
  
  
  

<meta name="author" content="Truong-Son Van" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="method-of-moments.html"/>
<link rel="next" href="optional-expectation-maximization-algorithm.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>




<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a>Mathematical Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Disclaimer</a></li>
<li class="chapter" data-level="" data-path="syllabus.html"><a href="syllabus.html"><i class="fa fa-check"></i>Syllabus</a>
<ul>
<li class="chapter" data-level="" data-path="syllabus.html"><a href="syllabus.html#key-information"><i class="fa fa-check"></i>Key information</a></li>
<li class="chapter" data-level="" data-path="textbooks-and-references.html"><a href="textbooks-and-references.html"><i class="fa fa-check"></i>Textbooks and references</a>
<ul>
<li class="chapter" data-level="" data-path="textbooks-and-references.html"><a href="textbooks-and-references.html#course-description"><i class="fa fa-check"></i>Course description</a></li>
<li class="chapter" data-level="" data-path="textbooks-and-references.html"><a href="textbooks-and-references.html#learning-objectives"><i class="fa fa-check"></i>Learning objectives</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="assessment.html"><a href="assessment.html"><i class="fa fa-check"></i>Assessment</a></li>
<li class="chapter" data-level="" data-path="core-content.html"><a href="core-content.html"><i class="fa fa-check"></i>Core content</a></li>
<li class="chapter" data-level="" data-path="project-description.html"><a href="project-description.html"><i class="fa fa-check"></i>Project description</a>
<ul>
<li class="chapter" data-level="" data-path="project-description.html"><a href="project-description.html#topics"><i class="fa fa-check"></i>Topics</a></li>
<li class="chapter" data-level="" data-path="project-description.html"><a href="project-description.html#deadlines"><i class="fa fa-check"></i>Deadlines</a></li>
<li class="chapter" data-level="" data-path="project-description.html"><a href="project-description.html#project-guidelines"><i class="fa fa-check"></i>Project Guidelines</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="late-assignments.html"><a href="late-assignments.html"><i class="fa fa-check"></i>Late assignments</a></li>
<li class="chapter" data-level="" data-path="time-expectations.html"><a href="time-expectations.html"><i class="fa fa-check"></i>Time expectations</a>
<ul>
<li class="chapter" data-level="" data-path="time-expectations.html"><a href="time-expectations.html#collaboration-plagiarism"><i class="fa fa-check"></i>Collaboration &amp; Plagiarism</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="learning-support.html"><a href="learning-support.html"><i class="fa fa-check"></i>Learning Support</a></li>
<li class="chapter" data-level="" data-path="wellbeing.html"><a href="wellbeing.html"><i class="fa fa-check"></i>Wellbeing</a></li>
<li class="chapter" data-level="" data-path="tentative-course-schedule.html"><a href="tentative-course-schedule.html"><i class="fa fa-check"></i>Tentative Course Schedule</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-1-background.html"><a href="part-1-background.html"><i class="fa fa-check"></i>PART 1: Background</a></li>
<li class="chapter" data-level="1" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>1</b> Probability</a>
<ul>
<li class="chapter" data-level="1.1" data-path="review.html"><a href="review.html"><i class="fa fa-check"></i><b>1.1</b> Review</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="review.html"><a href="review.html#probability-space"><i class="fa fa-check"></i><b>1.1.1</b> Probability Space</a></li>
<li class="chapter" data-level="1.1.2" data-path="review.html"><a href="review.html#random-variables"><i class="fa fa-check"></i><b>1.1.2</b> Random Variables</a></li>
<li class="chapter" data-level="1.1.3" data-path="review.html"><a href="review.html#joint-distribution-of-rvs"><i class="fa fa-check"></i><b>1.1.3</b> Joint distribution of RVs</a></li>
<li class="chapter" data-level="1.1.4" data-path="review.html"><a href="review.html#some-important-random-variables"><i class="fa fa-check"></i><b>1.1.4</b> Some important random variables</a></li>
<li class="chapter" data-level="1.1.5" data-path="review.html"><a href="review.html#independent-random-variables"><i class="fa fa-check"></i><b>1.1.5</b> Independent random variables</a></li>
<li class="chapter" data-level="1.1.6" data-path="review.html"><a href="review.html#transformations-of-rvs"><i class="fa fa-check"></i><b>1.1.6</b> Transformations of RVs</a></li>
<li class="chapter" data-level="1.1.7" data-path="review.html"><a href="review.html#expectation"><i class="fa fa-check"></i><b>1.1.7</b> Expectation</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="moment-generating-and-characteristic-functions.html"><a href="moment-generating-and-characteristic-functions.html"><i class="fa fa-check"></i><b>1.2</b> Moment Generating and Characteristic Functions</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="moment-generating-and-characteristic-functions.html"><a href="moment-generating-and-characteristic-functions.html#moment-generating-functions"><i class="fa fa-check"></i><b>1.2.1</b> Moment Generating Functions</a></li>
<li class="chapter" data-level="1.2.2" data-path="moment-generating-and-characteristic-functions.html"><a href="moment-generating-and-characteristic-functions.html#characteristic-functions"><i class="fa fa-check"></i><b>1.2.2</b> Characteristic Functions</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="inequalities.html"><a href="inequalities.html"><i class="fa fa-check"></i><b>1.3</b> Inequalities</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="inequalities.html"><a href="inequalities.html#typical-tail-bound-inequalities"><i class="fa fa-check"></i><b>1.3.1</b> Typical tail bound inequalities</a></li>
<li class="chapter" data-level="1.3.2" data-path="inequalities.html"><a href="inequalities.html#exponential-concentration-inequalities"><i class="fa fa-check"></i><b>1.3.2</b> Exponential concentration inequalities</a></li>
<li class="chapter" data-level="1.3.3" data-path="inequalities.html"><a href="inequalities.html#inequalities-for-expectations"><i class="fa fa-check"></i><b>1.3.3</b> Inequalities for expectations</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="law-of-large-numbers.html"><a href="law-of-large-numbers.html"><i class="fa fa-check"></i><b>1.4</b> Law of Large Numbers</a></li>
<li class="chapter" data-level="1.5" data-path="central-limit-theorem.html"><a href="central-limit-theorem.html"><i class="fa fa-check"></i><b>1.5</b> Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-2-inference.html"><a href="part-2-inference.html"><i class="fa fa-check"></i>PART 2: Inference</a></li>
<li class="chapter" data-level="2" data-path="sampling.html"><a href="sampling.html"><i class="fa fa-check"></i><b>2</b> Sampling</a>
<ul>
<li class="chapter" data-level="2.1" data-path="simple-random-sample.html"><a href="simple-random-sample.html"><i class="fa fa-check"></i><b>2.1</b> Simple Random Sample</a></li>
<li class="chapter" data-level="2.2" data-path="standard-random-sample.html"><a href="standard-random-sample.html"><i class="fa fa-check"></i><b>2.2</b> Standard Random Sample</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="parametric-inference-parameter-estimation.html"><a href="parametric-inference-parameter-estimation.html"><i class="fa fa-check"></i><b>3</b> Parametric Inference (Parameter Estimation)</a>
<ul>
<li class="chapter" data-level="3.1" data-path="point-estimation.html"><a href="point-estimation.html"><i class="fa fa-check"></i><b>3.1</b> Point Estimation</a></li>
<li class="chapter" data-level="3.2" data-path="confidence-set.html"><a href="confidence-set.html"><i class="fa fa-check"></i><b>3.2</b> Confidence set</a></li>
<li class="chapter" data-level="3.3" data-path="method-of-moments.html"><a href="method-of-moments.html"><i class="fa fa-check"></i><b>3.3</b> Method of Moments</a></li>
<li class="chapter" data-level="3.4" data-path="maximum-likelihood-estimation.html"><a href="maximum-likelihood-estimation.html"><i class="fa fa-check"></i><b>3.4</b> Maximum Likelihood Estimation</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="maximum-likelihood-estimation.html"><a href="maximum-likelihood-estimation.html#consistency"><i class="fa fa-check"></i><b>3.4.1</b> Consistency</a></li>
<li class="chapter" data-level="3.4.2" data-path="maximum-likelihood-estimation.html"><a href="maximum-likelihood-estimation.html#asymptotic-normality"><i class="fa fa-check"></i><b>3.4.2</b> Asymptotic normality</a></li>
<li class="chapter" data-level="3.4.3" data-path="maximum-likelihood-estimation.html"><a href="maximum-likelihood-estimation.html#efficiency"><i class="fa fa-check"></i><b>3.4.3</b> Efficiency</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="optional-expectation-maximization-algorithm.html"><a href="optional-expectation-maximization-algorithm.html"><i class="fa fa-check"></i><b>3.5</b> (Optional) Expectation-Maximization Algorithm</a></li>
<li class="chapter" data-level="3.6" data-path="bayesian-approach.html"><a href="bayesian-approach.html"><i class="fa fa-check"></i><b>3.6</b> Bayesian Approach</a></li>
<li class="chapter" data-level="3.7" data-path="comparing-estimator-decision-theory.html"><a href="comparing-estimator-decision-theory.html"><i class="fa fa-check"></i><b>3.7</b> Comparing Estimator / Decision Theory</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>4</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="4.1" data-path="procedure.html"><a href="procedure.html"><i class="fa fa-check"></i><b>4.1</b> Procedure</a></li>
<li class="chapter" data-level="4.2" data-path="neyman-pearson-lemma.html"><a href="neyman-pearson-lemma.html"><i class="fa fa-check"></i><b>4.2</b> Neyman-Pearson Lemma</a></li>
<li class="chapter" data-level="4.3" data-path="uniformly-most-powerful-test.html"><a href="uniformly-most-powerful-test.html"><i class="fa fa-check"></i><b>4.3</b> Uniformly Most Powerful Test</a></li>
<li class="chapter" data-level="4.4" data-path="wald-test.html"><a href="wald-test.html"><i class="fa fa-check"></i><b>4.4</b> Wald Test</a></li>
<li class="chapter" data-level="4.5" data-path="likelihood-ratio-test.html"><a href="likelihood-ratio-test.html"><i class="fa fa-check"></i><b>4.5</b> Likelihood Ratio Test</a></li>
<li class="chapter" data-level="4.6" data-path="comparing-samples.html"><a href="comparing-samples.html"><i class="fa fa-check"></i><b>4.6</b> Comparing samples</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MATH 310: Mathematical Statistics (brief notes)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="maximum-likelihood-estimation" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Maximum Likelihood Estimation<a href="maximum-likelihood-estimation.html#maximum-likelihood-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="definition">
<p><span id="def:unlabeled-div-81" class="definition"><strong>Definition 3.7  </strong></span>Suppose <span class="math inline">\(X_1, \dots, X_n \sim f_\theta\)</span>.
The <em>likelihood</em> function is defined by
<span class="math display">\[ \mathcal{L}_n(\theta) = \prod_{i = 1}^n f (X_i; \theta) \,. \]</span>
The <em>log-likelihood function</em> is defined by
<span class="math display">\[ \ell_n (\theta) h = \ln \mathcal{L}_n (\theta) \,. \]</span></p>
<p>The <em>maximum likelihood estimator</em> MLE, denoted by <span class="math inline">\(\hat \theta_n\)</span>, is the value of
<span class="math inline">\(\theta\)</span> that maximizes <span class="math inline">\(\mathcal{L}_n(\theta)\)</span>.</p>
</div>
<p><strong>Notation.</strong> Another common notation for the likelihood function is
<span class="math display">\[ L(X|\theta) = \mathcal{L}_n(\theta).\]</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-82" class="example"><strong>Example 3.4  </strong></span>Let <span class="math inline">\(X_1, \dots, X_n\)</span> be sample from <span class="math inline">\(\mathrm{Bernoulli}(p)\)</span>.
Use MLE to find an estimator for <span class="math inline">\(p\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-83" class="example"><strong>Example 3.5  </strong></span>Let <span class="math inline">\(X_1, \dots, X_n\)</span> be sample from <span class="math inline">\(N(\theta, 1)\)</span>.
Use MLE to find an estimator for <span class="math inline">\(\theta\)</span>.</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-84" class="exercise"><strong>Exercise 3.6  </strong></span>Let <span class="math inline">\(X_1, \dots, X_n\)</span> be sample from <span class="math inline">\(Uniform([0,\theta])\)</span>, where <span class="math inline">\(\theta &gt;0\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p>Find the MLE for <span class="math inline">\(\theta\)</span>.</p></li>
<li><p>Find an estimator by the method of moments.</p></li>
<li><p>Compute the mean and the variance of the two estimators above.</p></li>
<li><p>Can you find the MLE if we consider <span class="math inline">\(Uniform((0,\theta))\)</span>?</p></li>
</ol>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-85" class="theorem"><strong>Theorem 3.4  </strong></span>Let <span class="math inline">\(\tau = g(\theta)\)</span> be a bijective function of <span class="math inline">\(\theta\)</span>.
Suppose that <span class="math inline">\(\hat \theta_n\)</span> is the MLE of <span class="math inline">\(\theta\)</span>.
Then <span class="math inline">\(\hat \tau_n = g(\hat \theta_n)\)</span> is the MLE of <span class="math inline">\(\tau\)</span>.</p>
</div>
<div id="consistency" class="section level3 hasAnchor" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> Consistency<a href="maximum-likelihood-estimation.html#consistency" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="example">
<p><span id="exm:unlabeled-div-86" class="example"><strong>Example 3.6  (Inconsistency of MLE) </strong></span>Let <span class="math inline">\(Y_{i,1}, Y_{i,2} \sim N(\mu_1, \sigma^2)\)</span>. Our goal is to find MLE for <span class="math inline">\(\sigma^2\)</span>, which
turns out to be
<span class="math display">\[\hat \sigma^2 = \frac{1}{4n} \sum_{i=1}^n (Y_{i,1} - Y_{i,2})^2.\]</span>
By law of large number, this will converge to
<span class="math display">\[\mathbb{E}(\hat \sigma^2) = \sigma^2/2,\]</span>
which means that the MLE is not consistent.</p>
</div>
<p>To discuss about the consistency of the MLE, we define the
Kullback-Leibler distance between two pdf <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span>.</p>
<p><span class="math display">\[ D(f,g) = \int f(x) \ln \left( \frac{f(x)}{g(x)} \right) \, dx.\]</span></p>
<p>Abusing notation, we will write
<span class="math inline">\(D(\theta, \varphi)\)</span> to mean <span class="math inline">\(D(f(x;\theta), f(x;\varphi))\)</span>.</p>
<p>We say that a model <span class="math inline">\(\mathcal{F}\)</span> is <em>identifiable</em> if <span class="math inline">\(\theta \not= \varphi\)</span> implies
<span class="math inline">\(D(\theta, \varphi) &gt; 0\)</span>.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-87" class="theorem"><strong>Theorem 3.5  </strong></span>Let <span class="math inline">\(\theta_{\star}\)</span> denote the true value of <span class="math inline">\(\theta\)</span>. Define
<span class="math display">\[
M_n(\theta)=\frac{1}{n} \sum_i \log \frac{f\left(X_i ; \theta\right)}{f\left(X_i ; \theta_{\star}\right)}
\]</span>
and <span class="math inline">\(M(\theta)=-D\left(\theta_{\star}, \theta\right)\)</span>. Suppose that
<span class="math display">\[
\sup _{\theta \in \Theta}\left|M_n(\theta)-M(\theta)\right| \to 0
\]</span>
in probability
and that, for every <span class="math inline">\(\epsilon&gt;0\)</span>,
<span class="math display">\[
\sup _{\theta:|\theta-\theta,| \geq \epsilon} M(\theta)&lt;M\left(\theta_{\star}\right) .
\]</span></p>
<p>Let <span class="math inline">\(\widehat{\theta}_n\)</span> denote the MLE. Then <span class="math inline">\(\widehat{\theta}_n \to \theta_{\star}\)</span> in probability.</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-88" class="exercise"><strong>Exercise 3.7  </strong></span>Let <span class="math inline">\(X_1, \ldots, X_n\)</span> be a random sample from a distribution with density:
<span class="math display">\[ p(x; \theta) = \theta x^{-2}, \quad 0 &lt; \theta \leq x &lt; \infty. \]</span></p>
<ol style="list-style-type: decimal">
<li><p>Find the MLE for <span class="math inline">\(\theta\)</span>.</p></li>
<li><p>Find the Method of Moments estimator for <span class="math inline">\(\theta\)</span>.</p></li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-89" class="exercise"><strong>Exercise 3.8  </strong></span>Let <span class="math inline">\(X_1, \ldots, X_n \sim \text{Poisson}(\lambda)\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p>Find the method of moments estimator, the maximum likelihood estimator, and the Fisher information <span class="math inline">\(I(\lambda)\)</span>.</p></li>
<li><p>Use the fact that the mean and variance of the Poisson distribution are both <span class="math inline">\(\lambda\)</span> to propose two unbiased estimators of <span class="math inline">\(\lambda\)</span>.</p></li>
<li><p>Show that one of these estimators has a larger variance than the other.</p></li>
</ol>
</div>
<p>The conditions listed in the above theorem is not very easy to check.
Hogg-McKean-Craig has a better theorem (this is a good theorem to read).</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-90" class="theorem"><strong>Theorem 3.6  </strong></span>Assume that</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\theta \not = \theta&#39; \implies f_\theta \not = f_{\theta&#39;}\)</span></p></li>
<li><p><span class="math inline">\(f_\theta\)</span> has common support for all <span class="math inline">\(\theta\)</span></p></li>
<li><p><span class="math inline">\(\theta^*\)</span> is an interior point in <span class="math inline">\(\Omega\)</span></p>
<p>If <span class="math inline">\(f_\theta(x)\)</span> is differentiable with respect to <span class="math inline">\(\theta\)</span>.
Then the likelihood equation
<span class="math display">\[ \frac{\partial}{\partial \theta}  l_n(\theta) = 0 \]</span>
has a solution <span class="math inline">\(\hat \theta_n\)</span> such that
<span class="math display">\[\lim_{n\to \infty} \hat \theta_n \to \theta^*\]</span> in distribution.</p></li>
</ol>
</div>
</div>
<div id="asymptotic-normality" class="section level3 hasAnchor" number="3.4.2">
<h3><span class="header-section-number">3.4.2</span> Asymptotic normality<a href="maximum-likelihood-estimation.html#asymptotic-normality" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="definition">
<p><span id="def:unlabeled-div-91" class="definition"><strong>Definition 3.8  </strong></span>Given a RV <span class="math inline">\(X\)</span>.
The score function is defined to be
<span class="math display">\[ s (X;\theta) = \frac{\partial \log f(X; \theta)}{\partial \theta} .\]</span></p>
<p>The <em>Fisher information</em> is defined to be
<span class="math display">\[ I_n = \mathbb{V}_\theta \left( \sum_{i=1}^n s(X_i; \theta)  \right) = \sum_{i=1}^n \mathbb{V}_\theta \left( s(X_i; \theta)  \right).\]</span></p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-92" class="theorem"><strong>Theorem 3.7  </strong></span><span class="math inline">\(I_n (\theta) = n I(\theta)\)</span>.
Furthermore,
<span class="math display">\[ I(\theta) = \mathbb{E}_\theta \left( \frac{\partial^2 \log(f(X;\theta))}{\partial \theta^2} \right).\]</span></p>
</div>
<p>The significance of this is that you can think of
the Fisher information as the curvature (second derivative) on the “manifold”
of parameters.
So, error of the score function has certain geometric interpretation.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-93" class="theorem"><strong>Theorem 3.8  </strong></span>Let <span class="math inline">\(\mathrm{se} = \sqrt{\mathbb{V}(\hat \theta_n)}\)</span>.
Given some regularity conditions,
there exists a random variable <span class="math inline">\(Z \sim N(0,1)\)</span> such that
<span class="math display">\[\frac{\hat\theta_n - \theta}{\mathrm{se}} \to Z.\]</span></p>
</div>
</div>
<div id="efficiency" class="section level3 hasAnchor" number="3.4.3">
<h3><span class="header-section-number">3.4.3</span> Efficiency<a href="maximum-likelihood-estimation.html#efficiency" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As <span class="math inline">\(n\)</span> gets large, the MLE is the most efficient estimator.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-94" class="theorem"><strong>Theorem 3.9  (Cramer-Rao Inequality) </strong></span>Let <span class="math inline">\(X_1, \dots, X_n\)</span> be sample with density <span class="math inline">\(f(x;\theta)\)</span>.
Suppose <span class="math inline">\(\theta&#39;\)</span> is an unbiased estimator of <span class="math inline">\(\theta\)</span>, then
under similar regularity conditions as in asymptotic normality,
<span class="math display">\[ \mathbb{V}(\theta&#39;_n) \geq \frac{1}{n I(\theta)}.\]</span></p>
</div>
<p>Note that, in the proof of asymptotic normality, we have that
as <span class="math inline">\(n\)</span> gets large, the MLE <span class="math inline">\(\hat \theta\)</span>, if obeys the required regularity conditions, satisfies
<span class="math inline">\(\mathbb{V}( \hat \theta_n )\sim \frac{1}{n I(\theta)}\)</span>.</p>
<p>By consistency, <span class="math inline">\(\hat \theta \sim \theta\)</span> when <span class="math inline">\(n\)</span> is very large.
This means that
<span class="math display">\[\mathbb{V}( \hat \theta_n - \theta ) \sim\mathbb{V}( \hat \theta_n )\sim \frac{1}{n I(\theta)}.\]</span></p>
<div class="corollary">
<p><span id="cor:unlabeled-div-95" class="corollary"><strong>Corollary 3.1  </strong></span>Let <span class="math inline">\(X_1, \dots, X_n\)</span> be sample with density <span class="math inline">\(f(x;\theta)\)</span>.
Suppose <span class="math inline">\(\theta&#39;_n\)</span> is an unbiased estimator of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\hat \theta_n\)</span> the MLE of <span class="math inline">\(\theta\)</span>, then, under regularity condition as in asymptotic normality, we have
<span class="math display">\[ \lim_{n\to \infty} n \mathbb{V}(\theta&#39;_n) \geq \lim_{n\to \infty} n\mathbb{V}(\hat \theta_n - \theta) .\]</span></p>
</div>
<p>Note that this doesn’t say that MLE (if consistent) is the most efficient for any finite <span class="math inline">\(n\)</span>.
In fact, this is a difficult question and one can only verify it for some
specific estimator.</p>
<div class="exercise">
<p><span id="exr:unlabeled-div-96" class="exercise"><strong>Exercise 3.9  </strong></span>Show that for Poisson processes, the MLE <span class="math inline">\(\hat \theta_n\)</span> is the most efficient for every <span class="math inline">\(n\)</span>, compared to any other unbiased
estimator <span class="math inline">\(\theta&#39;_n\)</span>, i.e,
<span class="math display">\[\mathbb{V}(\theta&#39;_n) \geq \mathbb{V}(\hat\theta - \theta)\]</span> for every <span class="math inline">\(n\in \mathbb{N}\)</span>.</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-97" class="exercise"><strong>Exercise 3.10  (Rice, 8.10.6) </strong></span>Let <span class="math inline">\(X \sim \mathrm{Binomial}(n,p)\)</span> .</p>
<ol style="list-style-type: lower-alpha">
<li><p>Find the MLE of <span class="math inline">\(p\)</span>.</p></li>
<li><p>Show that the MLE from part (a) attains the Cramer-Rao lower bound.</p></li>
</ol>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="method-of-moments.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="optional-expectation-maximization-algorithm.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["MathStat.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
