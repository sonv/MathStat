[["index.html", "MATH 310: Mathematical Statistics (brief notes) Disclaimer", " MATH 310: Mathematical Statistics (brief notes) Truong-Son Van Disclaimer This is class notes for Mathematical Statistics at Fublbright University Vietnam. I claim no mathematiacal originality in this work as it is mostly taken from the reference books. The only original contribution of mine are typos and errors. "],["syllabus.html", "Syllabus", " Syllabus Key information Instructor: Truong-Son Van Email: son.van+310@fulbright.edu.vn Class time: T &amp; Th: 3:00 PM - 4:30 PM Class Location: BRDWY Music room Office hours: M &amp; W: 3-4PM, T &amp; Th: 10-11AM Prerequisites: Multivariable Calculus (MATH 104) and Probability (MATH 205) Midterm: Final: "],["textbooks-and-references.html", "Textbooks and references", " Textbooks and references John Rice, Mathematical Statistics &amp; Data Analysis, 3rd Edition (main reference) Cassella and Berger, Statistical Inference, 2nd Edition Larry Wasserman, All of Statistics Course description Learning objectives "],["assessment.html", "Assessment", " Assessment During the course, students are expected to compute their own percentage points based on the following scheme. The instructor is not responsible for providing the running percentage. Form of assessment Weight Weekly homeworks 40% Mini-project 15% Midterm 20% Final 25% The following is the letter grade breakdown. It is based on common practice in the United States. Letter Grade Percentage A [93,100] A- [90,93) B+ [87,90) B [83,87) B- [80, 83) C+ [77,80) C [73,77) C- [70,73) D+ [67,70) D [60, 66) F [0,60) "],["core-content.html", "Core content", " Core content Probability Review (2 weeks) Random Variables Concentration inequality (non-asymptotic theory) Limit Theorems (asymptotic theory) Special distributions Sampling distribution, confidence interval (Rice, 6.3, 7.1 – 7.3) (1 week) Parameter estimation &amp; Method of moments (Rice, 8.1 – 8.4) (1 week) Maximum likelihood estimation (Rice, 8.5) (1 week) Expection-Maximization Algorithm (Wasserman, 9.13.4) (1 week) Bayesian approach to parameter estimation (Wasserman, 11) (1 week) Unbiased estimators, Efficiency &amp; Cramer-Rao Inequality (Casella &amp; Berger, 7.3.2, Rice, 8.7) (1 week) Sufficiency and unbiasedness, Rao-Blackwell Theorem (Casella &amp; Berger, 7.3.3, Rice, 8.8) (1 week) Hypothesis testings, Neyman-Pearson Lemma, Wald test, Likelihood Ratio test (Rice, 9) (2 weeks) Comparing samples (Rice, 11) (1 week) Analysis of Variance (Rice, 12) (1 week) Linear regression and least squares (Wasserman, 13.1-13.3, Casella &amp; Berger, 11.3) (1 week) "],["project-description.html", "Project description", " Project description The project is a chance for you to learn things that we don’t have time to discuss in class but are still key to the study of mathematical statistics. The main goal is for you to explore on your own, with the equipped language learned from this class. The only rule is that each team can have at most TWO students. The project will have two components: Written report: students are expected to write a 5-10 page report about the chosen subject, written in LaTeX or similar mathematical writing tools such as LyX. Presentation: 20-minute presentations will be delivered by the end of the semester. Students are expected to write their own powerpoint/beamer slides to discuss what they learn. The expectation is that it should be understandable by anyone who has taken the class. Topics Students can pick the topics in the book of Wasserman from chapter 17 - chapter 24 or choose one of the liking (with the approval of the instructor) if they see interesting ideas outside of class. Deadlines Topic discussion with instructor: Friday, March 15, 2024 First draft: Friday, April 19, 2024 Presentation: TBD Project Guidelines You may work alone or in team of two. You need to discuss with me a proposal for your project. The proposal should include your group members names &amp; IDs, a brief description of the content of your project (as clear and explicit as possible), how you plan to present it, and a suggested grading rubric. You will need to deliver a 15 minutes presentation as well as a 5-page-minimum write up in 1.5 spacing). "],["late-assignments.html", "Late assignments", " Late assignments 15% of the possible total mark will be deducted for every 24 hrs (or part of 24 hrs) after the deadline. Work more than 2 days late will not be accepted. Except for exceptional circumstances (see definition), I will not extend the deadlines. "],["time-expectations.html", "Time expectations", " Time expectations Some materials require time to be accustomed to. Some students are quicker than others. However, on average, you should expect 10-15 hours per week (including class time) on the materials in order to know the subject relatively well. Collaboration &amp; Plagiarism Plagiarism is the act of submitting the intellectual property of another person as your own. It is one of the most serious of academic offenses. Acts of plagiarism include, but are not limited to: Copying, or allowing someone to copy, all or a part of another person’s work and presenting it as your own, or not giving proper credit. Purchasing a paper from someone (or a website) and presenting it as your own work. Re-submitting your work from another course to fulfill a requirement in another course. Further details can be found in the Code of Academic Integrity [link]. "],["learning-support.html", "Learning Support", " Learning Support In addition to your course instructors, there are other resources available to support your academic work at Fulbright, including one-on-one consultations with learning support staff, supplementary workshops, and both individual and group tutoring and mentoring in course content, language learning, and academic skills. If you would like to request learning support, please contact Fulbright Learning Support (https://learning-support.notion.site). "],["wellbeing.html", "Wellbeing", " Wellbeing Mental health and wellbeing are essential for the success of your academic journey. The Fulbright Wellness Center provides various services including counseling, safer community, and accessibility services. If you are experiencing undue personal or academic stress, are feeling unsafe, or would like to know more about issues related to wellbeing, please contact the Wellness Center via wellness@fulbright.edu.vn or visit the Wellness Center office on Level 5 of the Crescent campus. For more information, pleaes check https://onestop.fulbright.edu.vn/s/article/Health-and-Wellness-Introduction "],["tentative-course-schedule.html", "Tentative Course Schedule", " Tentative Course Schedule Lecture Topic Date 1 Probability review 01/09 2 Probability review 01/11 3 Probability review 01/16 4 Concentration inequalities 01/18 5 Limit theorems 01/23 6 Limit theorems 01/25 7 Inequalities 01/30 8 Inequalities 02/01 Tet 02/05 - 02/23 9 Inference, sampling 02/26 10 Estimation 02/28 11 Constructing estimators (method of moments, MLE) 03/05 "],["part-1-background.html", "PART 1: Background", " PART 1: Background Readings: Chapters 1-5 of Wasserman Chapters 1-5 of Rice "],["probability.html", "Chapter 1 Probability", " Chapter 1 Probability “If we have an atom that is in an excited state and so is going to emit a photon, we cannot say when it will emit the photon. It has a certain amplitude to emit the photon at any time, and we can predict only a probability for emission; we cannot predict the future exactly.” — Richard Feynman "],["review.html", "1.1 Review", " 1.1 Review 1.1.1 Probability Space Definition 1.1 (Sigma-algebra) Let \\(\\Omega\\) be a set. A set \\(\\Sigma \\subseteq \\mathcal{P}(\\Omega)\\) of subsets of \\(\\Omega\\) is called a \\(\\sigma\\)-algebra of \\(\\Omega\\) if \\(\\Omega\\in \\Sigma\\) \\(F \\in \\Sigma \\implies F^C \\in \\Sigma\\) If \\(F_n \\in \\Sigma\\) for all \\(n\\in \\mathbb{N}\\), then \\[ \\bigcup_n F_n \\in \\Sigma .\\] It is extremely convenient to deal with things called open sets. The definition of those are a bit out of the scope of this class. However, in the case of the real line \\(\\mathbb{R}\\), open sets are defined to be made of by finite intersections and arbitrary unions of open intervals \\((a,b)\\). For example, \\((0,1)\\cup (2,3)\\) is an open set. Interestingly, \\(\\mathbb{R}\\) and \\(\\emptyset\\) are called clopen sets (here’s a funny YouTube video about clopen sets: https://www.youtube.com/watch?v=SyD4p8_y8Kw) A Borel \\(\\sigma\\)-algebra is the smallest \\(\\sigma\\)-algebra that contains all the open sets. We denote the Borel \\(\\sigma\\)-algebra of a set \\(\\Omega\\) to be \\(\\mathcal{B}(\\Omega)\\). This is a rather abstract definition. There is no clear way to construct a sigma algebra from a collection of sets. However, the construction is not important as the reassurance that this object does exist to give us nice domains to work with when we define a probability measure (see below definition). Exercise 1.1 (Challenging– not required but good for the brain) It turns out that if \\(\\Omega\\) is a discrete set, it is typical to have the set of open sets contain every set of singletons, i.e., the set \\(\\{ a \\}\\) is open for every \\(a\\in \\Omega\\). Take this as an assumption, show that for any discrete set \\(\\Omega\\), \\(\\mathcal{B}(\\Omega) = \\mathcal{P}(\\Omega)\\). What open sets really are is not important for now. The important thing is that for \\(\\mathbb{R}^n\\) open sets are made of open intervals/ open boxes. Your typical intuitions still work. Philosophically, the \\(\\sigma\\)-algebra represents the details of information we could have access to. There are certain events that are building blocks of knowledge and that we don’t have access to finer details. Think about the \\(\\sigma\\)-algebra as a consistent model of what can be known (observed). For example, you can never know what’s going on in the houses on the street unless you have been to them. But somehow, together, you are still able to piece all the information you have about the houses to make sense of the world. This is related to the problem of information. How much information is enough to be useful in certain situation?! To have a consistent system is not the same as to know everything. The system you see/invent can never be exhaustively true, but you can still say something about the reality if you can have a system that is consistent with what you observe. This is why we do sampling!! When you have a consistent model, you now want to encode the model in such a way that it helps you with describing/predict the reality you see. A way to do that with no full knowledge of anything is to assign the certain number to measure the chance for something to happen at a given time. This encoding needs to happen on the model you constructed. This leads to the following definition of probability space. Definition 1.2 (Probability Space) A Probability Space is a triple \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\), where \\(\\Omega\\) is a set called sample space, \\(\\mathcal{F}\\) is a \\(\\sigma\\)-algebra on \\(\\Omega\\), \\(\\mathbb{P}: \\mathcal{F}\\to [0,1]\\), called a Probability Measure, is a function that satisfies the following: \\(\\mathbb{P}(\\Omega) =1\\), If \\(F\\) is a disjoint union of \\(\\left\\{ F_n \\right\\}_{n=1}^\\infty\\), then \\[ \\mathbb{P}(F) = \\sum_{n=1}^\\infty \\mathbb{P}(F_n) . \\] Each element \\(\\omega \\in \\Omega\\) is called an outcome and each subset \\(A \\in \\mathcal{F}\\) is called an event. Definition 1.3 (Independent Events) Let \\(A, B \\in \\mathcal{F}\\) be events. We say that \\(A\\) and \\(B\\) are independent if \\[ \\mathbb{P}(A \\cap B) = \\mathbb{P}(A) \\mathbb{P}(B) \\,. \\] Definition 1.4 (Conditional Probability) Let \\(A, B \\in \\mathcal{F}\\) be events such that \\(\\mathbb{P}(B) &gt;0\\). Then, the conditional probability of \\(A\\) given \\(B\\) is \\[ \\mathbb{P}(A \\vert B) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)} \\,. \\] Theorem 1.1 (Bayes's Theorem) Let \\(A, B \\in \\mathcal{F}\\) be events such that \\(\\mathbb{P}(A)&gt;0\\) and \\(\\mathbb{P}(B) &gt;0\\). Then, \\[\\mathbb{P}(A | B) = \\frac{\\mathbb{P}(B | A) \\mathbb{P}(A)}{\\mathbb{P}(B)} \\,.\\] In modern statistics, there are names for the above terms: \\(\\mathbb{P}(A | B)\\) is called Posterior Probability, \\(\\mathbb{P}(B | A)\\) is called Likelihood, \\(\\mathbb{P}(A)\\) is called Prior Probability, \\(\\mathbb{P}(B)\\) is called Evidence. The theorem is often expressed in words as: \\[ \\text{Posterior Probability} = \\frac{\\text{Likelihood} \\times \\text{Prior Probability}}{\\text{Evidence}} \\] It is a good idea to ponder why those mathematical terms have those names. 1.1.2 Random Variables The notion of probability alone isn’t sufficient for us to describe ideas about the world. We need to have a notion of objects that associated with probabilities. This brings about the idea of random variable. Definition 1.5 (Random Variable) Let \\((\\Omega, \\mathcal{B}(\\Omega), \\mathbb{P})\\) be a probability space and \\((S, \\mathcal{B}(S))\\) a \\(\\sigma\\)-algebra. A random variable is a (Borel measurable) function from \\(\\Omega \\to S\\). \\(S\\) is called the state space of \\(X\\). In this course, we will restrict our attentions to two types of random variables: discrete and continuous. Definition 1.6 (Discrete RV) \\(X: \\Omega \\to S\\) is called a discrete RV if \\(S\\) is a countable set. A probability function or probability mass function for \\(X\\) is a function \\(f_X: S \\to [0,1]\\) defined by \\[ f_X(x) = \\mathbb{P}(X = x) \\,. \\] In contrast to the simplicity of discrete RV. Continuous RVs are a little bit messier to describe. This is because of the lack of background in measure theory so we can talk about this concept in a more precise way. Definition 1.7 (Continuous RV) A continuous random variable is a measurable function \\(X:\\Omega \\to S\\) is continuous if it satisfies the following conditions: \\(S = \\mathbb{R}^n\\) for some \\(n\\in \\mathbb{N}\\). There exists an (integrable) function \\(f_X\\) such that \\(f_X(x) \\geq 0\\) for all \\(x, \\int_{\\mathbb{R}^n} f_X(x) d x=1\\) and for every open cube \\(C \\subseteq \\mathbb{R}^n\\), \\[ \\mathbb{P}(X\\in C)=\\int_C f_X(x) dV . \\] The function \\(f_X\\) is called the probability density function (PDF). If two RVs \\(X\\) and \\(Y\\) share the same probability function, we say that they have the same distribution and denote them by \\[ X \\stackrel{d}{=}Y.\\] In this case we also say that \\(X\\) and \\(Y\\) are equal in distribution. Remark. To make the presentation more compact and clean, notationally, we will write \\[ \\int f(x) dx \\] to mean both integral (for continuous RV) and summation (for discrete RV). There are more general concepts of continuous RV where we don’t need to require \\(S\\) to be a Euclidean space as in the above definition. However, such concepts require the readers to be familiar with advanced subjects like Topology and Measure Theory. It is particularly important to know these two subjects in order to thoroughly understand Stochastic Processes. Exercise 1.2 Create a random variable that represents the results of \\(n\\) coin flips. For real-valued RV \\(X:\\Omega \\to \\mathbb{R}\\) we have the concept of cumulative distribution function. Definition 1.8 (Cumulative Distribution Function) Given a RV \\(X:\\Omega \\to \\mathbb{R}\\). The cumulative distribution function of \\(X\\) or CDF, is a function \\(F_X : \\mathbb{R}\\to [0,1]\\) defined by \\[ F_X (x) = \\mathbb{P}(X \\leq x) \\,. \\] Notationally, we use the notation \\(X\\sim F\\) to mean RV \\(X\\) with distribution \\(F\\). Exercise 1.3 Given a real-valued continuous RV \\(X: \\Omega \\to \\mathbb{R}\\), prove that if \\(f_X\\) is continuous then \\[ F_X(x)=\\int_{-\\infty}^x f_X(t) d t \\] is differentiable for every \\(x\\) and \\(f_X(x)=F_X^{\\prime}(x)\\). Exercise 1.4 Let \\(X\\) be an RV with CDF \\(F\\) and \\(Y\\) with CDF \\(G\\). Suppose \\(F(x) = G(x)\\) for all \\(x\\). Show that for every set \\(A\\) that is a countable union of open intervals, \\[ \\mathbb{P}(X \\in A) = \\mathbb{P}(Y \\in A) \\,.\\] Exercise 1.5 Let \\(X:\\Omega \\to \\mathbb{R}\\) be an RV and \\(F_X\\) be its CDF. Prove the following: \\(F\\) is non-decreasing: if \\(x_1 \\leq x_2\\), then \\(F(x_1) \\leq F(x_2)\\). \\(F\\) is normalized: \\[ \\lim_{x\\to -\\infty} F(x) = 0 \\,,\\] and \\[ \\lim_{x\\to \\infty} F(x) = 1 \\,.\\] \\(F\\) is right-continuous: \\[ F(x) = F(x+) = \\lim_{y \\searrow x} F(y) \\,.\\] 1.1.3 Joint distribution of RVs Let \\(X:\\Omega \\to S\\) and \\(Y: \\Omega \\to S\\) be RVs. We denote \\[ \\mathbb{P}(X \\in A; Y \\in B) = \\mathbb{P}(\\{X\\in A\\} \\cap \\{Y \\in B \\}) \\,. \\] For discrete RVs, the joint probability function of \\(X\\) and \\(Y\\) has the following meaning \\[f_{XY}(x,y) = \\mathbb{P}(X = x; Y = y)\\] For continuous RVs, the situations are more complicated as we can’t make sense of \\(\\mathbb{P}(X = x; Y = y)\\) (this is always 0 in most situation and in some other situation, one can’t even talk about it– this is a topic of more advanced course in measure theory). However, we can have \\[\\mathbb{P}(X \\in A; Y \\in B) = \\int_{X \\in A} \\int_{Y \\in B} f_{XY} (x,y) \\, dx dy \\,.\\] Another way to look at the above is the following. We can even consider \\(X: \\Omega \\to S^n\\), where \\(n\\geq 2\\). Instead of thinking about this as one RV, we can think about this as a vector of RVs: \\[ X = \\begin{pmatrix} X_1 \\\\ \\vdots \\\\ X_n \\end{pmatrix},\\] where \\(X_1, \\dots X_n: \\Omega \\to S\\) are RVs. Because of this, we have can write the density function as \\[ f_X(x) = f_{X_1 X_2 \\dots X_n}(x)\\] Some people call \\(X\\) a random vector. \\(f_{X_1\\dots X_n}\\) is called the joint probability distribution. Exercise 1.6 True or false: \\(f_{XY}(x,y) = f_{X}(x) + f_{Y}(y)\\) \\(f_{XY}(x,y) = f_{X}(x) f_{Y}(y)\\) Definition 1.9 (Marginal density) The marginal density of \\(X_i\\) is \\[f_{X_i}(x_i) = \\int f_{X_1\\dots X_n}(x_1, \\dots, x_n) \\, dx_1\\dots dx_{i-1} dx_{i+1} \\dots dx_n\\] (integrate coordinate except the \\(i\\)-th coordinate. Exercise 1.7 Can you construct \\(f_{XY}\\) if you know \\(f_X\\) and \\(f_Y\\)? 1.1.4 Some important random variables Point mass distribution (Dirac delta): Given a discrete probability \\(X: \\Omega \\to S\\). \\(X\\) has a point mass distribution at \\(a \\in S\\) if \\[ \\mathbb{P}( X = a) = 1.\\] We call \\(X\\) a point mass RV and write \\(X \\sim \\delta_a\\). Question. Suppose \\(S = \\mathbb{N}\\). Write down \\(F_X\\) for the point mass RV \\(X\\). Discrete uniform distribution: \\(f_X(k) = \\frac 1 n\\,,\\) \\(k \\in \\{1,\\dots, n\\}\\). Bernoulli distribution: let \\(X:\\Omega \\to \\{0,1\\}\\) be RV that represents a binary coin flip. Suppose \\(\\mathbb{P}(X = 1) = p\\) for some \\(p \\in [0,1]\\). Then \\(X\\) has a Bernoulli distribution, written as \\(X \\sim \\text{Bernoulli}(p)\\). The probability function is \\[f_X(x) = p^x (1-p)^{1-x}.\\] We write \\(X \\sim \\text{Bernoulli}(p)\\). Binomial distribution: let \\(X:\\Omega \\to \\mathbb{N}\\) be the RV that represents the number of heads out of \\(n\\) independent coin flips. Then \\[ f_X = \\begin{cases} {n \\choose x} p^x (1-p)^{n-x}\\,, x\\in \\{0,1,\\dots, n\\}\\\\ 0 \\,, \\text{otherwise} \\end{cases}\\] We write \\(X \\sim \\text{Binomial}(n,p)\\). Poisson distribution: \\(X \\sim \\text{Poisson}(\\lambda)\\). \\[f_X (k) = e^{-\\lambda} \\frac{\\lambda^k}{k!}\\,, k = 0, 1, 2 \\dots\\] \\(X\\) is a RV that describe a given number of events occurring in a fixed interval of time or space if these events occur with a known constant mean rate and independently of the time since the last event Gaussian: \\(X \\sim N(\\sigma,\\mu)\\). \\[f_X(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-(x-\\mu)^2/(2\\sigma^2)}.\\] Exercise 1.8 Let \\(X_{n,p} \\sim \\text{Binomial}(n,p)\\). Suppose that as \\(n\\to \\infty\\), \\(p \\to 0\\) in such a way that \\(np = \\lambda\\) always. Let \\(x\\in \\mathbb{N}\\). For \\(n\\) very very large, what is the behaviour of \\[ \\frac{n!}{(n-x)!} \\,.\\] (You should just get some power of \\(n\\)) Show that \\[\\lim_{n\\to \\infty} f_{X_{n,p}}(x) = \\frac{\\lambda^x e^{-\\lambda}}{x!}.\\] Interpret this result. 1.1.5 Independent random variables Definition 1.10 Let \\(X:\\Omega \\to S\\) and \\(Y: \\Omega \\to S\\) be RVs. We say that \\(X\\) and \\(Y\\) are independent if, for every \\(A, B \\in \\mathcal{B}(S)\\), we have \\[ \\mathbb{P}( X \\in A; Y \\in B) = \\mathbb{P}(X \\in A) \\mathbb{P}(Y \\in B) \\,, \\] and write \\(X \\perp Y\\). So, if \\(X\\) and \\(Y\\) are independent, \\[f_{XY}(x,y) = f_X(x) f_Y(y).\\] Definition 1.11 Let \\(X:\\Omega \\to S\\) and \\(Y: \\Omega \\to S\\) be RVs. Suppose that \\(f_Y(y) &gt;0\\). The conditional probability mass function of \\(X\\) given \\(Y\\) is \\[ f_{X|Y} (x|y) = \\frac{f_{XY}(x,y)}{f_Y(y)} \\,.\\] Exercise 1.9 Let \\[ f(x,y) = \\begin{cases} x+y \\,, 0\\leq x \\leq 1, 0 \\leq y \\leq 1\\\\ 0\\,, \\text{otherwise} \\end{cases}.\\] What is \\(\\mathbb{P}( X &lt; 1/4 \\vert Y = 1/3)\\). Note that the above exercise is a little bit weird and counter-intuitive. While \\(\\mathbb{P}( X &lt; 1/4 , Y = 1/3) = 0\\) (why?), \\(\\mathbb{P}( X &lt; 1/4 | Y = 1/3) \\not= 0\\) A very important RV is the multivariate Normal RV, which obeys the following density function \\[f(x; \\mu, \\Sigma) = \\frac{1}{(2\\pi)^{n/2} | \\Sigma |^{1/2}} \\exp\\big( -\\frac{1}{2} (x-mu)^T \\Sigma^{-1} (x-\\mu) \\big).\\] 1.1.6 Transformations of RVs Sometimes, we don’t work with RV directly but certain characteristics of RVs. Those characteristics are represented by certain transformation. If the functions are nice enough, we can actually have a recipe to generate the probability density function. Suppose \\(g: S^n \\to \\mathbb{R}\\) and \\(Z = g(X_1, \\dots, X_n)\\). Let \\(A_z = \\{ (x_1,\\dots, x_n): g(x_1,\\dots, x_n) \\leq z \\}\\). Then \\[F_Z(z) = \\mathbb{P}(Z \\leq z) = \\mathbb{P}(g(X_1, \\dots, X_n) \\leq z) = \\int_{A_z} f_{X_1\\dots X_n}(x_1,\\dots,x_n) \\, dx_1\\dots dx_n, \\] and \\[f_Z(z) = F_Z&#39;(z).\\] Exercise 1.10 Let \\(X, Y \\sim \\text{Uniform}(0,1)\\) be independent RVs, i.e., \\[f_{X}(x) = f_{Y}(y) = 1\\,.\\] What is the density function for the RV \\(Z = X + Y\\)? Same question but \\(X, Y \\sim N(0,1)\\). Definition 1.12 RVs that are independent and share the same distribution are called independent and identically distributed RVs. We often shorthand this by IID RVs. 1.1.7 Expectation Definition 1.13 Let \\(X\\) be a RV. The expected value, or expectation, or mean, or first moment of \\(X\\) is defined to be \\[ \\mathbb{E}X = \\int x f(x) dx. \\] The variance of \\(X\\) is defined to be \\[\\mathbb{E}\\left( X - \\mathbb{E}X \\right)^2\\] We often denote \\(\\mu_X\\) to be the expectation of \\(X\\), \\(\\sigma_X^2\\) (\\(\\mathrm{Var}(X), \\mathbb{V}(X)\\)) to be the variance of \\(X\\). The square root of the variance, \\(\\sigma\\), is called the standard deviation. Theorem 1.2 Let \\(X:\\Omega \\to S\\) be a RV, \\(r: S \\to S\\) be a function and \\(Y = r(X)\\). Then \\[ \\mathbb{E}Y = \\mathbb{E}(r(X)) = \\int r(x) f(x) dx \\] Exercise 1.11 Let \\(X \\sim \\text{Uniform}(0,1)\\). Compute \\(\\mathbb{E}Y\\), where \\(Y = e^X\\). \\(Y = \\max(X, 1-X)\\) Let \\(X,Y\\) be RVs that have jointly uniform distribution on the unit square. Compute \\(\\mathbb{E}(X^2 + Y^2)\\). Definition 1.14 Let \\(X\\) and \\(Y\\) be RVs. The covariance between \\(X\\) and \\(Y\\) is defined by \\[ \\mathrm{Cov}(X,Y) = \\mathbb{E}\\left( (X- \\mu_X)(Y - \\mu_Y) \\right) .\\] The correlation of \\(X\\) and \\(Y\\) is defined to be \\[ \\rho_{X,Y} = \\frac{\\mathrm{Cov}(X,Y)}{\\sigma_X \\sigma_Y} \\,.\\] Theorem 1.3 Let \\(X_i\\), \\(i=1, \\dots, n\\) be RVs and \\(a_i\\)’s be constants. Then \\[ \\mathbb{E}\\left(\\sum_i a_i X_i \\right) = \\sum_i a_i \\mathbb{E}X_i. \\] \\[ \\mathbb{V}X_i = \\mathbb{E}(X_i^2) - \\mu_{X_i}^2 \\] \\[ \\mathbb{V}\\left( \\sum_i a_i X_i \\right) = \\sum_i a_i^2 \\mathbb{V}(X_i) \\] \\[ \\mathbb{V}\\left( \\sum_i a_i X_i \\right) = \\sum_i a_i^2 \\mathbb{V}(X_i) + 2\\sum_{i&lt;j} a_i a_j \\mathrm{Cov}(X_i, X_j) \\,. \\] Suppose further that \\(X_i\\)’s are independent, then \\[ \\mathbb{E}\\left( \\prod_{i=1}^n X_i \\right) = \\prod_{i=1}^n \\mathbb{E}X_i\\] and \\[ \\mathbb{V}\\left( \\sum_i a_i X_i \\right) = \\sum_i a_i^2 \\mathbb{V}(X_i) \\,. \\] Definition 1.15 (Conditional Expectation) Let \\(X,Y:\\Omega \\to S\\), where \\(S\\) is either \\(\\mathbb{N}\\) or \\(\\mathbb{R}\\). The conditional expectation of \\(X\\) given \\(Y\\) is a RV \\(\\mathbb{E}[X | Y] : \\Omega \\to \\mathbb{R}\\) that satisfies the following \\[ \\mathbb{E}[X | Y](y) := \\mathbb{E}[X | Y = y] = \\int x f_{X|Y}(x|y) \\, dx . \\] If \\(r:S^2 \\to S\\) is a function, then \\[ \\mathbb{E}[r(X,Y) | Y = y] = \\int r(x,y) f_{X|Y}(x|y) \\, dx .\\] One can generalize this definition to higher dimension via the coordinate-wise conditional expectation. We will omit this definition in order to keep the presentation simple. Theorem 1.4 Let \\(X\\) and \\(Y\\) be Rvs. We have that \\[ \\mathbb{E}[ \\mathbb{E}[X | Y]] = \\mathbb{E}X. \\] "],["moment-generating-and-characteristic-functions.html", "1.2 Moment Generating and Characteristic Functions", " 1.2 Moment Generating and Characteristic Functions Definition 1.16 Let \\(X\\) be a RV. 1. The moment generating function MGF, or Laplace transform, of \\(X\\) is \\(\\varphi: \\mathbb{R}\\to \\mathbb{R}\\) defined by \\[ \\varphi_X (t) = \\mathbb{E}\\left( e^{t X} \\right), \\] where \\(t\\) varies over the real numbers. The characteristic function, or Fourier transform of \\(X\\) is \\(\\varphi: \\mathbb{R}\\to \\mathbb{C}\\) defined by \\[\\phi_X(\\theta) = \\mathbb{E}e^{i\\theta X} .\\] Lemma 1.1 Let \\(X\\) be a RV and \\(Y = aX + b\\), then \\[ \\varphi_Y(t) = e^{bt} \\varphi_{X}(at)\\] \\[ \\varphi_X^{(k)}(0) = \\mathbb{E}(X^k) \\] Let \\(X_i\\), \\(i= 1, \\dots, n\\) be independent RVs and \\(Y = \\sum_i X_i\\). Then \\[ \\varphi_Y (t) = \\prod \\varphi_{X_i}(t). \\] \\[| \\phi (\\theta) | \\leq 1\\] Denote \\(\\overline{z}\\) to be the complex conjugate of \\(z\\) in the complex plane. \\[ \\phi_{-X} (\\theta) = \\overline{\\phi_X (\\theta)} \\] \\[\\phi_Y (\\theta) = e^{i b \\theta} \\phi(a\\theta) \\] Exercise 1.12 Prove the above lemma. Exercise 1.13 Let \\(X \\sim \\exp(1)\\), i.e, \\[ f_X(x) = \\begin{cases} e^{-x} \\,, x \\geq 0 \\\\ 0 \\,, x &lt; 0 \\,. \\end{cases}\\] Compute \\(\\varphi_X\\). Recall that two RVs \\(X \\stackrel{d}{=}Y\\) means that \\(F_X (x) = F_Y(x)\\). Two common ways to characterize the equality in distribution are to use the generating functions and the characteristic functions. These ideas are not orginally from probability but from engineering/mechanics, where Laplace and Fourier transforms are understood very well since the 18th century. Exercise 1.14 In general, differentiation is not commutative with integration, that is \\[ \\frac{d}{dt} \\int \\not= \\int \\frac{d}{dt}. \\] However, assuming that this is true for certain moment generating functions \\(\\varphi_X\\). Show that \\[ \\varphi_X^{(n)}(0) = \\mathbb{E}( X^n),\\] where \\(f^{(n)}\\) denotes the \\(n\\)-th derivative of \\(f\\). \\(\\mathbb{E}(X^n)\\) is called the \\(n\\)-th moment of \\(X\\) and it tells you the tail behavior of \\(f_X\\). 1.2.1 Moment Generating Functions Theorem 1.5 Let \\(X\\) and \\(Y\\) be RVs. If \\(\\varphi_X(t) = \\varphi_Y(t)\\) for all \\(t\\) in an interval around 0, then \\[ X \\stackrel{d}{=}Y \\,.\\] The full proof of this is beyond this class (and could be a great topic for a project). However, we will prove this for finte RVs. Proposition 1.1 (Finite RV case) Let \\(X,Y: \\Omega \\to \\{1,2, \\dots, N\\}\\) be RVs. If \\(\\varphi_X(t) = \\varphi_Y(t)\\) in an interval around in an interval \\((-\\epsilon , \\epsilon)\\), then \\[ X \\stackrel{d}{=}Y \\,.\\] Proof. We have that \\[ \\varphi_X (t) = \\mathbb{E}( e^{tX}) = \\sum_{i= 1}^N e^{it}\\mathbb{P}( X = i) \\] and \\[ \\varphi_Y (t) = \\mathbb{E}( e^{tX}) = \\sum_{i= 1}^N e^{it}\\mathbb{P}( Y = i). \\] Therefore, \\[ 0 = \\varphi_X(t) - \\varphi_Y(t) = \\sum_{i=1}^N (e^t)^i \\left( \\mathbb{P}(X = i) - \\mathbb{P}(Y = i) \\right) \\] for every \\(t \\in (-\\epsilon , \\epsilon)\\). Therefore, as the above is a polynomial, \\[ \\mathbb{P}( X = i) = \\mathbb{P}(Y = i) \\] where \\(i = 1, \\dots, N\\). Note that if the above summation is infinite, then we cannot conclude that \\(X\\) and \\(Y\\) has the same distribution as easily as we just did. More work has to be done to show this. A note of caution: the assumption that \\(\\varphi_X = \\varphi_Y\\) in an interval around \\(0\\) is crucial in general. An interesting observation arises: for analytic functions we have the Taylor series \\[ f(x) = \\sum_{i=0}^\\infty \\frac{f^{(n)}(0)}{n!} x^n. \\] Exercise 1.14 tells you that the \\(n\\)-th derivative at \\(0\\) of a moment generating function would be the \\(n\\)-th moment of the RV. Question: Is knowing the moments of \\(X\\) enough to determine its probability distribution? The answer is NO. One can take a look at the discussion about this problem here: https://mathoverflow.net/questions/3525/when-are-probability-distributions-completely-determined-by-their-moments. However, things are nice for finite RVs. Proposition 1.2 Let \\(X,Y: \\Omega \\to \\{1,2, \\dots, N\\}\\) be RVs. Suppose that \\[ \\mathbb{E}(X^n) = \\mathbb{E}(Y^n) &lt; \\infty\\] for every \\(n\\in \\mathbb{N}\\). Then \\[ X \\stackrel{d}{=}Y .\\] Proof. Consider \\[ \\varphi_X(t) = \\mathbb{E}(e^{Xt}) = \\sum_{i = 1}^N e^{it} \\mathbb{P}( X = i). \\] This is a finite sums of analytic functions and is, therefore, analytic. Thus, \\(\\varphi_X\\) can be expanded into Taylor series, i.e., \\[ \\varphi_X(t) = \\sum_{n=0}^\\infty \\frac{\\varphi_X^{(n)}(0)}{n!} t^n = \\sum_{n=0}^\\infty \\frac{\\mathbb{E}(X^n)}{n!} t^n.\\] This means that the moments of \\(X\\) determines its moment generating function (which may not be true in general). A similar argument can be made for \\(\\varphi_Y\\) and as the coefficents of the Taylor series are the same (being the moments of \\(X\\) and \\(Y\\)), we conclude that \\[\\varphi_X = \\varphi_Y.\\] Therefore, by Theorem 1.5, \\[ X \\stackrel{d}{=}Y, \\] as desired. 1.2.2 Characteristic Functions Similar idea with the moment generating functions, but characteristic functions are easier to work with and we don’t have to work with special case of finite RVs. Theorem 1.6 Let \\(X\\) and \\(Y\\) be RVs. If \\(\\phi_X(t) = \\phi_Y(t)\\) for all \\(t\\) in an interval around 0, then \\[ X \\stackrel{d}{=}Y \\,.\\] In order to prove this theorem, we need the following important result, called inversion formula of the characteristic functions. Theorem 1.7 (Inversion Formula) Let \\(X:\\Omega \\to S\\) be a RV (either continuous or discrete) and \\(\\phi_X\\) be its characteristic function. Then \\[ \\lim_{T \\to \\infty} \\frac{1}{2\\pi}\\int_{-T}^T \\frac{e^{-i\\theta a} - e^{-i\\theta b}}{i\\theta} \\phi_X(\\theta) \\, d\\theta = \\mathbb{P}( a &lt; X &lt; b) + \\frac{1}{2} \\left( \\mathbb{P}(X = a) + \\mathbb{P}(X = b) \\right). \\] Proof. We have \\[\\begin{aligned} \\frac{1}{2\\pi} \\int_{-T}^T \\frac{ e^{ - i \\theta a} - e^{- i \\theta b}}{i\\theta} \\phi_X(\\theta) \\, d\\theta &amp; = \\frac{1}{2\\pi} \\int_{-T}^T \\frac{ e^{ - i \\theta a} - e^{- i \\theta b}}{i\\theta} \\int_{\\mathbb{R}} e^{i\\theta x} f_X(x) \\, d\\theta dx \\\\ &amp; = \\int_{\\mathbb{R}}\\frac{1}{\\pi} \\int_{-T}^T \\frac{ e^{ i \\theta (x - a)} - e^{i \\theta (x- b)}}{ 2 i\\theta} f_X(x) \\, d\\theta dx \\\\ \\end{aligned}\\] Note that since \\(\\cos(t)/t\\) is odd and \\(\\sin(t)/t\\) is even, and that \\(e^{i\\theta} = \\cos(\\theta) + i \\sin(\\theta)\\), we have \\[ \\frac{1}{2}\\int_{-T}^T \\frac{e^{i\\theta c}}{i \\theta} = \\int_0^T \\frac{\\sin(\\theta c)}{\\theta} \\, d\\theta. \\] Therefore, \\[ \\begin{aligned} \\frac{1}{2\\pi} \\int_{-T}^T \\frac{ e^{ - i \\theta a} - e^{- i \\theta b}}{i\\theta} \\phi_X(\\theta) \\, d\\theta &amp;= \\frac{1}{\\pi} \\int_{\\mathbb{R}} \\int_0^T \\left( \\frac{\\sin((x - a)\\theta)}{\\theta} - \\frac{\\sin((x - b)\\theta)}{\\theta} \\right) f_X(x) \\, d\\theta dx \\end{aligned} \\] Taking the limit \\(T\\to\\infty\\) and using the fact that \\[ \\lim_{T \\to \\infty}\\int_0^T \\frac{\\sin ((x-a)\\theta)}{\\theta} d\\theta = \\begin{cases} \\frac{-\\pi}{2} \\,, &amp; x &lt; a \\,,\\\\ \\frac{\\pi}{2} \\,, &amp; x &gt; a \\,, \\\\ 0 \\,, &amp; x = a \\,. \\end{cases}\\] Therefore, \\[ \\begin{aligned} \\lim_{T\\to \\infty}\\frac{1}{2\\pi} \\int_{\\mathbb{R}}\\int_{-T}^T \\frac{ e^{ - i \\theta a} - e^{- i \\theta b}}{i\\theta} \\phi_X(\\theta) \\, d\\theta dx &amp;= \\left(\\int_{(a,\\infty)} f_X(x) \\, dx - \\int_{ (-\\infty, a]} f_X(x) \\, dx \\right)\\\\ &amp; \\quad - \\left(\\int_{(b,\\infty)} f_X(x) \\, dx - \\int_{ (-\\infty, b]} f_X(x) \\, dx \\right) \\\\ &amp;= (\\mathbb{P}(X&gt;a) - \\mathbb{P}(X\\leq a)) - (\\mathbb{P}(X&gt;b) - \\mathbb{P}(X\\leq b)) \\\\ &amp;= \\mathbb{P}( a &lt; X &lt; b) + \\frac{1}{2} \\left( \\mathbb{P}(X = a) + \\mathbb{P}(X = b) \\right), \\end{aligned} \\] as desired. Exercise 1.15 Verify that \\[\\lim_{T \\to \\infty} \\int_0^\\infty \\frac{\\sin(x)}{x} \\, dx = \\frac{\\pi}{2}.\\] If you can’t, watch this: https://www.youtube.com/watch?v=Bq5TB6cZNng. Another way is to use contour integral from complex analysis. Exercise 1.16 Let \\(X_1, \\dots, X_n \\sim \\mathrm{Uniform}(0,1)\\) be independent and \\(Y_n = \\max\\{ X_1, \\dots, X_n \\}\\). Find \\(\\mathbb{E}(Y_n)\\). Exercise 1.17 Let \\(X:\\Omega \\to (0,\\infty)\\) be continuous positive RV. Suppose \\(\\mathbb{E}(X)\\) exist. Show \\(\\mathbb{E}(X) = \\int_0^\\infty \\mathbb{P}(X &gt; x) \\, dx\\). (Hint: Fubini. This is called the layer cake theorem). Exercise 1.18 The exponential distribution with parameter \\(\\lambda\\) (denoted by \\(\\exp(\\lambda)\\)) is used to model waiting time (see https://en.wikipedia.org/wiki/Exponential_distribution). The probability density function of the exponential distribution is given by \\[f(x) = \\begin{cases} \\lambda e^{-\\lambda x} &amp; x\\geq 0 \\\\ 0 &amp; x&lt; 0 \\end{cases}.\\] Find the moment-generating function of \\(X \\sim \\exp(\\lambda)\\). Use moment-generating function to show that if \\(X\\) is exponential distributed, then so is \\(cX\\). Exercise 1.19 Let \\(X \\sim N(\\mu_1, \\sigma_1)\\) and \\(Y \\sim (\\mu_2, \\sigma_2)\\) be independent. Use the moment generating function to show that \\(Z = c_1 X + c_2 Y\\) is again a normal distribution. What are \\(\\mathbb{E}(Z)\\) and \\(\\mathbb{V}(Z)\\)? Exercise 1.20 Find the moment-generating function of a Bernoulli RV, and use it to find the mean, variance, and third moment. Exercise 1.21 Let \\(X: \\Omega \\to S\\) be a RV and \\(S = \\mathbb{N}\\). The probability generating function of \\(X\\) is defined to be \\[ G(s) = \\sum_{k=1}^\\infty s^k \\mathbb{P}(X = k). \\] Show that \\[ \\mathbb{P}( X = k) = \\frac{1}{k!} \\frac{d^k}{ds^k} G(s) \\vert_{s=0} \\] Show that \\[ \\frac{dG}{ds} \\vert_{s=1} = \\mathbb{E}(X) \\] and \\[ \\frac{d^2G}{ds^2} \\vert_{s=1} = \\mathbb{E}[X(X-1)]. \\] Express the probability-generating function in terms of moment-generating function. Find the probability-generating function of the Poisson distribution. "],["inequalities.html", "1.3 Inequalities", " 1.3 Inequalities 1.3.1 Typical tail bound inequalities Theorem 1.8 (Markov's Inequality) Let \\(X\\) be a non-negative RV and \\(\\mathbb{E}(X)\\) exists. Then, for each \\(k &gt;0\\), \\[ \\mathbb{P}( X &gt; k) \\leq \\frac{\\mathbb{E}(X)}{k}. \\] Theorem 1.9 (Chebyshev's Inequality) Let \\(X\\) be a RV such with expected value \\(\\mu\\) and standard variation \\(\\sigma\\). Then for each \\(k &gt;0\\), \\[ \\mathbb{P}(| X - \\mu | &gt; k \\sigma ) \\leq \\frac{1}{k^2}. \\] 1.3.2 Exponential concentration inequalities Theorem 1.10 (Mill's inequality) Let \\(Z \\sim N(0,1)\\). Then, for each \\(t &gt;0\\), \\[ \\mathbb{P}(|Z| &gt; t) \\leq \\sqrt{\\frac{2}{\\pi}} \\frac{e^{-t^2/2}}{t}. \\] Theorem 1.11 (Hoeffding's inequality) Let \\(X_1, \\dots, X_n\\) be independent RVs such that \\(\\mathbb{E}( X_i ) = 0\\), \\(a_i \\leq Y_i \\leq b_i\\). For each \\(\\epsilon &gt;0\\) and \\(t&gt;0\\), we have \\[ \\mathbb{P}\\left( \\sum_{i=1}^n X_i \\geq \\epsilon \\right) \\leq e^{-t\\epsilon} \\prod_{i=1}^n e^{t^2(b_i - a_i)^2/8}. \\] Exercise 1.22 Let \\(X_1, \\dots, X_n\\) be independent RVs such that \\(\\mathbb{E}( X_i ) = 0\\), \\(a_i \\leq Y_i \\leq b_i\\). Show that for each \\(\\epsilon &gt;0\\) and \\(t&gt;0\\), we have \\[ \\mathbb{P}\\left( \\left| \\frac{1}{n}\\sum_{i=1}^n X_i \\right| \\geq t \\right) \\leq 2 \\exp\\left( - \\frac{2 n^2 t^2}{\\sum_{i=1}^n (b_i - a_i)^2} \\right). \\] 1.3.3 Inequalities for expectations Theorem 1.12 (Cauchy-Schwartz inequality) Let \\(X, Y\\) be RVs with finite variances. Then, \\[\\mathbb{E}( |XY| ) \\leq \\sqrt{ \\mathbb{E}(X^2) \\mathbb{E}(Y^2) }.\\] Theorem 1.13 (Jensen's inequality) Suppose \\(g:\\mathbb{R}\\to \\mathbb{R}\\) is a convex function. Then \\[ \\mathbb{E}g(X) \\geq g(\\mathbb{E}X). \\] "],["law-of-large-numbers.html", "1.4 Law of Large Numbers", " 1.4 Law of Large Numbers Theorem 1.14 Let \\(X_i\\), \\(i\\in \\mathbb{N}\\) be independent RVs such that \\(\\mathbb{E}(X_i) = \\mu\\) and \\(\\mathbb{V}(X_i) = \\sigma^2\\). Then, for each \\(\\epsilon &gt; 0\\), \\[ \\lim_{n\\to \\infty} \\mathbb{P}\\left( \\left| \\frac{1}{n} \\sum_{i=1}^n X_i - \\mu \\right| &gt; \\epsilon \\right) = 0 \\,.\\] The above kind of convergence is sometimes called convergence in probability. There are other modes of convergence such as convergence almost surely and uniform convergence. "],["central-limit-theorem.html", "1.5 Central Limit Theorem", " 1.5 Central Limit Theorem Definition 1.17 (Convergence in distribution) Let \\(\\{X_i\\}_{i \\in \\mathbb{N}}\\) be a sequence of RVs with CDF \\(F_i\\). Let \\(X\\) be a RV with CDF \\(F\\). We say that \\(X_n\\) convergence to \\(X\\) in distribution if \\[ \\lim_{n\\to \\infty} F_n (x) = F(x) \\] at every point at which \\(F\\) is continuous. Theorem 1.15 (Continuity theorems) Let \\(X_i\\), \\(i \\in \\mathbb{N}\\) RVs with CDF \\(F_i\\) and \\(X\\) a RV with CDF \\(\\bar F\\). Suppose that either \\(\\varphi_{X_n}(s)\\) converges to \\(\\varphi_X(s)\\) for all \\(s\\) in some open interval around \\(0\\). \\(\\lim_{n\\to\\infty} \\phi_{X_n}(s) = \\phi_X(s)\\) for every \\(s \\in \\mathbb{R}\\). Then \\(X_n \\to X\\) in distribution. Theorem 1.16 (Central limit theorem) Let \\(\\{ X_i \\}_{i\\in \\mathbb{N}}\\) be a sequence of IID RVs with mean \\(0\\) and variance \\(\\sigma^2 &lt; \\infty\\). Define \\[ Z_n = \\frac{\\sum_{i=1}^n X_i}{\\sigma \\sqrt{n}}. \\] Then \\(Z_n\\) converges to \\(Z \\sim N(0,1)\\) in distribution. There are a few ways to go about proving this theorem. Two most common ways employ the MGF and the characteristic function. Both methods rely on the one crucial idea of using the Taylor expansion, which we will see shortly. We present the proof using MGF (adopted from Rice’s book) and leave it to the reader the proof using characteristic function. Proof. For each \\(n \\in \\mathbb{N}\\), we have that \\[ \\varphi_{Z_n} (t) = \\left( \\varphi_{X_1} \\left( \\frac{t}{\\sigma \\sqrt{n}} \\right) \\right)^n. \\] Observe first that \\(\\varphi_{X_1}&#39;(0) = \\mathbb{E}X_1 = 0\\) and \\(\\varphi_{X_1}&#39;&#39;(0) = \\mathbb{E}X_1^2 = \\sigma^2\\). So, performing Taylor expansion for \\(\\varphi_{X_1}\\), we get \\[ \\begin{aligned} \\varphi_{X_1} \\left( \\frac{t}{\\sigma \\sqrt{n}} \\right) &amp;= 1 + \\varphi_{X_1}&#39;(0) \\left( \\frac{t}{\\sigma \\sqrt{n}} \\right) + \\frac{1}{2} \\varphi_{X_1}&#39;&#39;(0) \\left( \\frac{t}{\\sigma \\sqrt{n}} \\right)^2 + \\epsilon_n \\\\ &amp; = 1 + \\frac{1}{2} \\sigma^2 \\left( \\frac{t}{\\sigma \\sqrt{n}} \\right)^2 + \\epsilon_n \\\\ &amp; = 1 + \\frac{1}{2} \\left( \\frac{t^2}{n} \\right) + \\epsilon_n \\,. \\end{aligned}\\] where \\(\\epsilon_n / (t^2 / (n\\sigma^2)) \\to 0\\) as \\(n \\to \\infty\\). It can then be shown that \\[ \\lim_{n\\to \\infty} \\varphi_{Z_n}(t) = \\lim_{n\\to \\infty} \\left( 1 + \\frac{1}{2} \\left( \\frac{t^2}{n} \\right) + \\epsilon_n \\right) = e^{t^2/2} = \\varphi_Z .\\] Combine this with Theorem 1.15, we arrive at our result. Exercise 1.23 Prove the central limit theorem using the characteristic function. "],["part-2-inference.html", "PART 2: Inference", " PART 2: Inference Readings: Rice Chapters 7, 8 Wasserman Chapter 7 Casella-Berger Chapter 7 "],["sampling-estimating-cdf-and-statistical-functionals.html", "Chapter 2 Sampling, Estimating CDF and Statistical Functionals ", " Chapter 2 Sampling, Estimating CDF and Statistical Functionals "],["sampling.html", "2.1 Sampling", " 2.1 Sampling Sampling is the act of gathering data from a certain population, in order to make prediction about some property of the population of interest. Each time one goes out to the field to sample, one gets different answers for the same set of quantities of interest, making those answers random variables. For finite population, there are two techniques called sampling with replacement and sampling without replacement. Sampling without replacement is sometimes called simple random sampling and one needs to be careful with it. However, if the population size is very very large compared to the sample size (a very subjective judgement), it is common in practice to treat the sampling data as I.I.D. RVs. 2.1.1 Simple Random Sample We will not discuss Simple Random Sampling in this class. Interested readers can consult Rice, Chapter 7.3. 2.1.2 Standard Random Sample Definition 2.1 (Standard Random Sample) The random variables \\(X_i\\), \\(i = 1,\\dots , n\\) are called standard random sample of size \\(n\\) from population \\(f(x)\\) if \\(X_i\\)’s are I.I.D. RVs from the same probability density function \\(f\\). There is a few nuisances regarding general practice in statistics and this definition. Definition 2.1 is either for infinite population or finite population with sampling with replacement. For finite population of size \\(n\\), sample data \\(X_i\\) from sampling without replacement can never be independent as \\(\\mathbb{P}(X_2 = y | X_1 = y) = 0\\) and \\(\\mathbb{P}(X_2 = y | X_1 = x) = 1/(n-1)\\). In this course, when we talk about sampling, we will understand it as in Definition 2.1. "],["statistical-estimation.html", "2.2 Statistical Estimation", " 2.2 Statistical Estimation Reading: Wasserman Chapter 6. Statistical inference, often rebranded as learning in computer science, is the process of figuring out certain information of a distribution function \\(F\\) given sample \\(X_1, \\dots, X_n \\sim F\\). Typically, we don’t know which distribution function our sample comes from. However, sometimes, with some background theory (or simply just to make life easier), we may assume that the data come from certain family of distributions so that we can narrow our search. This gives rise to the following definitions. Definition 2.2 A statistical model \\(\\mathcal{F}\\) is a set of distributions (or densities). A parametric model is a set set \\(\\mathcal{F}\\) that can be parametrized by a finite number of parameters. A non-parametric model is a statistical model that is not parametric. Example 2.1 The set of Gaussians is a two parameter models: \\[ \\mathcal{F} = \\left\\{ f(x; \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp\\left\\{ -\\frac{(x-\\mu)^2}{2\\sigma^2} \\right\\}, \\mu \\in \\mathbb{R}, \\sigma &gt; 0 \\right\\}. \\] The set of Bernoulli distributions is a set of one parameter model: \\[ \\mathcal{F} = \\left\\{ \\mathbb{P}(X = 1) = p, \\mathbb{P}(X = 0) = 1 -p, 0\\leq p \\leq 1 \\right\\}.\\] Generally, a parametric model has the following form \\[\\mathcal{F} = \\left\\{ f(x;\\theta) : \\theta \\in \\Theta \\right\\} ,\\] where \\(\\Theta\\) is some parameter space. Notation. Given a parametric model \\(\\mathcal{F} = \\left\\{ f(x;\\theta) : \\theta \\in \\Theta \\right\\} ,\\) we denote \\[ \\mathbb{P}(X \\in A ) = \\int_A f(x;\\theta) \\, dx\\] and \\[ \\mathbb{E}_\\theta ( r(X)) = \\int r(x) f(x;\\theta) \\, dx \\,.\\] 2.2.1 Point Estimation (Casella - Berger Chapter 7, Wasserman Chapter 6.1) Definition 2.3 Let \\(\\{X_i\\}\\), \\(i = 1, \\dots, n\\) be a sample. A point estimator of \\(\\{X_i\\}\\) is a function \\(g(X_1, \\dots, X_n)\\). The purpose of the point estimator is to provide the “best guess” of certain quantity of interest. Those quantities could be a parameter in a parametric model, a CDF, PDF,… Typically, the quantity of interest is denoted by \\(\\theta\\), the point estimator is denoted by \\(\\hat \\theta\\) or \\(\\hat \\theta_n\\). So, combined with the above definition, \\[ \\hat \\theta_n = g(X_1, \\dots, X_n).\\] Note that, \\(\\hat \\theta_n\\) is still a random variable as this is a function of your sample data, which are RVs themselves. Of course, we know that there are cases when samples suffer from biases. A way to measure biases is to compare the expected value of \\(\\hat \\theta_n\\) and the true value of the quantity of interest \\(\\theta\\). Definition 2.4 The bias of an estimator is defined by \\[ b(\\hat \\theta_n) = \\mathbb{E}(\\hat \\theta_n) - \\theta. \\] We say that \\(\\hat \\theta_n\\) is unbiased if \\(b(\\hat \\theta_n) = 0\\). We also define the variance of an estimator by \\[ \\mathbb{V}_\\theta(\\hat \\theta_n) = \\mathbb{E}_\\theta (\\hat \\theta_n - \\mathbb{E}_\\theta (\\hat \\theta_n))^2 .\\] The standard error (\\(\\mathrm{se}\\) for short sometimes) is then \\[ \\mathrm{se}(\\hat \\theta_n) = \\sqrt{\\mathbb{V}_\\theta (\\hat \\theta_n)}. \\] Classically, unbiased estimators received a lot of attention since people wanted to have unbiased samples. However, modern statistics has a different point of view: because data is large, it doesn’t matter if the samples are biased as long as the estimators converge to the true quantity of interest. This gives rise to the following definition Definition 2.5 A point estimator \\(\\hat \\theta_n\\) of a parameter \\(\\theta\\) is consistent if \\(\\hat \\theta_n\\) converges to \\(\\theta\\) in probability. Here comes the million-dollar question: How do we measure bias in the samples? One possible approach is to use the so-called mean squared error. Definition 2.6 The mean squared error of an estimator is defined by \\[ MSE = \\mathbb{E}_\\theta (\\theta - \\hat \\theta_n)^2 \\,.\\] Theorem 2.1 (Bias-Variance decomposition) \\[ MSE = b_\\theta^2(\\hat \\theta_n) + \\mathbb{V}_\\theta(\\hat \\theta_n) \\] Exercise 2.1 Prove the Bias-Variance decomposition. Theorem 2.2 If, as \\(n\\to \\infty\\), \\(b_\\theta^2(\\hat \\theta_n) \\to 0\\) and \\(\\mathbb{V}_\\theta(\\hat \\theta_n) \\to 0\\), then \\(\\hat\\theta_n\\) is consistent. Exercise 2.2 Prove the above theorem. A big part of elementary statistics dealt with estimators being approximately related to the Normal distribution. Definition 2.7 An estimator is said to be asymptotically Normal if \\[ \\frac{\\hat \\theta_n - \\theta}{\\mathrm{se}(\\hat \\theta_n)} \\to N(0,1) \\] in distribution. 2.2.2 Confidence set In elementary statistics, given sample \\(X_1, \\dots, X_n\\), we define confidence interval with significance level \\(\\alpha\\) to be the interval \\((a,b)\\) such that \\(\\mathbb{P}_\\theta( \\theta \\in (a,b) ) \\geq 1 - \\alpha\\). Note that \\((a,b)\\) depends on your sample, i.e., \\(a = a(X_1, \\dots, X_n), b = b(X_1, \\dots, X_n)\\). It must be stressed that \\(\\theta\\) is fixed and \\((a,b)\\) is random. For higher dimension / different kinds of data, the notion of confidence interval is replaced by the notion of confidence set. Definition 2.8 Given sample \\(X_1, \\dots, X_n\\). A confidence set associated with significance level \\(\\alpha\\) is the set (random) \\(C_n\\) (depending on the sample) such that \\[ \\mathbb{P}_\\theta(\\theta \\in C_n) \\geq 1 - \\alpha. \\] Confidence set is not a probability statement about the parameter \\(\\theta\\). It is rather a statement about the uncertainty of your data. Example 2.2 (Example 6.14 in Wasserman) Let \\(\\theta \\in \\mathbb{R}\\). Let \\(X_1, X_2\\) RVs coming from the distribution \\(\\mathbb{P}(X_i = 1) = \\mathbb{P}(X_i = -1) = 1/2\\). Suppose \\(Y_i = \\theta + X_i\\) are your observed data. Define \\[ C = \\begin{cases} \\{ Y_1 - 1\\} &amp; Y_1 = Y_2 \\,, \\\\ \\{ (Y_1 + Y_2)/2 \\} &amp; Y_1 \\not= Y_2 \\,. \\end{cases}\\] For all \\(\\theta\\in \\mathbb{R}\\), \\(\\mathbb{P}_\\theta(\\theta \\in C ) = 3/4\\). Suppose we get \\(Y_1 = 9\\), \\(Y_2 = 11\\), \\(C = \\{ 10 \\}\\). Then, for sure, \\(\\theta = 10\\). Therefore, \\(\\mathbb{P}(\\theta \\in C | Y_1, Y_2) = 1\\). Exercise 2.3 Recall Hoeffding’s inequality \\[ \\mathbb{P}\\left( \\left| \\frac{1}{n}\\sum_{i=1}^n X_i \\right| \\geq t \\right) \\leq 2 \\exp\\left( - \\frac{2 n^2 t^2}{\\sum_{i=1}^n (b_i - a_i)^2} \\right) \\] for \\(X_i \\in [a_i, b_i]\\) and \\(\\mathbb{E}X_i = 0\\). Apply this to the Bernoulli parametric model \\[\\mathcal{F} = \\left\\{ \\mathbb{P}(X= 1) = p, \\mathbb{P}(X = 0) = 1-p; p \\in [0,1] \\right\\}.\\] Question: Suppose our sample comes from a Bernoulli distribution. What is a confidence interval that gives significance level \\(\\alpha\\)? Try with two approaches: Hoeffding and Chebyshev. Exercise 2.4 Let \\(X_1, \\ldots, X_n \\sim \\operatorname{Bernoulli}(p)\\) and let \\(\\widehat{p}_n=n^{-1} \\sum_{i=1}^n X_i\\). Compute \\(\\mathbb{V}( X_i)\\) and \\(\\mathbb{V}(\\hat p_n)\\) Suppose we don’t know \\(\\mathbb{V}(\\hat p_n)\\), so we use an estimator of this quantity \\[\\widehat{\\mathrm{se}}^2 = {\\widehat{p}_n\\left(1-\\widehat{p}_n\\right) / n} \\,.\\] Convince yourself that, by the Central Limit Theorem, \\(\\widehat{p}_n \\approx N\\left(p, \\widehat{\\operatorname{se}}^2\\right)\\). Find the confidence interval for the significance level \\(\\alpha\\). Compare this with the confidence interval in the previous exercise. You should see that the Normal-based interval is shorter but it only has approximately (when sample size is large ) correct coverage. "],["constructing-estimators.html", "2.3 Constructing estimators", " 2.3 Constructing estimators 2.3.1 Method of Moments Let \\(l \\in \\mathbb{N}\\), the \\(l\\) sample moment is \\[ \\hat m_l = \\frac 1 n \\sum_{i=1}^n X_i^l \\,. \\] Suppose that we want to determine \\(k\\) different parameters in a parametric model. The population moments are functions of those parameters: \\[ \\mu_l = \\mu_l (\\theta_1, \\dots, \\theta_k) \\,. \\] The method of moments says that one can construct parameters \\(\\hat \\theta_1, \\dots, \\hat \\theta_k\\) by solving \\[\\begin{equation} \\begin{aligned} \\hat m_1 &amp;= \\mu_1 (\\hat \\theta_1, \\dots, \\hat \\theta_k) \\\\ &amp;\\vdots\\\\ \\hat m_k &amp;= \\mu_k (\\hat \\theta_1, \\dots, \\hat \\theta_k) \\\\ \\end{aligned} \\tag{2.1} \\end{equation}\\] Example 2.3 Let \\(X_1, \\dots, X_n \\sim N(\\theta, \\sigma^2)\\). Construct estimators for the two parameters \\(\\theta\\) and \\(\\sigma^2\\). Example 2.4 Let \\(X_1, \\dots, X_n \\sim \\mathrm{Binomial}(k,p)\\), i.e., \\[ \\mathbb{P}(X_i = x) ={k \\choose x} p^x (1-p)^{k-x}. \\] Construct estimators for \\(k\\) and \\(p\\). Suppose the model we are considering has \\(k\\) parameters \\(\\theta_j \\in \\mathbb{R}\\), where \\(j = 1, \\dots, k\\). Define a function \\(g: \\mathbb{R}^k \\to \\mathbb{R}^k\\) by \\[ g(\\theta) = \\mu, \\] where \\[\\theta = (\\theta_1, \\dots, \\theta_k)\\] and \\[\\mu = (\\mu_1, \\dots, \\mu_k).\\] We can rephrase the above construction of the estimators as solving for \\(\\hat \\theta\\), given \\(\\hat \\mu\\) in the equation \\[\\begin{equation} \\hat \\mu = g(\\hat \\theta) . \\tag{2.2} \\end{equation}\\] (Note that \\(\\hat \\mu\\) and \\(\\hat \\theta\\) depends on the sample (size)) Two natural questions arise: Can we solve this equation? Are the estimators consistent? The answer to the first question is not so obvious. However, if we can solve the first problem, then the second problem is somewhat more manageable, given some reasonable assumptions. Exercise 2.5 Suppose that \\(g:\\mathbb{R}^k \\to \\mathbb{R}^k\\) defined above is a bijection with continuous inverse. Then, for each \\(\\epsilon &gt;0\\), there exists an \\(N&gt;0\\) such that for every \\(n \\geq N\\), \\[\\lim_{n\\to \\infty}\\mathbb{P}( |\\hat \\theta - \\bar \\theta| ) \\geq 1 - 1/ \\epsilon.\\] That is, \\(\\hat \\theta\\) is consistent. Of course, \\(g\\) is nonlinear generally. So, the assumption that \\(g\\) is a bijection may seem to be too strong and not too satisfying. To fix this issue, let’s consider a more general version of the above construction of estimators. Define a modified version of the estimator \\(\\hat \\theta\\) as follows \\[ \\tilde \\theta = \\begin{cases} \\hat \\theta &amp; \\text{if it is solvable } \\\\ 0 &amp; \\text{otherwise}\\end{cases}\\] Theorem 2.3 Suppose that all the moments of the underlying population are finte, \\(g\\) is of class \\(C^1\\) and that \\(\\det[Dg]\\not= 0\\). For each \\(\\epsilon &gt;0\\), \\[\\lim_{n\\to \\infty}\\mathbb{P}( |\\tilde \\theta - \\bar \\theta| &gt; \\epsilon ) = 0.\\] Proof. Let \\(\\epsilon, \\alpha &gt;0\\). From the weak law of large number, there exists \\(\\bar m\\) so that \\(\\hat m \\to \\bar m\\) as \\(n \\to \\infty\\) in probability. Note, that \\(\\bar m\\) is the list of moments that are generated from the underlying population, therefore, \\[\\bar m = g (\\bar \\theta),\\] where \\(\\bar\\theta\\) is the list of underlying parameters. By the inverse function theorem, there exists a \\(\\delta &gt;0\\) such that: \\(g\\) is invertible in the ball \\(B(\\bar m, \\delta)\\), \\(g^{-1}\\) is of class \\(C^1\\), \\(g^{-1}(B(\\bar m, \\delta)) \\subseteq B(\\bar \\theta, \\epsilon)\\). Let \\(N\\) be such that for every \\(n &gt; N\\), \\[\\mathbb{P}( |\\hat m - \\bar m| &lt; \\delta) \\geq 1 - \\alpha. \\] Since \\(|\\hat m - \\bar m| &lt; \\delta\\) implies that \\(\\hat\\theta\\) is uniquely solvable, i.e.  \\(\\hat\\theta = g^{-1}(\\hat m)\\), we have \\[ \\mathbb{P}(| \\tilde \\theta - \\bar \\theta | &gt; \\epsilon) \\leq \\mathbb{P}( |\\hat m - \\bar m| \\geq \\delta) \\leq \\alpha. \\] Therefore, since \\(\\alpha\\) is arbitrary, \\[\\lim_{n\\to \\infty}\\mathbb{P}( |\\tilde \\theta - \\bar \\theta| &gt; \\epsilon ) = 0,\\] as desired. 2.3.2 Maximum Likelihood Estimation Definition 2.9 Suppose \\(X_1, \\dots, X_n \\sim f_\\theta\\). The likelihood function is defined by \\[ \\mathcal{L}_n(\\theta) = \\prod_{i = 1}^n f (X_i; \\theta) \\,. \\] The log-likelihood function is defined by \\[ \\ell_n (\\theta) h = \\ln \\mathcal{L}_n (\\theta) \\,. \\] The maximum likelihood estimator MLE, denoted by \\(\\hat \\theta_n\\), is the value of \\(\\theta\\) that maximizes \\(\\mathcal{L}_n(\\theta)\\). Notation. Another common notation for the likelihood function is \\[ L(\\theta| X) = \\mathcal{L}_n(\\theta).\\] Example 2.5 Let \\(X_1, \\dots, X_n\\) be sample from \\(\\mathrm{Bernoulli}(p)\\). Use MLE to find an estimator for \\(p\\). Example 2.6 Let \\(X_1, \\dots, X_n\\) be sample from \\(N(\\theta, 1)\\). Use MLE to find an estimator for \\(\\theta\\). Theorem 2.4 Let \\(\\tau = g(\\theta)\\) be a bijective function of \\(\\theta\\). Suppose that \\(\\hat \\theta_n\\) is the MLE of \\(\\theta\\). Then \\(\\hat \\tau_n = g(\\hat \\theta_n)\\) is the MLE of \\(\\tau\\). To discuss about the consistency of the MLE, we define the Kullback-Leibler distance between two pdf \\(f\\) and \\(g\\). \\[ D(f,g) = \\int f(x) \\ln \\left( \\frac{f(x)}{g(x)} \\right) \\, dx.\\] Abusing notation, we will write \\(D(\\theta, \\varphi)\\) to mean \\(D(f(x;\\theta), f(x;\\varphi))\\). We say that a model \\(\\mathcal{F}\\) is identifiable if \\(\\theta \\not= \\varphi\\) implies \\(D(\\theta, \\varphi) &gt; 0\\). Theorem 2.5 Let \\(\\theta_{\\star}\\) denote the true value of \\(\\theta\\). Define \\[ M_n(\\theta)=\\frac{1}{n} \\sum_i \\log \\frac{f\\left(X_i ; \\theta\\right)}{f\\left(X_i ; \\theta_{\\star}\\right)} \\] and \\(M(\\theta)=-D\\left(\\theta_{\\star}, \\theta\\right)\\). Suppose that \\[ \\sup _{\\theta \\in \\Theta}\\left|M_n(\\theta)-M(\\theta)\\right| \\to 0 \\] in probability and that, for every \\(\\epsilon&gt;0\\), \\[ \\sup _{\\theta:|\\theta-\\theta,| \\geq \\epsilon} M(\\theta)&lt;M\\left(\\theta_{\\star}\\right) . \\] Let \\(\\widehat{\\theta}_n\\) denote the MLE. Then \\(\\widehat{\\theta}_n \\to \\theta_{\\star}\\) in probability. "],["empirical-distribution.html", "2.4 Empirical Distribution", " 2.4 Empirical Distribution "],["statistical-functionals.html", "2.5 Statistical Functionals", " 2.5 Statistical Functionals "],["bootstrap.html", "2.6 Bootstrap", " 2.6 Bootstrap "],["parametric-inference-parameter-estimation.html", "Chapter 3 Parametric Inference (Parameter Estimation) ", " Chapter 3 Parametric Inference (Parameter Estimation) "],["method-of-moments-1.html", "3.1 Method of Moments", " 3.1 Method of Moments "],["method-of-maximum-likelihood.html", "3.2 Method of Maximum Likelihood", " 3.2 Method of Maximum Likelihood "],["bayesian-approach.html", "3.3 Bayesian Approach", " 3.3 Bayesian Approach "],["expectation-maximization-algorithm.html", "3.4 Expectation-Maximization Algorithm", " 3.4 Expectation-Maximization Algorithm "],["unbiased-estimators.html", "3.5 Unbiased Estimators", " 3.5 Unbiased Estimators "],["efficiency-cramer-rao-inequality.html", "3.6 Efficiency: Cramer-Rao Inequality", " 3.6 Efficiency: Cramer-Rao Inequality "],["sufficiency-and-unbiasedness-rao-blackwell-theorem.html", "3.7 Sufficiency and Unbiasedness: Rao-Blackwell Theorem", " 3.7 Sufficiency and Unbiasedness: Rao-Blackwell Theorem "],["hypothesis-testing.html", "Chapter 4 Hypothesis Testing ", " Chapter 4 Hypothesis Testing "],["neyman-pearson-lemma.html", "4.1 Neyman-Pearson Lemma", " 4.1 Neyman-Pearson Lemma "],["wald-test.html", "4.2 Wald Test", " 4.2 Wald Test "],["likelihood-ratio-test.html", "4.3 Likelihood Ratio Test", " 4.3 Likelihood Ratio Test "],["comparing-samples.html", "4.4 Comparing samples", " 4.4 Comparing samples "],["part-3-models.html", "PART 3: Models", " PART 3: Models "],["linear-least-squares.html", "Chapter 5 Linear Least Squares ", " Chapter 5 Linear Least Squares "],["simple-linear-regression.html", "5.1 Simple Linear Regression", " 5.1 Simple Linear Regression "],["matrix-approach.html", "5.2 Matrix Approach", " 5.2 Matrix Approach "],["statistical-properties.html", "5.3 Statistical Properties", " 5.3 Statistical Properties "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
