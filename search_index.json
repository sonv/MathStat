<<<<<<< HEAD
[["index.html", "MATH 104: Multivariable Calculus (brief notes) Spring 2024", " MATH 104: Multivariable Calculus (brief notes) Truong-Son Van Spring 2024 "],["disclaimer.html", "Disclaimer", " Disclaimer This is class notes for Multivariable Calculus at Fublbright University Vietnam. I claim no originality in this work as it is mostly taken from the reference books. However, all errors and typos are solely mine. "],["syllabus.html", "Syllabus HOW THIS COURSE WILL FUNCTION Materials and references Assessment Core content Late assignments Time expectations Learning Support Wellbeing Tentative Course Schedule", " Syllabus Key information Instructor: Truong-Son Van Email: son.van+104@fulbright.edu.vn Class time: Mon&amp;Wed 9:45-11:15AM Class Location: Classroom 1 Office hours: M &amp; W: 3-4PM, T &amp; Th: 10-11AM Prerequisites: Calculus (MATH 101) TA: Tran Lan Phuc, phuc.tran.200077@student.fulbright.edu.vn TA office hours: M &amp; F: 2-3PM HOW THIS COURSE WILL FUNCTION This course is going to be very different from what you have been used to with math classes before. You are expected to do preparation work at home before coming to class. Here’s the break down of the process: At home: Students watch assigned YouTube videos from Prof. Rob Ghrist of the University of Pennsylvania. While watching, it is important that you take notes of concepts you don’t understand. (Optional) Try to read in the textbooks/notes about the concepts you don’t understand. In class: Prof. Son will go through the concepts that you don’t understand on the board. You will work through worksheets and together with Prof. Son, you will learn how to solve problems. Materials and references No textbook is required. We will follow the University of Pennsylvania’s guide (https://www.amazon.com/Calculus-Blue-Guide-Robert-Ghrist/dp/1944655077/). However, a physical copy of this book is not required. Required materials YouTube videos from the Calculus BLUE series of user Prof Ghrist Math: First Volume: https://youtu.be/Jes5jwLl1q8?si=SZTtuHAEWKkNtNu- Second Volume: https://youtu.be/HS0mivVKLyo?si=WGrj0tXiB05N9nGZ Third Volume: https://youtu.be/CQr4Dq8aPB4?si=iAqRRAlzycdsZ47C Fourth Volume: https://youtu.be/cFscZ9c0AIk?si=mGAL9921qBBDjk_F Worksheets are designed so to guide you through the process. You will find useful information (including a breakdown of the playlists) on the website of Prof. Rob Ghrist of UPenn, who developed the materials course: https://www2.math.upenn.edu/~ghrist/BLUE.html Additional References The following books are highly recommended. If you find the style of the videos doesn’t fit you and would love to have something concrete to read, they are your friends. Active Calculus: Multivariable by Schlicker et al. 2018 edition. (https://activecalculus.org/multi/preface-2.html) Thomas’ Calculus: Early Transcendentals by Hass, Heil, et al. \\(14^{th}\\) edition. Calculus Early Transcendental by Stewart. \\(8^{th}\\) edition. Anything you can find on Google would work. Calculus is a subject that people have written about so much. So, there’s no excuse for not having access to the knowledge. 3-D grapher: https://www.math3d.org/ Very good graphers: https://www.desmos.com/, https://www.geogebra.org/ Course description How do we describe the trajectory of a space shuttle? How is the human body affected by scuba diving to different depths for different lengths of time? The mathematics required to describe most real life systems involves functions of more than one variable. The concepts of the derivative and integral from a first course in calculus must therefore be extended to higher dimensional settings. In this course students will be guided through the essential ideas of multivariable calculus, including partial derivatives, multiple integrals and vector calculus, and their applications. These mathematical tools are used extensively in the physical sciences and engineering, and in other areas including economics and computer graphics. Learning objectives After the course, students are expected to: Be confident in handling functions of two or more variables and familiar with how they can be represented graphically Understand the key concepts of multivariable calculus, including partial derivatives, the gradient vector, multiple integrals, line and surface integrals, the divergence and curl of a vector function Know how such derivatives and integrals are calculated and some of their uses Be able to apply these ideas to real world problems Have improved analytic, computational and problem solving skills Assessment During the course, students are expected to compute their own percentage points based on the following scheme. The instructor is not responsible for providing the running percentage. Form of assessment Weight Weekly homework 40% Daily quizzes 10% Midterm 20% Final 30% The following is the non-negotiable letter grade breakdown. It is based on common practice in the United States for standard courses such as Calculus. Letter Grade Percentage A [93,100] A- [90,93) B+ [87,90) B [83,87) B- [80, 83) C+ [77,80) C [73,77) C- [70,73) D+ [67,70) D [60, 66) F [0,60) Core content Introduction Functions of two variables Graphs in three dimensions, surfaces and level curves Functions of three or more variables Limits and continuity Vectors (review) Partial Derivatives Partial derivatives Tangent planes, linear approximations and differentials Chain rule Directional derivatives and gradient vectors Extrema and optimization Lagrange Multipliers Multiple Integrals Double integrals Double integrals in polar coordinates Triple integrals Triple integrals in cylindrical and spherical coordinates Applications of multiple integrals Vector Calculus Vector functions and their derivatives Vector fields Line integrals The fundamental theorem of line integrals Green’s Theorem Parametric surfaces and surface integrals Curl and divergence Divergence Theorem Stokes Theorem Late assignments 15% of the possible total mark will be deducted for every 24 hrs (or part of 24 hrs) after the deadline. Work more than 2 days late will not be accepted. Except for exceptional circumstances (see definition), I will not extend the deadlines. Time expectations Some materials require time to be accustomed to. Some students are quicker than others. However, on average, you should expect 10-15 hours per week (including class time) on the materials in order to know the subject relatively well. Collaboration &amp; Plagiarism Plagiarism is the act of submitting the intellectual property of another person as your own. It is one of the most serious of academic offenses. Acts of plagiarism include, but are not limited to: Copying, or allowing someone to copy, all or a part of another person’s work and presenting it as your own, or not giving proper credit. Purchasing a paper from someone (or a website) and presenting it as your own work. Re-submitting your work from another course to fulfill a requirement in another course. Further details can be found in the Code of Academic Integrity [link]. Learning Support In addition to your course instructors, there are other resources available to support your academic work at Fulbright, including one-on-one consultations with learning support staff, supplementary workshops, and both individual and group tutoring and mentoring in course content, language learning, and academic skills. If you would like to request learning support, please contact Fulbright Learning Support (https://learning-support.notion.site). Wellbeing Mental health and wellbeing are essential for the success of your academic journey. The Fulbright Wellness Center provides various services including counseling, safer community, and accessibility services. If you are experiencing undue personal or academic stress, are feeling unsafe, or would like to know more about issues related to wellbeing, please contact the Wellness Center via wellness@fulbright.edu.vn or visit the Wellness Center office on Level 5 of the Crescent campus. For more information, pleaes check https://onestop.fulbright.edu.vn/s/article/Health-and-Wellness-Introduction Tentative Course Schedule The following schedule will be updated as we go so that students will know what to watch/read before/after class. Week Topcs Watch 1 lines, planes, surfaces, coordinates, vectors; dot, cross, &amp; scalar triple products Vol 1: Chapters 1,2,3,4 2 intro to vector calculus Vol 1: Chapters 5,6,7,8 3 motivating matrices, matrix algebra, linear systems, &amp; row reduction, inverses, linear transformation Vol 1: Chapters 9,10,11,12,13, 14 4 multivariate functions &amp; partial derivatives, derivatives as linear transformations Vol 2: Chapters 1, 2, 3, 4 Break Tet (Feb 5 - 23) 5 derivatives as linear transformations, chain rule, derivative rules, inverse &amp; implicit function theorems Vol 2: Chapters 5, 6, 7, 8 6 gradients, tangents, &amp; linearization, multivariate Taylor expansion Vol 2: Chapters 9, 10, 11, 12, 13 7 Applications: Optimization Vol 2: Chapters 14, 15, 16, 17, 18 8 Integrals, Riemann sums Vol 3: Chapters 1, 2, 3, 4, 5 9 Applications: Mass &amp; Probability Vol 3: Chapters 6, 7, 8, 9, 10, 11, 12 10 Changing coordinates Vol 3: Chapters 13, 14, 15, 16, 17, 18 11 Path integrals Vol 4: Chapters 1, 2, 3, 4, 5 12 Differential forms &amp; Fundamental theorems Vol 4: Chapters 6, 7, 8, 9, 10, 11, 12 "],["vectors-matrices.html", "1 Vectors &amp; Matrices 1.1 Basics Rules to manipulate vectors Properties of vector operations 1.2 Products 1.3 Matrices", " 1 Vectors &amp; Matrices 1.1 Basics Reading: Stewart Chapter 12, Thomas Calculus Chapter 12, Active Calculus Chapter 9 You should be able to answer the following questions after reading this section: What is a vector? What does it mean for two vectors to be equal? How do we add two vectors together and multiply a vector by a scalar? How do we determine the magnitude of a vector? What is a unit vector How do we find a unit vector in the direction of a given vector? Typically, we talk about 3-dimensional vectors (as discussed in Stewart and Thomas). However, since talking about \\(n\\)-dimensional vectors doesn’t require much more effort, we will talk about \\(n\\)-dimensional vectors instead. Definition 1.1 An \\(n\\)-dimensional Euclidean space \\(\\mathbb{R}^n\\) is the Cartesian product of \\(n\\) Euclidean spaces \\(\\mathbb{R}\\). Definition 1.2 An \\(n\\)-dimensional vector \\(\\textbf{v}\\in \\mathbb{R}^n\\) is a tuple \\[\\begin{equation} \\textbf{v} = \\langle v_1,\\dots, v_n \\rangle \\,, \\end{equation}\\] where \\(v_i \\in \\mathbb{R}\\). In dimensions less than or equal to 3, we represent a vector geometrically by an arrow, whose length represents the magnitude. Remark. A point in \\(\\mathbb{R}^n\\) is also represented by an \\(n\\)-tuple but with round brackets. A vector connecting two points \\(A= (a_1, \\dots, a_n)\\) and \\(B=(b_1, \\dots, b_n)\\) can be constructed as \\[\\begin{equation*} \\textbf{x} = \\langle b_1-a_1, \\dots, b_n - a_n \\rangle \\,. \\end{equation*}\\] We denote the above vector as \\(\\vec{AB}\\) where \\(A\\) is the tail (initial point) and \\(B\\) is the tip/head (terminal point). We denote \\(\\textbf{0}\\) to be the zero vector, i.e., \\[\\begin{equation*} \\textbf{0} = \\langle 0, \\dots, 0 \\rangle \\,. \\end{equation*}\\] Definition 1.3 The length of a vector \\(\\textbf{v}\\) (denoted by \\(| \\textbf{v}|\\)) is defined to be \\[\\begin{equation} |\\textbf{v}| = \\sqrt{ v_1^2 + \\dots + v_n^2} \\,. \\end{equation}\\] Definition 1.4 A unit vector is a vector that has magnitude 1. Exercise 1.1 Turn a vector \\(\\textbf{v} \\in \\mathbb{R}^n\\) into a unit vector with the same direction. Rules to manipulate vectors Let \\(\\textbf{a}, \\textbf{b} \\in \\mathbb{R}^n\\) and \\(c,d \\in \\mathbb{R}\\). Then, \\[\\begin{equation*} c( \\textbf{a} + \\textbf{b}) = \\langle c a_1 + c b_1, \\dots, c a_n + c b_n \\rangle = c\\textbf{a} + c\\textbf{b} \\,, \\end{equation*}\\] and \\[\\begin{equation*} (c+d) \\textbf{a} = c\\mathbf{a} + d\\mathbf{a} \\,. \\end{equation*}\\] These formulas are deceptively simple. Make sure you understand all the implications. Because of this rule, sometimes it is good to write vectors in terms of elementary vectors: \\[\\begin{equation*} \\mathbf{u} = u_1 \\mathbf{e_1} + \\dots + u_n \\mathbf{e_n} \\,, \\end{equation*}\\] where \\(e_i = \\langle 0,\\dots, 1, \\dots, 0\\rangle\\) is the vector which has zero at all entries except that the \\(i^{th}\\) entry is 1. In 3D, \\[\\begin{equation*} \\mathbf{e_1} = \\mathbf{i} \\,, \\qquad \\mathbf{e_2} = \\mathbf{j} \\,, \\qquad \\mathbf{e_3} = \\mathbf{k} \\,. \\end{equation*}\\] Properties of vector operations Read the book (Make sure you understand the geometric intepretation) 1.2 Products 1.2.1 Dot product How is the dot product of two vectors defined and what geometric information does it tell us? How can we tell if two vectors in \\(\\mathbb{R}^n\\) are perpendicular? How do we find the projection of one vector onto another? Definition 1.5 The dot product of vectors \\(\\textbf{u} = \\langle u_1, \\dots, u_n \\rangle\\) and \\(\\textbf{v} = \\langle v_1, \\dots, v_n \\rangle\\) in \\(\\mathbb{R}^n\\) is the scalar \\[\\begin{equation*} \\textbf{u} \\cdot \\textbf{v} = u_1 v_1 +\\dots + u_n v_n \\,. \\end{equation*}\\] Properties of dot product Let \\(\\textbf{u}, \\textbf{v}, \\textbf{w} \\in \\mathbb{R}^n\\). Then, \\(\\textbf{u}\\cdot \\textbf{v} = \\textbf{v}\\cdot \\textbf{u}\\), \\((\\textbf{u} + \\textbf{v})\\cdot \\textbf{w} = (\\textbf{u}\\cdot \\textbf{w}) + (\\textbf{v}\\cdot \\textbf{w})\\), If \\(c\\) is a scalar, then \\((c \\textbf{u})\\cdot \\textbf{w} = c (\\textbf{u}\\cdot \\textbf{w})\\). Theorem 1.1 (Law of cosine) If \\(\\theta\\) is the angle between the vectors \\(\\textbf{u}\\) and \\(\\textbf{v}\\), then \\[\\begin{equation*} \\textbf{u}\\cdot \\textbf{v} = |\\textbf{u}|| \\textbf{v}| \\cos \\theta \\,. \\end{equation*}\\] Corollary 1.1 Two vectors \\(\\textbf{u}\\) and \\(\\textbf{v}\\) are orthogonal to each other if \\(\\textbf{u} \\cdot \\textbf{v} = 0\\). Projection Let \\(\\textbf{u}, \\textbf{v}\\in \\mathbb{R}^n\\). The component of \\(\\textbf{u}\\) in the direction of \\(\\textbf{v}\\) is the scalar \\[\\begin{equation*} \\mathrm{comp}_{\\mathbf{v}}\\mathbf{u} = \\frac{\\mathbf{u}\\cdot \\mathbf{v}}{|\\mathbf{v}|} \\,, \\end{equation*}\\] and the projection of \\(\\mathbf{u}\\) onto \\(\\mathbf{v}\\) is the vector \\[\\begin{equation*} \\mathrm{proj}_{\\mathbf{v}}\\mathbf{u} =\\left( \\mathbf{u}\\cdot \\frac{\\mathbf{v}}{|\\mathbf{v}|}\\right) \\frac{\\mathbf{v}}{|\\mathbf{v}|} = \\frac{\\mathbf{u}\\cdot \\mathbf{v}}{\\mathbf{v} \\cdot\\mathbf{v}} \\mathbf{v} \\,. \\end{equation*}\\] 1.2.2 3D special: Cross product This concept is very specific to \\(\\mathbb{R}^3\\). It will not make sense in other dimensions. Definition 1.6 Let \\(\\mathbf{a}, \\mathbf{b} \\in \\mathbb{R}^3\\). The cross product of \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) is defined to be \\[\\begin{equation*} \\mathbf{a} \\times \\mathbf{b} = \\langle a_2 b_3 - a_3 b_2, a_3b_1 - a_1 b_3, a_1b_2 - a_2b_1 \\rangle \\,. \\end{equation*}\\] Theorem 1.2 Let \\(\\theta\\) be the angle between \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\). Then, \\[\\begin{equation*} | \\mathbf{a} \\times \\mathbf{b} | = |\\mathbf{a}||\\mathbf{b}| \\sin\\theta \\,. \\end{equation*}\\] Theorem 1.3 The vector \\(\\mathbf{a}\\times \\mathbf{b}\\) is orthogonal to both \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\). 1.2.3 Distance from a point We can use the cross and dot products to measure the distance of one point to either a plane or a line. Let \\(P \\in \\mathbb{R}^n\\) and \\(\\vec{r}(t) = R_0 + t \\vec{v}\\) be a line. Then the distance from \\(P\\) to \\(\\vec{r}(t)\\) is \\[ Dist = \\frac{| \\vec{R_0 P} \\times \\vec{v}|}{| \\vec{v} |}\\] 1.3 Matrices A matrix is an 2 dimensional array with rows and columns. \\[ A = \\begin{pmatrix} A_{11} &amp; \\dots &amp; A_{1n}\\\\ \\vdots &amp; &amp; \\vdots \\\\ A_{n1} &amp; \\dots &amp; A_{nn} \\end{pmatrix}\\] Another way to write out matrix \\(A\\) is \\[ A = (A_{ij})\\] where the first index \\(i\\) represents the row and the second index \\(j\\) represents the column. 1.3.1 Operations on matrices Addition: let \\(A\\) and \\(B\\) be two matrices with same dimension \\(m\\times n\\). Then \\(A + B\\) is an \\(m\\times n\\) matrix such that \\[[A + B]_{ij} = A_{ij} + B_{ij}.\\] Scalar multiplication: let \\(A\\) be a \\(m\\times n\\) matrix, \\(c\\) is a constant scalar. then \\(cA\\) is a \\(m\\times n\\) matrix such that \\[((cA)_{ij}) = (cA_{ij}).\\] Matrix multiplication: let \\(A\\) be \\(m\\times n\\) matrix and \\(B\\) be \\(n\\times l\\) matrix. Then the multiplication \\(AB\\) is a \\(m\\times l\\) matrix such that \\[ [AB]_{ij} = \\sum_{k} A_{ik} B_{kj} .\\] 1.3.2 Linear transformation A linear transformation is a function \\(f: \\mathbb{R}^n \\to \\mathbb{R}^m\\) such that \\[ f(a \\vec{u} + b \\vec{v} ) = a f(\\vec{u}) + b f(\\vec{v}) \\] for all \\(a,b \\in \\mathbb{R}\\) and \\(u,v \\in \\mathbb{R}^n\\). It turns out that every linear transformation \\(f: \\mathbb{R}^n \\to \\mathbb{R}^m\\) can be represented as a \\(m\\times n\\) matrix. "],["some-basic-equations-in-mathbbr3.html", "2 Some basic equations in \\(\\mathbb{R}^3\\) 2.1 Equations for lines 2.2 Equations for planes 2.3 Cylinders 2.4 Quadric surfaces", " 2 Some basic equations in \\(\\mathbb{R}^3\\) Just to build some toy examples for the future, we will play with some basic equations in three dimensions. 2.1 Equations for lines A line is a collection of points that is parallel to a vector and goes through a specific point. To capture this idea, we have the following representation for a line \\[\\begin{equation*} L = \\{\\mathbf{r}(t) \\,| \\mathbf{r}(t) = \\mathbf{r}_0 + t \\mathbf{v}, t\\in \\mathbb{R}\\} \\,, \\end{equation*}\\] where \\({r}_0\\) is the initial position and \\(\\mathbf{v}\\) is the direction. The equation for \\(\\mathbf{r}(t)\\) is called a vector equation for a line \\(L\\). Let \\(\\mathbf{v} = \\langle v_1, v_2, v_3 \\rangle\\) and \\(\\mathbf{r}_0 = ( x_0, y_0, z_0 )\\). The parametric equations of \\(L\\) is the following system of equations \\[\\begin{gather*} x = x_0 + v_1 t\\,, \\\\ y = y_0 + v_2 t\\,, \\\\ z = z_0 + v_3 t \\,. \\end{gather*}\\] This leads to the symmetric equations of \\(L\\) \\[\\begin{equation*} \\frac{x - x_0}{v_1} = \\frac{y - y_0}{v_2} = \\frac{z - z_0}{v_3} \\,. \\end{equation*}\\] Definition 2.1 Two lines are parallel if their directional vectors are parallel (scalar multiple of each other). Two lines that are not parallel and don’t intersect each other are said to be skew. 2.2 Equations for planes A plane is a collection of points that is perpendicular to one specific direction represented by a some vector called a normal vector. Note that due to scaling, there are more than one normal vector. To capture this idea, we have the following representation of a plane \\[\\begin{equation*} P = \\{ \\mathbf{r} \\, | \\, \\mathbf{n} \\cdot (\\mathbf{r}- \\mathbf{r}_0 ) = 0 \\} \\,. \\end{equation*}\\] This is called a vector equation for the plane \\(P\\). Multiplying things out, we have the scalar equation of the plane \\(P\\) with normal vector \\(\\mathbf{n} = \\langle n_1, n_2, n_3 \\rangle\\) through a point \\(P_0(x_0, y_0, z_0)\\) \\[\\begin{equation*} n_1(r_1- x_0) + n_2 (r_2 - y_0) + n_3(r_3 - z_0) = 0 \\,. \\end{equation*}\\] The equation of the form \\[\\begin{equation*} ax + by + cz + d = 0 \\end{equation*}\\] is called a linear equation. Definition 2.2 Two planes are said to be parallel if their normal vectors are parallel. If two planes are not parallel, they intersect in a straight line and the angle between the two planes is defined to be the angle between the two normal vectors. 2.3 Cylinders Definition 2.3 A cylinder is a surface that consists of all lines (called rulings) that are parallel to a given line. Example 2.1 \\(z = x^2\\) \\(x^2 + y^2 = 1\\) 2.4 Quadric surfaces Definition 2.4 A quadric surface is the graph of a second-degree equation in three variables \\(x,y\\) and \\(z\\). The equation that represents these surfaces is \\[Ax^2 + By^2 + Cz^2 + Dz = E\\,.\\] Example 2.2 Ellipsoid \\[\\frac{x^2}{a^2} + \\frac{y^2}{b^2} + \\frac{z^2}{c^2} = 1\\,. \\] Hyperbolic paraboloid \\[\\frac{y^2}{b^2} - \\frac{x^2}{a^2} = \\frac{z}{c} \\,.\\] Elliptical cone \\[\\frac{x^2}{a^2} + \\frac{y^2}{b^2} = \\frac{z^2}{c^2} \\,.\\] Read the books for more surfaces and pictures. "],["functions-in-higher-dimensions.html", "3 Functions in higher dimensions 3.1 Functions of several variables 3.2 Vector functions 3.3 Activity: on osculating circle and curvature", " 3 Functions in higher dimensions 3.1 Functions of several variables Definition 3.1 A function of several variables is a function \\(f: D \\to C\\) where \\(D \\subseteq \\mathbb{R}^m\\) and \\(C \\subseteq \\mathbb{R}^n\\), where \\(m\\geq 2\\) and \\(n\\geq 1\\). \\[f({x}) = ( f_1(x_1,\\dots, x_m),\\dots, f_n(x_1,\\dots, x_m) ) \\,.\\] \\(D\\) is called the domain of \\(f\\) and \\(C\\) is called the codomain of \\(f\\). The domain of \\(f\\) is where each of the component \\(f_i\\) of \\(f\\) is defined. Example 3.1 The following are some examples of multivariable functions \\(f(x,y) = x^2 - 2xy + y^2\\) \\(f(x,y,z) = \\frac{1}{1 - xy^2}\\) 3.2 Vector functions 3.2.1 Limit, continuity and differentiation The expression in the vector equation for a line is an example of a function that maps from \\(\\mathbb{R}\\) to \\(\\mathbb{R}^n\\). There’s no one who would stop us from considering more general kinds of function. Definition 3.2 A vector function (vector-valued function) is a function that has the codomain that belongs to \\(\\mathbb{R}^n\\) where \\(n\\geq 2\\). In other words, \\(f: D \\to \\mathbb{R}^n\\). Example 3.2 The following are some examples of vector functions. \\(\\mathbf{r}(t) = \\mathbf{r}_0 + t\\mathbf{v}\\) \\(\\mathbf{f}(t) = \\langle \\cos(t),\\sin(t), t \\rangle\\) Note that my definition is more general than that in the book. However, In this course, whenever we talk about vector valued function, we will only refer to that which has one dimensional domain (\\(D \\subseteq \\mathbb{R}\\)). By and large, there’s nothing different between a vector function and a one-variable scalar function. All the concepts such as limit, continuity and differentiability are applied to each coordinate the same way as in one dimensional case. Theorem 3.1 Let \\(\\mathbf{r}: \\mathbb{R}\\to \\mathbb{R}^n\\), given by \\(\\mathbf{r}(t) = \\langle r_1(t), \\dots , r_n(t) \\rangle\\). Then, \\(\\mathbf{r}\\) is said to be continuous at \\(t_0\\) if \\[\\begin{equation*} \\mathbf{r}(t_0) = \\lim_{t\\to t_0} \\mathbf{r}(t) \\,, \\end{equation*}\\] where \\[\\begin{equation*} \\lim_{t\\to t_0} \\mathbf{r}(t) = \\langle \\lim_{t\\to t_0}r_1(t) , \\dots , \\lim_{t\\to t_0} r_n(t) \\rangle \\,. \\end{equation*}\\] Furthermore, we can define the derivative of \\(\\mathbf{r}\\) \\[\\begin{equation*} \\frac{d}{dt} \\mathbf{r}(t) = \\mathbf{r}&#39;(t) = \\lim_{h\\to 0} \\frac{\\mathbf{r}(t+h) - \\mathbf{r}(t)}{h} \\end{equation*}\\] if this limit exists. When \\(\\mathbf{r}:I \\to \\mathbb{R}^n\\) (\\(I\\) is an interval in \\(\\mathbb{R}\\)) is continuous, we call it a space curve (to describe the intuitive picture of what a curve should look like in our mind). Geometrically, if \\(\\mathbf{r}&#39;(t)\\) exists and \\(\\mathbf{r}&#39;(t) \\not= \\mathbf{0}\\), it represents the tangent vector of the curve \\(\\mathbf{r}\\) at \\(t\\). Definition 3.3 A parametric equation for a curve is an equation of the form \\[ x=x(t)\\,, \\quad y = y(t)\\,, \\quad z = z(t) \\,. \\] Typical differentiation rules apply. Theorem 3.2 (Differentiation rules) \\((\\mathbf{u}(t) + \\mathbf{v}(t))&#39; = \\mathbf{u}&#39;(t) + \\mathbf{v}&#39;(t)\\) \\((c \\mathbf{u}(t))&#39; = c \\mathbf{u}&#39;(t)\\) \\((f(t) \\mathbf{u}(t))&#39; = f&#39;(t) \\mathbf{u}(t) + f(t) \\mathbf{u}&#39;(t)\\) \\((\\mathbf{u}(t) \\cdot \\mathbf{v}(t))&#39; = \\mathbf{u}&#39;(t)\\cdot \\mathbf{v}(t) + \\mathbf{u}(t)\\cdot \\mathbf{v}&#39;(t)\\) \\((\\mathbf{u}(t) \\times \\mathbf{v}(t))&#39; = \\mathbf{u}&#39;(t)\\times \\mathbf{v}(t) + \\mathbf{u}(t)\\times \\mathbf{v}&#39;(t)\\) \\((\\mathbf{u}(f(t)))&#39; = \\mathbf{u}&#39;(f(t)) f&#39;(t)\\) 3.2.2 Integrals There are different ways to play with integrals for vector functions, each has its own interpretation and physical applications. 3.2.2.1 Indefinite integral \\[\\begin{equation*} \\int_a^b \\mathbf{r}(t) \\, dt = \\left\\langle \\int_a^b r_1(t) \\, dt, \\int_a^b r_2(t) \\, dt, \\int_a^b r_3(t) \\, dt \\right\\rangle \\end{equation*}\\] 3.2.2.2 Arc Length and curvature Definition 3.4 The length the curve \\(\\mathbf{r}:[a,b] \\to \\mathbb{R}^n\\) is defined to be \\[\\begin{equation*} L = \\int_a^b \\left| \\mathbf{r}&#39;(t) \\right| \\, dt \\,. \\end{equation*}\\] If one wants to keep track the length of the curve \\(\\mathbf{r}:[a,b] \\to \\mathbb{R}^n\\) made by an airplane at any time \\(t\\), one uses the arc length function \\[\\begin{equation*} \\ell(t) = \\int_a^t \\left| \\mathbf{r}&#39;(u) \\right| \\, du \\,. \\end{equation*}\\] Re-parametrize with respect to arc length The nice thing about \\(\\ell(t)\\) is that it is a strictly increasing function with respect to \\(t\\), given that \\(\\mathbf{r}&#39;\\) is non-zero for all \\(t\\). Therefore, letting \\(s = \\ell(t)\\), we can talk about the inverse of \\(\\ell\\), \\(\\ell^{-1}:[0,L] \\to [a,b]\\) \\[\\begin{equation*} t = \\ell^{-1}(s) \\,. \\end{equation*}\\] Therefore, we can re-write \\[\\begin{equation*} \\mathbf{r}(t) = \\mathbf{r}(\\ell^{-1}(s)) \\,. \\end{equation*}\\] Theorem 3.3 \\[\\left| \\frac{d r(t)}{ds} \\right| = 1 \\,.\\] Thus, \\[l(s) = \\int_0^s \\left| \\frac{d}{ds} \\mathbf{r}(t) \\right| \\, dt = s \\,.\\] Because of the unchanging nature of the arc-length (with respect to the parametrization), it is used to define a geometric quantity of a space curve called curvature. Definition 3.5 (curvature) Let \\(\\mathbf{T}(t)\\) be the unit tangent vector of the curve \\(\\mathbf{r}:[a,b] \\to \\mathbb{R}^3\\). The curvature of \\(\\mathbf{r}(t(s))\\) is defined to be \\[\\begin{equation*} \\kappa(s) = \\left| \\frac{d \\mathbf{T}(t(s))}{ds}\\right| \\,. \\end{equation*}\\] To convert this into the parameter \\(t\\), we write \\(s= s(t)\\) and use chain rule to get. Theorem 3.4 We have that \\[\\begin{equation*} \\kappa(s(t)) = \\frac{|\\mathbf{T}&#39;(t)|}{|\\mathbf{r}&#39;(t)|} \\,. \\end{equation*}\\] 3.2.3 Space curve in \\(\\mathbb{R}^3\\) and motion in space Read the book. This part is not required but it is so beautiful, you may want to read it as an exercise at home (to test how much you understand what we’ve been discussing so far). 3.3 Activity: on osculating circle and curvature For those who are interested in the geometrical meaning of the curvature without having to accept from the book that the curvature is the inverse of the radius of the osculating circle, please take a look at https://github.com/sonv/MultiCalc/blob/main/Writing/latexbuild/osculating.pdf. "],["partial-derivatives.html", "4 Partial derivatives 4.1 Multivariable scalar function 4.2 Limits and continuity 4.3 Partial derivatives 4.4 Differentiability 4.5 Chain rule 4.6 Directional derivative 4.7 Tangent planes", " 4 Partial derivatives Read Stewart Chapter 14, Thomas Chapter 14, We will study multivariable scalar functions \\[ f: D \\to \\mathbb{R}\\,,\\] where \\(D\\subseteq \\mathbb{R}^n\\), \\(n\\geq 2\\). 4.1 Multivariable scalar function The following definition is from Thomas’s book. Definition 4.1 Suppose \\(D\\) is a set of \\(n\\)-tuples of real numbers \\((x_1, x_2, \\ldots, x_n)\\). A real-valued/scalar function \\(f\\) on \\(D\\) is a rule that assigns a unique (single) real number \\[w = f(x_1, x_2, \\ldots, x_n)\\] to each element in \\(D\\). The set \\(D\\) is the function’s domain. The set of \\(w\\)-values taken on by \\(f\\) is the function’s range. The symbol \\(w\\) is the dependent variable of \\(f\\), and \\(f\\) is said to be a function of the \\(n\\) independent variables \\(x_1\\) to \\(x_n\\). We also call the \\(x_j\\)’s the function’s input variables and call \\(w\\) the function’s output variable. As prototypes, we only focus on \\(n=2\\) and \\(n=3\\). 4.1.1 Graphs Definition 4.2 The graph of function \\(f:D \\to \\mathbb{R}\\) is the set of all points \\((\\mathbf{x}, f(\\mathbf{x}))\\), where \\(\\mathbf{x}\\in D\\). Here \\(D\\subseteq \\mathbb{R}^n\\). For 2D, the graph of \\(f\\) is also called the surface \\(z = f(x_1,x_2)\\). We cannot visualize the graph of a 3D function since it will be a four dimensional object. 4.1.2 Level Definition 4.3 In 2D, the \\(c\\)-level curves of a function \\(f\\) of two variables are curves with equations \\(f(x,y) = c\\), where \\(c\\) is a constant. In 3D, the \\(c\\)-level surface of a function \\(f\\) of three variables are surfaces with equations \\(f(x,y,z) = c\\), where \\(c\\) is a constant. 4.2 Limits and continuity The following definition is from Stewart. Definition 4.4 Let \\(f\\) be a function of two variables whose domain \\(D\\) includes points arbitrarily close to \\((a,b)\\). Then we say that the limit of \\(f(x,y)\\) as \\((x,y)\\) approaches \\((a,b)\\) is \\(L\\) and we write \\[\\lim_{(x,y)\\to(a,b)} f(x,y) = L\\] if for every number \\(\\epsilon &gt; 0\\) there is a corresponding number \\(\\delta &gt; 0\\) such that \\(|f(x,y) - L| &lt; \\epsilon\\) if \\((x,y) \\in D\\) and \\(0 &lt; \\sqrt{(x-a)^2 + (y-b)^2} &lt; \\delta\\). Finding if a function has limit as a point in higher dimension is not as simple as the case for 1 dimension. Determining whether a multivariable function has a limit sometimes is an art and it requires a lot of experiences and practice. However, there are certain rules that could help us. Theorem 4.1 Let \\(L,M\\) and \\(k\\) be real numbers and that \\[\\begin{equation*} \\lim_{(x,y) \\to (x_0,y_0)} f(x,y) = L \\,, \\qquad \\lim_{(x,y) \\to (x_0,y_0)} g(x,y) = M \\,. \\end{equation*}\\] We then have \\(\\displaystyle \\lim_{(x,y) \\to (x_0,y_0)} (f(x,y) + g(x,y)) = L + M\\), \\(\\displaystyle \\lim_{(x,y) \\to (x_0,y_0)} (k f(x,y)) = kL\\), \\(\\displaystyle \\lim_{(x,y) \\to (x_0,y_0)} (f(x,y) g(x,y)) = LM\\), \\(\\displaystyle \\lim_{(x,y) \\to (x_0,y_0)} \\frac{f(x,y)}{g(x,y)} = \\frac{L}{M}\\) if \\(M \\not= 0\\), \\(\\displaystyle \\lim_{(x,y) \\to (x_0,y_0)} {f(x,y)^p} = L^p\\) for \\(p&gt;0\\), Strategy to find out that a two-variable function does NOT have a limit. If \\(\\lim_{(x,y) \\to (a,b)} f(x,y) = L_1\\) as \\((x,y) \\to (a,b)\\) along a path \\(C_1\\), and \\(\\lim_{(x,y) \\to (a,b)} f(x,y) = L_2\\) as \\((x,y) \\to (a,b)\\) along a path \\(C_2\\), where \\(L_1 \\neq L_2\\), then \\(\\lim_{(x,y) \\to (a,b)} f(x,y)\\) does not exist. Example 4.1 \\(\\lim_{(x,y)\\to (0,0)} \\frac{x^2 - y^2}{x^2 + y^2}\\) does not exist. \\(\\lim_{(x,y)\\to (0,0)} \\frac{xy}{x^2 + y^2}\\) does not exist. \\(\\lim_{(x,y)\\to (0,0)} \\frac{xy^2}{x^4 + y^4}\\) does not exist. \\(\\lim_{(x,y)\\to (0,0)} \\frac{3x^2y}{x^2 + y^2} = 0\\). 4.3 Partial derivatives Given a function \\(f(x,y)\\). The partial derivative of \\(f\\) with respect to \\(x\\) and \\((a,b)\\), denoted by \\(f_x(a,b)\\), is defined to be \\[\\begin{equation*} f_x(a,b) = \\lim_{h\\to 0} \\frac{ f(a+h,b) - f(a,b)}{h} \\,. \\end{equation*}\\] Likewise, the partial derivative of \\(f\\) with respect to \\(y\\) and \\((a,b)\\), denoted by \\(f_y(a,b)\\), is defined to be \\[\\begin{equation*} f_y(a,b) = \\lim_{h\\to 0} \\frac{ f(a,b+h) - f(a,b)}{h} \\,. \\end{equation*}\\] Instead of thinking about derivative with respect to \\(x,y\\), we could think about derivative with respect to the first and second direction. This way of thinking is a bit better when one thinks about higher dimension. Notations. If \\(z = f(x,y)\\), we write \\[\\begin{equation*} f_x(x,y) = f_x = \\partial_x f = \\frac{\\partial f}{\\partial x} = \\frac{\\partial}{\\partial x} f(x,y) = f_1 = D_1 f = D_x f \\,. \\end{equation*}\\] When you take partial derivatives, just treat other variables as constants and proceed as in the case of one dimension. More generally, given a function \\(f(x_1, \\dots, x_n)\\), its partial derivative with respect to the \\(i\\)th variable \\(x_i\\) is \\[\\begin{equation*} f_{x_i}(x_1, \\dots, x_n) = \\lim_{h\\to 0} \\frac{ f(x_1, \\dots, x_{i-1}, x_i + h , x_{i+1}, \\dots, x_n) - f(x_1, \\dots, x_{i-1}, x_i , x_{i+1}, \\dots, x_n)}{h} \\,. \\end{equation*}\\] From here, one can define higher partial derivatives such as the following \\[\\begin{equation*} \\partial^3_{x_1 x_2 x_2} f\\,. \\end{equation*}\\] Note that the power over the symbol \\(\\partial\\) represents the order of derivatives. Theorem 4.2 (Clairaut's Theorem) Suppose \\(f\\) is defined on a disk \\(D\\) that contains the point \\((a,b)\\). If the functions \\(f_{xy}\\) and \\(f_{yx}\\) are both continuous on \\(D\\), then \\[\\begin{equation*} f_{xy}(a,b) = f_{yx}(a,b) \\,. \\end{equation*}\\] Some important notations Let \\(f:D \\to \\mathbb{R}\\) be a function. We write the following, if exist, \\[\\begin{equation*} \\nabla f = \\begin{bmatrix} \\partial_{x_1} f\\\\ \\vdots \\\\ \\partial_{x_n} f\\\\ \\end{bmatrix} \\end{equation*}\\] \\[\\begin{equation*} \\Delta f = \\partial_{x_1}^2 f + \\dots \\partial_{x_n}^2 f \\,. \\end{equation*}\\] 4.4 Differentiability Let \\(S\\) be a surface that has equation \\(z = f(x,y)\\). Let \\(C_1\\) and \\(C_2\\) be two different curves on a surface \\(S\\) intersect at a point \\(P(x_0, y_0, z_0)\\). Heuristically, “the” tangent plane to the surface \\(S\\) at point \\(P\\) is defined the be the plane that contains both of the tangent lines of both curves at \\(P\\). How do you know if there is only one tangent plane at a point? You actually don’t know. That’s why the following definition exists. Definition 4.5 (Differentiability) Let \\(f:D \\to \\mathbb{R}\\) and \\(a\\in \\mathbb{R}^n\\). Let \\(z = f(x)\\) and \\(\\Delta z = f(a + \\Delta x ) - f(a)\\). Then \\(f\\) is differentiable at \\(a\\) if \\(\\Delta z\\) can be expressed in the form \\[\\begin{equation*} \\Delta z = \\sum_{i=1}^n \\partial_i f(a) \\Delta x_i + \\epsilon_i \\Delta x_i \\,, \\end{equation*}\\] where \\(\\epsilon_i \\to 0\\) as \\(\\Delta x_i \\to (0,0)\\) and \\(\\Delta x_i = x_i - a_i\\). \\(f\\) is said to be differentiable if it is differentiable at every point on the domain. The graph of a differentiable function \\(f\\) whose domain is two dimensional is called a smooth surface. Let \\(\\Delta x = (\\Delta x_1, \\dots, \\Delta x_n)\\). Another reformulation of the definition of differentiability is that \\(f\\) is differentiable if \\[\\begin{equation*} \\lim_{|\\Delta x| \\to 0} \\frac{\\Delta z - \\nabla f \\cdot \\Delta x }{| \\Delta x|} = 0 \\,. \\end{equation*}\\] It would be a good exercise to see why this is equivalent with the definition above. For some good intuition, please go to https://mathinsight.org/differentiability_multivariable_definition. Theorem 4.3 If the partial derivatives \\(\\partial_i f\\) (\\(i= 1, \\dots, n\\)) exist near \\(a\\in \\mathbb{R}^n\\) and are continuous at \\(a\\), then \\(f\\) is differentiable at \\(a\\). Theorem 4.4 If a function \\(f(x)\\) is differentiable at \\(a\\) then \\(f\\) is continuous at \\(a\\). 4.5 Chain rule Theorem 4.5 Let \\(f(x_1,\\dots, x_n), g_i(y_1,\\dots, y_m)\\) (\\(i = 1,\\dots, n\\)) be differentiable functions. Then, \\[z(y_1, \\dots, y_m) = f(g_1(y_1, \\dots, y_m), \\dots, g_n(y_1, \\dots, y_m))\\] is differentiable and \\[\\begin{equation*} \\frac{\\partial z}{\\partial y_i} = \\sum_{j=1}^n \\frac{\\partial f}{\\partial x_j} \\frac{\\partial g_j}{\\partial y_i} \\,. \\end{equation*}\\] 4.6 Directional derivative Definition 4.6 Let \\(\\mathbf{u} \\in \\mathbb{R}^n\\). The directional derivative of \\(f:\\mathbb{R}^n \\to \\mathbb{R}\\) at \\(a\\in \\mathbb{R}^n\\) in the direction of \\(\\mathbf{u}\\) is the following limit (if exists) \\[\\begin{equation*} D_{\\mathbf{u}} f(a) = \\lim_{h \\to 0} \\frac{ f( a + h \\mathbf{u}) - f(a)}{h}\\,. \\end{equation*}\\] How can one compute directional derivative? Theorem 4.6 If \\(f:\\mathbb{R}^n \\to \\mathbb{R}\\) is differentiable then \\[\\begin{equation*} D_{\\mathbf{u}} f(a) = \\nabla f(a) \\cdot \\mathbf{u} \\,. \\end{equation*}\\] 4.7 Tangent planes Let’s think about tangent planes in a more systematic way, based on the definition of a plane learned in the first chapter. Recall the \\(c\\)-level surface of a function \\(f(x,y,z)\\) is the collection \\[\\begin{equation*} \\{ (x,y,z) | f(x,y,z) = c \\} \\,. \\end{equation*}\\] Definition 4.7 The tangent plane at the point \\(P(x_0, y_0, z_0)\\) on the \\(c\\)-level surface of a differentiable \\(f\\) is the plane through \\(P_0\\), normal to \\(\\nabla f (x_0, y_0, z_0)\\). "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
=======
[["index.html", "MATH 310: Mathematical Statistics (brief notes) Disclaimer", " MATH 310: Mathematical Statistics (brief notes) Truong-Son Van Disclaimer This is class notes for Mathematical Statistics at Fublbright University Vietnam. I claim no originality in this work as it is mostly taken from the reference books. However, all errors and typos are solely mine. "],["syllabus.html", "Syllabus Textbooks and references Assessment Core content Project description Late assignments Time expectations Learning Support Wellbeing Tentative Course Schedule", " Syllabus Key information Instructor: Truong-Son Van Email: son.van+310@fulbright.edu.vn Class time: T &amp; Th: 3:00 PM - 4:30 PM Class Location: BRDWY Music room Office hours: M &amp; W: 3-4PM, T &amp; Th: 10-11AM Prerequisites: Multivariable Calculus (MATH 104) and Probability (MATH 205) Textbooks and references John Rice, Mathematical Statistics &amp; Data Analysis, 3rd Edition (main reference) Cassella and Berger, Statistical Inference, 2nd Edition Larry Wasserman, All of Statistics Course description Learning objectives Assessment During the course, students are expected to compute their own percentage points based on the following scheme. The instructor is not responsible for providing the running percentage. Form of assessment Weight Weekly homeworks 40% Mini-project 15% Midterm 20% Final 25% The following is the letter grade breakdown. It is based on common practice in the United States. Letter Grade Percentage A [93,100] A- [90,93) B+ [87,90) B [83,87) B- [80, 83) C+ [77,80) C [73,77) C- [70,73) D+ [67,70) D [60, 66) F [0,60) Core content Probability Review (2 weeks) Random Variables Concentration inequality (non-asymptotic theory) Limit Theorems (asymptotic theory) Special distributions Sampling distribution, confidence interval (Rice, 6.3, 7.1 – 7.3) (1 week) Parameter estimation &amp; Method of moments (Rice, 8.1 – 8.4) (1 week) Maximum likelihood estimation (Rice, 8.5) (1 week) Expection-Maximization Algorithm (Wasserman, 9.13.4) (1 week) Bayesian approach to parameter estimation (Wasserman, 11) (1 week) Unbiased estimators, Efficiency &amp; Cramer-Rao Inequality (Casella &amp; Berger, 7.3.2, Rice, 8.7) (1 week) Sufficiency and unbiasedness, Rao-Blackwell Theorem (Casella &amp; Berger, 7.3.3, Rice, 8.8) (1 week) Hypothesis testings, Neyman-Pearson Lemma, Wald test, Likelihood Ratio test (Rice, 9) (2 weeks) Comparing samples (Rice, 11) (1 week) Analysis of Variance (Rice, 12) (1 week) Linear regression and least squares (Wasserman, 13.1-13.3, Casella &amp; Berger, 11.3) (1 week) Project description Project Guidelines You may work alone or in team of two. You need to discuss with me a proposal for your project. The proposal should include your group members names &amp; IDs, a brief description of the content of your project (as clear and explicit as possible), how you plan to present it, and a suggested grading rubric. You will need to deliver a 15 minutes presentation as well as a 5-page-minimum write up in 1.5 spacing). Project Timeline TBD Possible topics TBD Late assignments 15% of the possible total mark will be deducted for every 24 hrs (or part of 24 hrs) after the deadline. Work more than 2 days late will not be accepted. Except for exceptional circumstances (see definition), I will not extend the deadlines. Time expectations Some materials require time to be accustomed to. Some students are quicker than others. However, on average, you should expect 10-15 hours per week (including class time) on the materials in order to know the subject relatively well. Collaboration &amp; Plagiarism Plagiarism is the act of submitting the intellectual property of another person as your own. It is one of the most serious of academic offenses. Acts of plagiarism include, but are not limited to: Copying, or allowing someone to copy, all or a part of another person’s work and presenting it as your own, or not giving proper credit. Purchasing a paper from someone (or a website) and presenting it as your own work. Re-submitting your work from another course to fulfill a requirement in another course. Further details can be found in the Code of Academic Integrity [link]. Learning Support In addition to your course instructors, there are other resources available to support your academic work at Fulbright, including one-on-one consultations with learning support staff, supplementary workshops, and both individual and group tutoring and mentoring in course content, language learning, and academic skills. If you would like to request learning support, please contact Fulbright Learning Support (https://learning-support.notion.site). Wellbeing Mental health and wellbeing are essential for the success of your academic journey. The Fulbright Wellness Center provides various services including counseling, safer community, and accessibility services. If you are experiencing undue personal or academic stress, are feeling unsafe, or would like to know more about issues related to wellbeing, please contact the Wellness Center via wellness@fulbright.edu.vn or visit the Wellness Center office on Level 5 of the Crescent campus. For more information, pleaes check https://onestop.fulbright.edu.vn/s/article/Health-and-Wellness-Introduction Tentative Course Schedule Lecture Topic Date 1 Probability review 01/09 2 Probability review 01/11 3 Probability review 01/16 4 Concentration inequalities 01/18 5 Limit theorems 01/23 6 Limit theorems 01/25 7 Statistical inference 01/30 8 Sampling, CDF, statistical functionals 02/01 Tet 02/05 - 02/23 "],["part-1-background.html", "PART 1: Background", " PART 1: Background "],["probability.html", "Chapter 1 Probability 1.1 Review 1.2 Inequalities 1.3 Law of Large Numbers 1.4 Central Limit Theorem", " Chapter 1 Probability “If we have an atom that is in an excited state and so is going to emit a photon, we cannot say when it will emit the photon. It has a certain amplitude to emit the photon at any time, and we can predict only a probability for emission; we cannot predict the future exactly.” — Richard Feynman 1.1 Review 1.1.1 Probability Space Definition 1.1 (Sigma-algebra) Let \\(\\Omega\\) be a set. A set \\(\\Sigma \\subseteq \\mathcal{P}(\\Omega)\\) of subsets of \\(\\Omega\\) is called a \\(\\sigma\\)-algebra of \\(\\Omega\\) if \\(\\Omega\\in \\Sigma\\) \\(F \\in \\Sigma \\implies F^C \\in \\Sigma\\) If \\(F_n \\in \\Sigma\\) for all \\(n\\in \\mathbb{N}\\), then \\[ \\bigcup_n F_n \\in \\Sigma .\\] It is extremely convenient to deal with things called open sets. The definition of those are a bit out of the scope of this class. However, in the case of the real line \\(\\mathbb{R}\\), open sets are defined to be made of by finite intersections and arbitrary unions of open intervals \\((a,b)\\). For example, \\((0,1)\\cup (2,3)\\) is an open set. Interestingly, \\(\\mathbb{R}\\) and \\(\\emptyset\\) are called clopen sets (here’s a funny YouTube video about clopen sets: https://www.youtube.com/watch?v=SyD4p8_y8Kw) A Borel \\(\\sigma\\)-algebra is the smallest \\(\\sigma\\)-algebra that contains all the open sets. We denote the Borel \\(\\sigma\\)-algebra of a set \\(\\Omega\\) to be \\(\\mathcal{B}(\\Omega)\\). This is a rather abstract definition. There is no clear way to construct a sigma algebra from a collection of sets. However, the construction is not important as the reassurance that this object does exist to give us nice domains to work with when we define a probability measure (see below definition). Exercise 1.1 (Challenging– not required but good for the brain) It turns out that if \\(\\Omega\\) is a discrete set, it is typical to have the set of open sets contain every set of singletons, i.e., the set \\(\\{ a \\}\\) is open for every \\(a\\in \\Omega\\). Take this as an assumption, show that for any discrete set \\(\\Omega\\), \\(\\mathcal{B}(\\Omega) = \\mathcal{P}(\\Omega)\\). What open sets really are is not important for now. The important thing is that for \\(\\mathbb{R}^n\\) open sets are made of open intervals/ open boxes. Your typical intuitions still work. Philosophically, the \\(\\sigma\\)-algebra represents the details of information we could have access to. There are certain events that are building blocks of knowledge and that we don’t have access to finer details. Think about the \\(\\sigma\\)-algebra as a consistent model of what can be known (observed). For example, you can never know what’s going on in the houses on the street unless you have been to them. But somehow, together, you are still able to piece all the information you have about the houses to make sense of the world. This is related to the problem of information. How much information is enough to be useful in certain situation?! To have a consistent system is not the same as to know everything. The system you see/invent can never be exhaustively true, but you can still say something about the reality if you can have a system that is consistent with what you observe. This is why we do sampling!! When you have a consistent model, you now want to encode the model in such a way that it helps you with describing/predict the reality you see. A way to do that with no full knowledge of anything is to assign the certain number to measure the chance for something to happen at a given time. This encoding needs to happen on the model you constructed. This leads to the following definition of probability space. Definition 1.2 (Probability Space) A Probability Space is a triple \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\), where \\(\\Omega\\) is a set called sample space, \\(\\mathcal{F}\\) is a \\(\\sigma\\)-algebra on \\(\\Omega\\), \\(\\mathbb{P}: \\mathcal{F}\\to [0,1]\\), called a Probability Measure, is a function that satisfies the following: \\(\\mathbb{P}(\\Omega) =1\\), If \\(F\\) is a disjoint union of \\(\\left\\{ F_n \\right\\}_{n=1}^\\infty\\), then \\[ \\mathbb{P}(F) = \\sum_{n=1}^\\infty \\mathbb{P}(F_n) . \\] Each element \\(\\omega \\in \\Omega\\) is called an outcome and each subset \\(A \\in \\mathcal{F}\\) is called an event. Definition 1.3 (Independent Events) Let \\(A, B \\in \\mathcal{F}\\) be events. We say that \\(A\\) and \\(B\\) are independent if \\[ \\mathbb{P}(A \\cap B) = \\mathbb{P}(A) \\mathbb{P}(B) \\,. \\] Definition 1.4 (Conditional Probability) Let \\(A, B \\in \\mathcal{F}\\) be events such that \\(\\mathbb{P}(B) &gt;0\\). Then, the conditional probability of \\(A\\) given \\(B\\) is \\[ \\mathbb{P}(A \\vert B) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)} \\,. \\] Theorem 1.1 (Bayes's Theorem) Let \\(A, B \\in \\mathcal{F}\\) be events such that \\(\\mathbb{P}(A)&gt;0\\) and \\(\\mathbb{P}(B) &gt;0\\). Then, \\[\\mathbb{P}(A | B) = \\frac{\\mathbb{P}(B | A) \\mathbb{P}(A)}{\\mathbb{P}(B)} \\,.\\] In modern statistics, there are names for the above terms: \\(\\mathbb{P}(A | B)\\) is called Posterior Probability, \\(\\mathbb{P}(B | A)\\) is called Likelihood, \\(\\mathbb{P}(A)\\) is called Prior Probability, \\(\\mathbb{P}(B)\\) is called Evidence. The theorem is often expressed in words as: \\[ \\text{Posterior Probability} = \\frac{\\text{Likelihood} \\times \\text{Prior Probability}}{\\text{Evidence}} \\] It is a good idea to ponder why those mathematical terms have those names. 1.1.2 Random Variables The notion of probability alone isn’t sufficient for us to describe ideas about the world. We need to have a notion of objects that associated with probabilities. This brings about the idea of random variable. Definition 1.5 (Random Variable) Let \\((\\Omega, \\mathcal{B}(\\Omega), \\mathbb{P})\\) be a probability space and \\((S, \\mathcal{B}(S))\\) a \\(\\sigma\\)-algebra. A random variable is a (Borel measurable) function from \\(\\Omega \\to S\\). \\(S\\) is called the state space of \\(X\\). In this course, we will restrict our attentions to two types of random variables: discrete and continuous. Definition 1.6 (Discrete RV) \\(X: \\Omega \\to S\\) is called a discrete RV if \\(S\\) is a countable set. A probability function or probability mass function for \\(X\\) is a function \\(f_X: S \\to [0,1]\\) defined by \\[ f_X(x) = \\mathbb{P}(X = x) \\,. \\] In contrast to the simplicity of discrete RV. Continuous RVs are a little bit messier to describe. This is because of the lack of background in measure theory so we can talk about this concept in a more precise way. Definition 1.7 (Continuous RV) A continuous random variable is a measurable function \\(X:\\Omega \\to S\\) is continuous if it satisfies the following conditions: \\(S = \\mathbb{R}^n\\) for some \\(n\\in \\mathbb{N}\\). There exists an (integrable) function \\(f_X\\) such that \\(f_X(x) \\geq 0\\) for all \\(x, \\int_{\\mathbb{R}^n} f_X(x) d x=1\\) and for every open cube \\(C \\subseteq \\mathbb{R}^n\\), \\[ \\mathbb{P}(X\\in C)=\\int_C f_X(x) dV . \\] The function \\(f_X\\) is called the probability density function (PDF). If two RVs \\(X\\) and \\(Y\\) share the same probability function, we say that they have the same distribution and denote them by \\[ X \\stackrel{d}{=}Y.\\] In this case we also say that \\(X\\) and \\(Y\\) are equal in distribution. Remark. To make the presentation more compact and clean, notationally, we will write \\[ \\int f(x) dx \\] to mean both integral (for continuous RV) and summation (for discrete RV). There are more general concepts of continuous RV where we don’t need to require \\(S\\) to be a Euclidean space as in the above definition. However, such concepts require the readers to be familiar with advanced subjects like Topology and Measure Theory. It is particularly important to know these two subjects in order to thoroughly understand Stochastic Processes. Exercise 1.2 Create a random variable that represents the results of \\(n\\) coin flips. For real-valued RV \\(X:\\Omega \\to \\mathbb{R}\\) we have the concept of cumulative distribution function. Definition 1.8 (Cumulative Distribution Function) Given a RV \\(X:\\Omega \\to \\mathbb{R}\\). The cumulative distribution function of \\(X\\) or CDF, is a function \\(F_X : \\mathbb{R}\\to [0,1]\\) defined by \\[ F_X (x) = \\mathbb{P}(X \\leq x) \\,. \\] Notationally, we use the notation \\(X\\sim F\\) to mean RV \\(X\\) with distribution \\(F\\). Exercise 1.3 Given a real-valued continuous RV \\(X: \\Omega \\to \\mathbb{R}\\), prove that if \\(f_X\\) is continuous then \\[ F_X(x)=\\int_{-\\infty}^x f_X(t) d t \\] is differentiable for every \\(x\\) and \\(f_X(x)=F_X^{\\prime}(x)\\). Exercise 1.4 Let \\(X\\) be an RV with CDF \\(F\\) and \\(Y\\) with CDF \\(G\\). Suppose \\(F(x) = G(x)\\) for all \\(x\\). Show that for every set \\(A\\) that is a countable union of open intervals, \\[ \\mathbb{P}(X \\in A) = \\mathbb{P}(Y \\in A) \\,.\\] Exercise 1.5 Let \\(X:\\Omega \\to \\mathbb{R}\\) be an RV and \\(F_X\\) be its CDF. Prove the following: \\(F\\) is non-decreasing: if \\(x_1 \\leq x_2\\), then \\(F(x_1) \\leq F(x_2)\\). \\(F\\) is normalized: \\[ \\lim_{x\\to -\\infty} F(x) = 0 \\,,\\] and \\[ \\lim_{x\\to \\infty} F(x) = 1 \\,.\\] \\(F\\) is right-continuous: \\[ F(x) = F(x+) = \\lim_{y \\searrow x} F(y) \\,.\\] 1.1.3 Joint distribution of RVs Let \\(X:\\Omega \\to S\\) and \\(Y: \\Omega \\to S\\) be RVs. We denote \\[ \\mathbb{P}(X \\in A; Y \\in B) = \\mathbb{P}(\\{X\\in A\\} \\cap \\{Y \\in B \\}) \\,. \\] For discrete RVs, the joint probability function of \\(X\\) and \\(Y\\) has the following meaning \\[f_{XY}(x,y) = \\mathbb{P}(X = x; Y = y)\\] For continuous RVs, the situations are more complicated as we can’t make sense of \\(\\mathbb{P}(X = x; Y = y)\\) (this is always 0 in most situation and in some other situation, one can’t even talk about it– this is a topic of more advanced course in measure theory). However, we can have \\[\\mathbb{P}(X \\in A; Y \\in B) = \\int_{X \\in A} \\int_{Y \\in B} f_{XY} (x,y) \\, dx dy \\,.\\] Another way to look at the above is the following. We can even consider \\(X: \\Omega \\to S^n\\), where \\(n\\geq 2\\). Instead of thinking about this as one RV, we can think about this as a vector of RVs: \\[ X = \\begin{pmatrix} X_1 \\\\ \\vdots \\\\ X_n \\end{pmatrix},\\] where \\(X_1, \\dots X_n: \\Omega \\to S\\) are RVs. Because of this, we have can write the density function as \\[ f_X(x) = f_{X_1 X_2 \\dots X_n}(x)\\] Some people call \\(X\\) a random vector. \\(f_{X_1\\dots X_n}\\) is called the joint probability distribution. Exercise 1.6 True or false: \\(f_{XY}(x,y) = f_{X}(x) + f_{Y}(y)\\) \\(f_{XY}(x,y) = f_{X}(x) f_{Y}(y)\\) Definition 1.9 (Marginal density) The marginal density of \\(X_i\\) is \\[f_{X_i}(x_i) = \\int f_{X_1\\dots X_n}(x_1, \\dots, x_n) \\, dx_1\\dots dx_{i-1} dx_{i+1} \\dots dx_n\\] (integrate coordinate except the \\(i\\)-th coordinate. Exercise 1.7 Can you construct \\(f_{XY}\\) if you know \\(f_X\\) and \\(f_Y\\)? 1.1.4 Some important random variables Point mass distribution (Dirac delta): Given a discrete probability \\(X: \\Omega \\to S\\). \\(X\\) has a point mass distribution at \\(a \\in S\\) if \\[ \\mathbb{P}( X = a) = 1.\\] We call \\(X\\) a point mass RV and write \\(X \\sim \\delta_a\\). Question. Suppose \\(S = \\mathbb{N}\\). Write down \\(F_X\\) for the point mass RV \\(X\\). Discrete uniform distribution: \\(f_X(k) = \\frac 1 n\\,,\\) \\(k \\in \\{1,\\dots, n\\}\\). Bernoulli distribution: let \\(X:\\Omega \\to \\{0,1\\}\\) be RV that represents a binary coin flip. Suppose \\(\\mathbb{P}(X = 1) = p\\) for some \\(p \\in [0,1]\\). Then \\(X\\) has a Bernoulli distribution, written as \\(X \\sim \\text{Bernoulli}(p)\\). The probability function is \\[f_X(x) = p^x (1-p)^{1-x}.\\] We write \\(X \\sim \\text{Bernoulli}(p)\\). Binomial distribution: let \\(X:\\Omega \\to \\mathbb{N}\\) be the RV that represents the number of heads out of \\(n\\) independent coin flips. Then \\[ f_X = \\begin{cases} {n \\choose x} p^x (1-p)^{n-x}\\,, x\\in \\{0,1,\\dots, n\\}\\\\ 0 \\,, \\text{otherwise} \\end{cases}\\] We write \\(X \\sim \\text{Binomial}(n,p)\\). Poisson distribution: \\(X \\sim \\text{Poisson}(\\lambda)\\). \\[f_X (x) = e^{-\\lambda} \\frac{\\lambda^x}{x!}\\,, x \\geq 0.\\] \\(X\\) is a RV that describe a given number of events occurring in a fixed interval of time or space if these events occur with a known constant mean rate and independently of the time since the last event Gaussian: \\(X \\sim N(\\sigma,\\mu)\\). \\[f_X(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-(x-\\mu)^2/(2\\sigma^2)}.\\] Exercise 1.8 Let \\(X_{n,p} \\sim \\text{Binomial}(n,p)\\). Suppose that as \\(n\\to \\infty\\), \\(p \\to 0\\) in such a way that \\(np = \\lambda\\) always. Let \\(x\\in \\mathbb{N}\\). For \\(n\\) very very large, what is the behaviour of \\[ \\frac{n!}{(n-x)!} \\,.\\] (You should just get some power of \\(n\\)) Show that \\[\\lim_{n\\to \\infty} f_{X_{n,p}}(x) = \\frac{\\lambda^x e^{-\\lambda}}{x!}.\\] Interpret this result. 1.1.5 Independent random variables Definition 1.10 Let \\(X:\\Omega \\to S\\) and \\(Y: \\Omega \\to S\\) be RVs. We say that \\(X\\) and \\(Y\\) are independent if, for every \\(A, B \\in \\mathcal{B}(S)\\), we have \\[ \\mathbb{P}( X \\in A; Y \\in B) = \\mathbb{P}(X \\in A) \\mathbb{P}(Y \\in B) \\,, \\] and write \\(X \\perp Y\\). So, if \\(X\\) and \\(Y\\) are independent, \\[f_{XY}(x,y) = f_X(x) f_Y(y).\\] Definition 1.11 Let \\(X:\\Omega \\to S\\) and \\(Y: \\Omega \\to S\\) be RVs. Suppose that \\(f_Y(y) &gt;0\\). The conditional probability mass function of \\(X\\) given \\(Y\\) is \\[ f_{X|Y} (x|y) = \\frac{f_{XY}(x,y)}{f_Y(y)} \\,.\\] Exercise 1.9 Let \\[ f(x,y) = \\begin{cases} x+y \\,, 0\\leq x \\leq 1, 0 \\leq y \\leq 1\\\\ 0\\,, \\text{otherwise} \\end{cases}.\\] What is \\(\\mathbb{P}( X &lt; 1/4 \\vert Y = 1/3)\\). Note that the above exercise is a little bit weird and counter-intuitive. While \\(\\mathbb{P}( X &lt; 1/4 , Y = 1/3) = 0\\) (why?), \\(\\mathbb{P}( X &lt; 1/4 | Y = 1/3) \\not= 0\\) A very important RV is the multivariate Normal RV, which obeys the following density function \\[f(x; \\mu, \\Sigma) = \\frac{1}{(2\\pi)^{n/2} | \\Sigma |^{1/2}} \\exp\\big( -\\frac{1}{2} (x-mu)^T \\Sigma^{-1} (x-\\mu) \\big).\\] 1.1.6 Transformations of RVs Sometimes, we don’t work with RV directly but certain characteristics of RVs. Those characteristics are represented by certain transformation. If the functions are nice enough, we can actually have a recipe to generate the probability density function. Suppose \\(g: S^n \\to \\mathbb{R}\\) and \\(Z = g(X_1, \\dots, X_n)\\). Let \\(A_z = \\{ (x_1,\\dots, x_n): g(x_1,\\dots, x_n) \\leq z \\}\\). Then \\[F_Z(z) = \\mathbb{P}(Z \\leq z) = \\mathbb{P}(g(X_1, \\dots, X_n) \\leq z) = \\int_{A_z} f_{X_1\\dots X_n}(x_1,\\dots,x_n) \\, dx_1\\dots dx_n, \\] and \\[f_Z(z) = F_Z&#39;(z).\\] Exercise 1.10 Let \\(X, Y \\sim \\text{Uniform}(0,1)\\) be independent RVs, i.e., \\[f_{X}(x) = f_{Y}(y) = 1\\,.\\] What is the density function for the RV \\(Z = X + Y\\)? Same question but \\(X, Y \\sim N(0,1)\\). Definition 1.12 RVs that are independent and share the same distribution are called independent and identically distributed RVs. We often shorthand this by IID RVs. 1.1.7 Expectation Definition 1.13 Let \\(X\\) be a RV. The expected value, or expectation, or mean, or first moment of \\(X\\) is defined to be \\[ \\mathbb{E}X = \\int x f(x) dx. \\] The variance of \\(X\\) is defined to be \\[\\mathbb{E}\\left( X - \\mathbb{E}X \\right)^2\\] We often denote \\(\\mu_X\\) to be the expectation of \\(X\\), \\(\\sigma_X^2\\) (\\(\\mathrm{Var}(X), \\mathbb{V}(X)\\)) to be the variance of \\(X\\). The square root of the variance, \\(\\sigma\\), is called the standard deviation. Theorem 1.2 Let \\(X:\\Omega \\to S\\) be a RV, \\(r: S \\to S\\) be a function and \\(Y = r(X)\\). Then \\[ \\mathbb{E}Y = \\mathbb{E}(r(X)) = \\int r(x) f(x) dx \\] Exercise 1.11 Let \\(X \\sim \\text{Uniform}(0,1)\\). Compute \\(\\mathbb{E}Y\\), where \\(Y = e^X\\). \\(Y = \\max(X, 1-X)\\) Let \\(X,Y\\) be RVs that have jointly uniform distribution on the unit square. Compute \\(\\mathbb{E}(X^2 + Y^2)\\). Definition 1.14 Let \\(X\\) and \\(Y\\) be RVs. The covariance between \\(X\\) and \\(Y\\) is defined by \\[ \\mathrm{Cov}(X,Y) = \\mathbb{E}\\left( (X- \\mu_X)(Y - \\mu_Y) \\right) .\\] The correlation of \\(X\\) and \\(Y\\) is defined to be \\[ \\rho_{X,Y} = \\frac{\\mathrm{Cov}(X,Y)}{\\sigma_X \\sigma_Y} \\,.\\] Theorem 1.3 Let \\(X_i\\), \\(i=1, \\dots, n\\) be RVs and \\(a_i\\)’s be constants. Then \\[ \\mathbb{E}\\left(\\sum_i a_i X_i \\right) = \\sum_i a_i \\mathbb{E}X_i. \\] \\[ \\mathbb{V}X_i = \\mathbb{E}(X_i^2) - \\mu_{X_i}^2 \\] \\[ \\mathbb{V}\\left( \\sum_i a_i X_i \\right) = \\sum_i a_i^2 \\mathbb{V}(X_i) \\] \\[ \\mathbb{V}\\left( \\sum_i a_i X_i \\right) = \\sum_i a_i^2 \\mathbb{V}(X_i) + 2\\sum_{i&lt;j} a_i a_j \\mathrm{Cov}(X_i, X_j) \\,. \\] Suppose further that \\(X_i\\)’s are independent, then \\[ \\mathbb{E}\\left( \\prod_{i=1}^n X_i \\right) = \\prod_{i=1}^n \\mathbb{E}X_i\\] and \\[ \\mathbb{V}\\left( \\sum_i a_i X_i \\right) = \\sum_i a_i^2 \\mathbb{V}(X_i) \\,. \\] Definition 1.15 (Conditional Expectation) Let \\(X,Y:\\Omega \\to S\\), where \\(S\\) is either \\(\\mathbb{N}\\) or \\(\\mathbb{R}\\). The conditional expectation of \\(X\\) given \\(Y\\) is a RV \\(\\mathbb{E}[X | Y] : \\Omega \\to \\mathbb{R}\\) that satisfies the following \\[ \\mathbb{E}[X | Y](y) := \\mathbb{E}[X | Y = y] = \\int x f_{X|Y}(x|y) \\, dx . \\] If \\(r:S^2 \\to S\\) is a function, then \\[ \\mathbb{E}[r(X,Y) | Y = y] = \\int r(x,y) f_{X|Y}(x|y) \\, dx .\\] One can generalize this definition to higher dimension via the coordinate-wise conditional expectation. We will omit this definition in order to keep the presentation simple. Theorem 1.4 Let \\(X\\) and \\(Y\\) be Rvs. We have that \\[ \\mathbb{E}[ \\mathbb{E}[X | Y]] = \\mathbb{E}X. \\] 1.1.8 Moment Generating and Characteristics Functions Definition 1.16 Let \\(X\\) be a RV. 1. The moment generating function MGF, or Laplace transform, of \\(X\\) is \\(\\varphi: \\mathbb{R}\\to \\mathbb{R}\\) defined by \\[ \\varphi_X (t) = \\mathbb{E}\\left( e^{t X} \\right), \\] where \\(t\\) varies over the real numbers. The characteristics function, or Fourier transform of \\(X\\) is \\(\\varphi: \\mathbb{R}\\to \\mathbb{C}\\) defined by \\[\\phi_X(\\theta) = \\mathbb{E}e^{i\\theta X} .\\] Lemma 1.1 Let \\(X\\) be a RV and \\(Y = aX + b\\), then \\[ \\varphi_Y(t) = e^{bt} \\varphi_{X}(at)\\] \\[ \\varphi_X^{(k)}(0) = \\mathbb{E}(X^k) \\] Let \\(X_i\\), \\(i= 1, \\dots, n\\) be independent RVs and \\(Y = \\sum_i X_i\\). Then \\[ \\varphi_Y (t) = \\prod \\varphi_{X_i}(t). \\] \\[| \\phi (\\theta) | \\leq 1\\] Denote \\(\\overline{z}\\) to be the complex conjugate of \\(z\\) in the complex plane. \\[ \\phi_{-X} (\\theta) = \\overline{\\phi_X (\\theta)} \\] \\[\\phi_Y (\\theta) = e^{i b \\theta} \\phi(a\\theta) \\] Exercise 1.12 Prove the above lemma. Exercise 1.13 Let \\(X \\sim \\exp(1)\\), i.e, \\[ f_X(x) = \\begin{cases} e^{-x} \\,, x \\geq 0 \\\\ 0 \\,, x &lt; 0 \\,. \\end{cases}\\] Compute \\(\\varphi_X\\). Recall that two RVs \\(X \\stackrel{d}{=}Y\\) means that \\(F_X (x) = F_Y(x)\\). Two common ways to characterize the equality in distribution are to use the generating functions and the characteristic functions. These ideas are not orginally from probability but from engineering/mechanics, where Laplace and Fourier transforms are understood very well since the 18th century. Theorem 1.5 Let \\(X\\) and \\(Y\\) be RVs. If \\(\\varphi_X(t) = \\varphi_Y(t)\\) for all \\(t\\) in an interval around 0, then \\[ X \\stackrel{d}{=}Y \\,.\\] The full proof of this is beyond this class (and could be a great topic for a project). However, we will prove this for finte RVs. Proposition 1.1 (Finite RV case) Let \\(X,Y: \\Omega \\to \\{1,2, \\dots, N\\}\\) be RVs. If \\(\\varphi_X(t) = \\varphi_Y(t)\\) in an interval around in an interval \\((-\\epsilon , \\epsilon)\\), then \\[ X \\stackrel{d}{=}Y \\,.\\] Proof. We have that \\[ \\varphi_X (t) = \\mathbb{E}( e^{tX}) = \\sum_{i= 1}^N e^{it}\\mathbb{P}( X = i) \\] and \\[ \\varphi_Y (t) = \\mathbb{E}( e^{tX}) = \\sum_{i= 1}^N e^{it}\\mathbb{P}( Y = i). \\] Therefore, \\[ 0 = \\varphi_X(t) - \\varphi_Y(t) = \\sum_{i=1}^N (e^t)^i \\left( \\mathbb{P}(X = i) - \\mathbb{P}(Y = i) \\right) \\] for every \\(t \\in (-\\epsilon , \\epsilon)\\). Therefore, as the above is a polynomial, \\[ \\mathbb{P}( X = i) = \\mathbb{P}(Y = i) \\] where \\(i = 1, \\dots, N\\). Note that if the above summation is infinite, then we cannot conclude that \\(X\\) and \\(Y\\) has the same distribution easily as we just did. We have a similar statement for characteristics function. However, the proof of this is a little more manageable. Theorem 1.6 Let \\(X\\) and \\(Y\\) be RVs. If \\(\\phi_X(t) = \\phi_Y(t)\\) for all \\(t\\) in an interval around 0, then \\[ X \\stackrel{d}{=}Y \\,.\\] Proof. to be written 1.2 Inequalities 1.3 Law of Large Numbers 1.4 Central Limit Theorem "],["part-2-inference.html", "PART 2: Inference", " PART 2: Inference "],["sampling-estimating-cdf-and-statistical-functionals.html", "Chapter 2 Sampling, Estimating CDF and Statistical Functionals 2.1 Empirical Distribution 2.2 Statistical Functionals 2.3 Bootstrap", " Chapter 2 Sampling, Estimating CDF and Statistical Functionals 2.1 Empirical Distribution 2.2 Statistical Functionals 2.3 Bootstrap "],["parametric-inference-parameter-estimation.html", "Chapter 3 Parametric Inference (Parameter Estimation) 3.1 Method of Moments 3.2 Method of Maximum Likelihood 3.3 Bayesian Approach 3.4 Expectation-Maximization Algorithm 3.5 Unbiased Estimators 3.6 Efficiency: Cramer-Rao Inequality 3.7 Sufficiency and Unbiasedness: Rao-Blackwell Theorem", " Chapter 3 Parametric Inference (Parameter Estimation) 3.1 Method of Moments 3.2 Method of Maximum Likelihood 3.3 Bayesian Approach 3.4 Expectation-Maximization Algorithm 3.5 Unbiased Estimators 3.6 Efficiency: Cramer-Rao Inequality 3.7 Sufficiency and Unbiasedness: Rao-Blackwell Theorem "],["hypothesis-testing.html", "Chapter 4 Hypothesis Testing 4.1 Neyman-Pearson Lemma 4.2 Wald Test 4.3 Likelihood Ratio Test 4.4 Comparing samples", " Chapter 4 Hypothesis Testing 4.1 Neyman-Pearson Lemma 4.2 Wald Test 4.3 Likelihood Ratio Test 4.4 Comparing samples "],["part-3-models.html", "PART 3: Models", " PART 3: Models "],["linear-least-squares.html", "Chapter 5 Linear Least Squares 5.1 Simple Linear Regression 5.2 Matrix Approach 5.3 Statistical Properties", " Chapter 5 Linear Least Squares 5.1 Simple Linear Regression 5.2 Matrix Approach 5.3 Statistical Properties "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
>>>>>>> parent of 798ce7a (update)
