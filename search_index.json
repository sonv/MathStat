[["index.html", "MATH 310: Mathematical Statistics (brief notes) Disclaimer", " MATH 310: Mathematical Statistics (brief notes) Truong-Son Van Disclaimer This is class notes for Mathematical Statistics at Fublbright University Vietnam. I claim no originality in this work as it is mostly taken from the reference books. However, all errors and typos are solely mine. "],["syllabus.html", "Syllabus Textbooks and references Assessment Core content Project description Late assignments Time expectations Learning Support Wellbeing Tentative Course Schedule", " Syllabus Key information Instructor: Truong-Son Van Email: son.van+310@fulbright.edu.vn Class time: T &amp; Th: 9:45a - 11:15a Class Location: CRES CR 2 Office hours: M &amp; W, 10a-11a (or by appointment) Prerequisites: Multivariable Calculus (MATH 104) and Probability (MATH 205) Textbooks and references John Rice, Mathematical Statistics &amp; Data Analysis, 3rd Edition (main reference) Cassella and Berger, Statistical Inference, 2nd Edition Larry Wasserman, All of Statistics Course description Learning objectives Assessment During the course, students are expected to compute their own percentage points based on the following scheme. The instructor is not responsible for providing the running percentage. Form of assessment Weight Weekly homeworks 40% Mini-project 15% Midterm 20% Final 25% The following is the letter grade breakdown. It is based on common practice in the United States. Letter Grade Percentage A [93,100] A- [90,93) B+ [87,90) B [83,87) B- [80, 83) C+ [77,80) C [73,77) C- [70,73) D+ [67,70) D [60, 66) F [0,60) Core content Probability Review (2 weeks) Random Variables Concentration inequality (non-asymptotic theory) Limit Theorems (asymptotic theory) Special distributions Sampling distribution, confidence interval (Rice, 6.3, 7.1 – 7.3) (1 week) Parameter estimation &amp; Method of moments (Rice, 8.1 – 8.4) (1 week) Maximum likelihood estimation (Rice, 8.5) (1 week) Expection-Maximization Algorithm (Wasserman, 9.13.4) (1 week) Bayesian approach to parameter estimation (Wasserman, 11) (1 week) Unbiased estimators, Efficiency &amp; Cramer-Rao Inequality (Casella &amp; Berger, 7.3.2, Rice, 8.7) (1 week) Sufficiency and unbiasedness, Rao-Blackwell Theorem (Casella &amp; Berger, 7.3.3, Rice, 8.8) (1 week) Hypothesis testings, Neyman-Pearson Lemma, Wald test, Likelihood Ratio test (Rice, 9) (2 weeks) Comparing samples (Rice, 11) (1 week) Analysis of Variance (Rice, 12) (1 week) Linear regression and least squares (Wasserman, 13.1-13.3, Casella &amp; Berger, 11.3) (1 week) Project description Project Guidelines You may work alone or in team of two. You need to discuss with me a proposal for your project. The proposal should include your group members names &amp; IDs, a brief description of the content of your project (as clear and explicit as possible), how you plan to present it, and a suggested grading rubric. You will need to deliver a 15 minutes presentation as well as a 5-page-minimum write up in 1.5 spacing). Project Timeline TBD Possible topics TBD Late assignments 15% of the possible total mark will be deducted for every 24 hrs (or part of 24 hrs) after the deadline. Work more than 2 days late will not be accepted. Except for exceptional circumstances (see definition), I will not extend the deadlines. Time expectations Some materials require time to be accustomed to. Some students are quicker than others. However, on average, you should expect 10-15 hours per week (including class time) on the materials in order to know the subject relatively well. Collaboration &amp; Plagiarism Plagiarism is the act of submitting the intellectual property of another person as your own. It is one of the most serious of academic offenses. Acts of plagiarism include, but are not limited to: Copying, or allowing someone to copy, all or a part of another person’s work and presenting it as your own, or not giving proper credit. Purchasing a paper from someone (or a website) and presenting it as your own work. Re-submitting your work from another course to fulfill a requirement in another course. Further details can be found in the Code of Academic Integrity [link]. Learning Support In addition to your course instructors, there are other resources available to support your academic work at Fulbright, including one-on-one consultations with learning support staff, supplementary workshops, and both individual and group tutoring and mentoring in course content, language learning, and academic skills. If you would like to request learning support, please contact Fulbright Learning Support (https://learning-support.notion.site). Wellbeing Mental health and wellbeing are essential for the success of your academic journey. The Fulbright Wellness Center provides various services including counseling, safer community, and accessibility services. If you are experiencing undue personal or academic stress, are feeling unsafe, or would like to know more about issues related to wellbeing, please contact the Wellness Center via wellness@fulbright.edu.vn or visit the Wellness Center office on Level 5 of the Crescent campus. For more information, pleaes check https://onestop.fulbright.edu.vn/s/article/Health-and-Wellness-Introduction Tentative Course Schedule TBD "],["part-1-background.html", "PART 1: Background", " PART 1: Background "],["probability.html", "Chapter 1 Probability 1.1 Review 1.2 Inequalities 1.3 Law of Large Numbers 1.4 Central Limit Theorem", " Chapter 1 Probability “If we have an atom that is in an excited state and so is going to emit a photon, we cannot say when it will emit the photon. It has a certain amplitude to emit the photon at any time, and we can predict only a probability for emission; we cannot predict the future exactly.” — Richard Feynman 1.1 Review 1.1.1 Probability Space Definition 1.1 (Sigma-algebra) Let \\(\\Omega\\) be a set. A set \\(\\Sigma \\subseteq \\mathcal{P}(\\Omega)\\) of subsets of \\(\\Omega\\) is called a \\(\\sigma\\)-algebra of \\(\\Omega\\) if \\(\\Omega\\in \\Sigma\\) \\(F \\in \\Sigma \\implies F^C \\in \\Sigma\\) If \\(F_n \\in \\Sigma\\) for all \\(n\\in \\mathbb{N}\\), then \\[ \\bigcup_n F_n \\in \\Sigma .\\] A Borel \\(\\sigma\\)-algebra is the smallest \\(\\sigma\\)-algebra that contains all the open sets. Definition 1.2 (Probability Space) A Probability Space is a triple \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\), where \\(\\Omega\\) is a set called sample space, \\(\\mathcal{F}\\) is a \\(\\sigma\\)-algebra on \\(\\Omega\\), \\(\\mathbb{P}: \\mathcal{F}\\to [0,1]\\), called a Probability Measure, is a function that satisfies the following: \\(P(\\Omega) =1\\), If \\(F\\) is a disjoint union of \\(\\left\\{ F_n \\right\\}_{n=1}^\\infty\\), then \\[ \\mathbb{P}(F) = \\sum_{n=1}^\\infty \\mathbb{P}(F_n) . \\] Each element \\(\\omega \\in \\Omega\\) is called an outcome and each subset \\(A \\in \\mathcal{F}\\) is called an event. Philosophically, the \\(\\sigma\\)-algebra represents the details of information we could have access to. There are certain events that are building blocks of knowledge and that we don’t have access to finer details. Definition 1.3 (Independent Events) Let \\(A, B \\in \\mathcal{F}\\) be events. We say that \\(A\\) and \\(B\\) are independent if \\[ \\mathbb{P}(A \\cap B) = \\mathbb{P}(A) \\mathbb{P}(B) \\,. \\] Definition 1.4 (Conditional Probability) Let \\(A, B \\in \\mathcal{F}\\) be events such that \\(\\mathbb{P}(B) &gt;0\\). Then, the conditional probability of \\(A\\) given \\(B\\) is \\[ \\mathbb{P}(A \\vert B) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)} \\,. \\] Theorem 1.1 (Bayes's Theorem) Let \\(A, B \\in \\mathcal{F}\\) be events such that \\(\\mathbb{P}(A)&gt;0\\) and \\(\\mathbb{P}(B) &gt;0\\). Then, \\[\\mathbb{P}(A | B) = \\frac{\\mathbb{P}(B | A) \\ P(A)}{\\mathbb{P}(B)} \\,.\\] In modern statistics, there are names for the above terms: \\(\\mathbb{P}(A | B)\\) is called Posterior Probability, \\(\\mathbb{P}(B | A)\\) is called Likelihood, \\(\\mathbb{P}(A)\\) is called Prior Probability, \\(\\mathbb{P}(B)\\) is called Evidence. The theorem is often expressed in words as: \\[ \\text{Posterior Probability} = \\frac{\\text{Likelihood} \\times \\text{Prior Probability}}{\\text{Evidence}} \\] It is a good idea to ponder why those mathematical terms have those names. 1.1.2 Random Variables The notion of probability alone isn’t sufficient for us to describe ideas about the world. We need to have a notion of objects that associated with probabilities. This brings about the idea of random variable. Definition 1.5 (Random Variable) Let \\((\\Omega, \\mathcal{B}(\\Omega), \\mathbb{P})\\) and \\((S, \\mathcal{B}(S))\\). A random variable is a (Borel measurable) function from \\(\\Omega \\to S\\). \\(S\\) is called the state space of \\(X\\). In this course, we will restrict our attentions to two types of random variables: discrete and continuous. Definition 1.6 (Discrete RV) \\(X: \\Omega \\to S\\) is called a discrete RV if \\(S\\) is a countable set. Definition 1.7 (Continuous RV) A random variable \\(X\\) is continuous if there exists a function \\(f_X\\) such that \\(f_X(x) \\geq 0\\) for all \\(x, \\int_{-\\infty}^{\\infty} f_X(x) d x=1\\) and for every \\(a \\leq b\\), \\[ \\mathbb{P}(a&lt;X&lt;b)=\\int_a^b f_X(x) d x . \\] The function \\(f_X\\) is called the probability density function (PDF). We have that \\[ F_X(x)=\\int_{-\\infty}^x f_X(t) d t \\] and \\(f_X(x)=F_X^{\\prime}(x)\\) at all points \\(x\\) at which \\(F_X\\) is differentiable. Exercise 1.1 Create a random variable that represents the results of \\(n\\) coin flips. Definition 1.8 (Cumulative Distribution Function) Given a RV \\(X:\\Omega \\to \\mathbb{R}\\). The cumulative distribution function of \\(X\\) or CDF, is a function \\(F_X : \\mathbb{R}\\to [0,1]\\) defined by \\[ F_X (x) = \\mathbb{P}(X \\leq x) \\,. \\] Exercise 1.2 Let \\(X:\\Omega \\to \\mathbb{R}\\) be an RV and \\(F_X\\) be its CDF. Prove the following: \\(F\\) is non-decreasing: if \\(x_1 \\leq x_2\\), then \\(F(x_1) \\leq F(x_2)\\). \\(F\\) is normalized: \\[ \\lim_{x\\to -\\infty} F(x) = 0 \\,,\\] and \\[ \\lim_{x\\to \\infty} F(x) = 1 \\,.\\] \\(F\\) is right-continuous: \\[ F(x) = F(x+) = \\lim_{y \\searrow x} F(y) \\,.\\] 1.2 Inequalities 1.3 Law of Large Numbers 1.4 Central Limit Theorem "],["part-2-inference.html", "PART 2: Inference", " PART 2: Inference "],["sampling-estimating-cdf-and-statistical-functionals.html", "Chapter 2 Sampling, Estimating CDF and Statistical Functionals 2.1 Empirical Distribution 2.2 Statistical Functionals 2.3 Bootstrap", " Chapter 2 Sampling, Estimating CDF and Statistical Functionals 2.1 Empirical Distribution 2.2 Statistical Functionals 2.3 Bootstrap "],["parametric-inference-parameter-estimation.html", "Chapter 3 Parametric Inference (Parameter Estimation) 3.1 Method of Moments 3.2 Method of Maximum Likelihood 3.3 Bayesian Approach 3.4 Expectation-Maximization Algorithm 3.5 Unbiased Estimators 3.6 Efficiency: Cramer-Rao Inequality 3.7 Sufficiency and Unbiasedness: Rao-Blackwell Theorem", " Chapter 3 Parametric Inference (Parameter Estimation) 3.1 Method of Moments 3.2 Method of Maximum Likelihood 3.3 Bayesian Approach 3.4 Expectation-Maximization Algorithm 3.5 Unbiased Estimators 3.6 Efficiency: Cramer-Rao Inequality 3.7 Sufficiency and Unbiasedness: Rao-Blackwell Theorem "],["hypothesis-testing.html", "Chapter 4 Hypothesis Testing 4.1 Neyman-Pearson Lemma 4.2 Wald Test 4.3 Likelihood Ratio Test 4.4 Comparing samples", " Chapter 4 Hypothesis Testing 4.1 Neyman-Pearson Lemma 4.2 Wald Test 4.3 Likelihood Ratio Test 4.4 Comparing samples "],["part-3-models.html", "PART 3: Models", " PART 3: Models "],["linear-least-squares.html", "Chapter 5 Linear Least Squares 5.1 Simple Linear Regression 5.2 Matrix Approach 5.3 Statistical Properties", " Chapter 5 Linear Least Squares 5.1 Simple Linear Regression 5.2 Matrix Approach 5.3 Statistical Properties "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
